{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqZxwiqFKDs8"
      },
      "source": [
        "이 코드는 Aladdin Persson의 코드가 출처임을 밝힙니다.\n",
        "- https://www.youtube.com/watch?v=n9_XyCGr-MI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Sx-qkXOBor4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 여기서 box format은 박스 정보를 x1,x2,y1,y2로 받을지, x,y,width,height로 받을지 구분하는 것.\n",
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "\n",
        "    if box_format == \"midpoin\":\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "\n",
        "    if box_format == \"corners\":\n",
        "        box1_x1 = boxes_preds[..., 0:1] # 여기서 slicing을 하는 이유는 input shape를 유지하기 위해서이다. 만약 [..., 0]을 하면 (N,1)이 아니라 (N,)이 된다.\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4]\n",
        "\n",
        "        box2_x1 = boxes_preds[..., 0:1]\n",
        "        box2_y1 = boxes_preds[..., 1:2]\n",
        "        box2_x2 = boxes_preds[..., 2:3]\n",
        "        box2_y2 = boxes_preds[..., 3:4]\n",
        "\n",
        "\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    intersection = (x2-x1).clamp(0) * (y2-y1).clamp(0)\n",
        "    # clamp는 intersection이 전혀 없는, 0인 경우를 처리하기 위함이다. (https://pytorch.org/docs/stable/generated/torch.clamp.html)\n",
        "\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y1-box1_y2))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y1-box2_y2))\n",
        "\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6) # 분모의 1e-6 는 0 나누기 에러 방지를 위함이다.\n",
        "\n",
        "\n",
        "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"conrners\"):\n",
        "    # prediction = [ [1, 0.9, x1,y1, x2,y2], [~~], [~~] ]\n",
        "      # prediction 형태 설명:\n",
        "      # 세 개의 bounding box가 있다.\n",
        "      # 각각의 리스트는 6개의 element가 있다. -> 클래스(예: 1은 \"비행기\"), bounding box의 확률(안에 object가 있을 확률인 것 같다.), x1,y1,x2,y2 값\n",
        "\n",
        "    assert type(bboxes) == list\n",
        "\n",
        "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
        "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True) # bboxes를 높은 확률 값 순으로 내림차순 한다.\n",
        "    bboxes_after_nms = [] # non-max-suppression 후의 bbox 리스트\n",
        "\n",
        "    while bboxes: # bounding box가 여전히 리스트에 남아있다면.\n",
        "        chosen_box = bboxes.pop(0) # box 하나를 택해서\n",
        "        bboxes = [\n",
        "            box\n",
        "            for box in bboxes\n",
        "            if box[0] != chosen_box[0] # 만약 두 box가 같은 class가 아니고, iou값이 thrshold값 미만이면\n",
        "            or intersection_over_union(torch.tensor(chosen_box[2:]),torch.tensor(box[2:]),box_format=box_format)< iou_threshold # chosen_box[2:]라는건 [1, 0.9, x1,y1, x2,y2] 여기서 x1부터 원소를 고려하겠다는 의미다.\n",
        "        ] # IoU가 큰지 본다.\n",
        "\n",
        "        bboxes_after_nms.append(chosen_box)\n",
        "\n",
        "    return bboxes_after_nms\n",
        "\n",
        "def mean_average_precision(\n",
        "    pred_boxes, true_boxes, iou_threshold = 0.5, box_format=\"corners\", num_classes=20# prediction box는 모든 test set에서의 prediction box이다.\n",
        "):\n",
        "\n",
        "    # pred boxes (list)의 구조: [ [train_idx, class_pred, prob_score, x1,y1,x2,y2], [same....], [] ]\n",
        "        # train_idx: 이 bounding box가 어디서부터 오는지\n",
        "        # class_pred: bounding box에 대한 class prediction\n",
        "\n",
        "\n",
        "    average_precisions = []\n",
        "    epsilon = 1e-6\n",
        "    for c in range(num_classes):\n",
        "        detections = []\n",
        "        ground_truths = []\n",
        "\n",
        "        for detection in pred_boxes: # 각각의 예측 box에 대해서\n",
        "            if detection[1] == c: # 만약 해당 box의 class가 c가 맞다면\n",
        "                detections.append(detection) # detections에 box를 추가해라\n",
        "\n",
        "        for true_box in true_boxes:\n",
        "            if true_box[1] == c:\n",
        "                ground_truths.append(true_box) # 마찬가지로 class c인 true box를 grount_truths에 추가해라\n",
        "\n",
        "\n",
        "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "        # counter의 역할은 i번째 이미지가 j개의 bbox를 가지고 있다면, {0:3, 1: 2}와 같이 0번째  이미지는 bbox 3개를, 1번째 이미지는 2개의 bbox를 가지고있다. 라고 볼 수 있다.\n",
        "\n",
        "        for key, val in amount_bboxes.items():\n",
        "            amount_bboxes[key] = torch.zeros(val)\n",
        "\n",
        "        # amount_boxes = {0: torch.tensor([0,0,0]), 1: torch.tensor([0,0])) 와 같은 형태로 바뀔것이다.\n",
        "        detections.sort(key = lambda x: x[2], reverse=True) # 예측 class가 c인 예측 box들을 prediction score에 대해 sorting해라\n",
        "        TP = torch.zeros((len(detections)))\n",
        "        FP = torch.zeros((len(detections)))\n",
        "        total_true_bboxes = len(ground_truths)\n",
        "\n",
        "        for detection_idx, detection in enumerate(detections):\n",
        "            ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]\n",
        "            # 예를들어 첫번째 예측 bounding box가 1번 이미지에 대한거라면 1번 이미지에 대한 true box만을 ground_truth_img에 추가하겠다는 뜻\n",
        "\n",
        "            num_gts = len(ground_truth_img) # target bounding box의 개수\n",
        "            best_iou = 0\n",
        "\n",
        "            for idx, gt in enumerate(ground_truth_img):\n",
        "                iou = intersection_over_union(\n",
        "                    torch.tesnor(detection[3:]), torch.tensor(gt[3:]),\n",
        "                    box_format=box_format)\n",
        "\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = idx\n",
        "\n",
        "            if best_iou > iou_threshold:\n",
        "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "                    TP[detection_idx] = 1\n",
        "                    amount_bboxes[detection[0]][best_gt_idx] = 1 # 1로 표시함으로써, 이 이미지의 이 box는 다뤘다.\n",
        "                else:\n",
        "                    FP[detection_idx] = 1 # 이미 이 box는 전에 다뤘다.\n",
        "            else:\n",
        "                FP[detection_idx] = 1 # 이 box는 iou_treshold를 못넘어서 FP에 해당한다.\n",
        "\n",
        "\n",
        "        # cumsum의 예: [1,1,0,1,0] -> [1,2,2,3,3]\n",
        "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "        recalls = TP_cumsum/(total_true_bboxes + epsilon)\n",
        "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
        "        precisions = torch.cat((torch.tesnor([1]), precisions))\n",
        "        recalls = torch.cat((torch.tesnor([0]), recalls))\n",
        "\n",
        "        average_precisions.append(torch.trapz(precisions, recalls)) # y축에 precisions, x축에 recalls가 있을 때 이 둘로 이루어진 면적을 구해주는 함수인것 같다.\n",
        "\n",
        "    return sum(average_precisions) / len(average_precisions)\n",
        "        # precision에 1, recall에 0을 추가한 이유는, https://youtu.be/FppOzcDvaDI?list=PLhhyoLH6Ijfw0TpCTVTNk42NN08H6UvNq&t=409 여기서 (0,1) 부분의 값이 없었기 때문\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nz7lUCFHubrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image(image, boxes):\n",
        "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
        "    im = np.array(image)\n",
        "    height, width, _ = im.shape\n",
        "\n",
        "    # Create figure and axes\n",
        "    fig, ax = plt.subplots(1)\n",
        "    # Display the image\n",
        "    ax.imshow(im)\n",
        "\n",
        "    # box[0] is x midpoint, box[2] is width\n",
        "    # box[1] is y midpoint, box[3] is height\n",
        "\n",
        "    # Create a Rectangle potch\n",
        "    for box in boxes:\n",
        "        box = box[2:]\n",
        "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
        "        upper_left_x = box[0] - box[2] / 2\n",
        "        upper_left_y = box[1] - box[3] / 2\n",
        "        rect = patches.Rectangle(\n",
        "            (upper_left_x * width, upper_left_y * height),\n",
        "            box[2] * width,\n",
        "            box[3] * height,\n",
        "            linewidth=1,\n",
        "            edgecolor=\"r\",\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def get_bboxes(\n",
        "    loader,\n",
        "    model,\n",
        "    iou_threshold,\n",
        "    threshold,\n",
        "    pred_format=\"cells\",\n",
        "    box_format=\"midpoint\",\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    all_pred_boxes = []\n",
        "    all_true_boxes = []\n",
        "\n",
        "    # make sure model is in eval before get bboxes\n",
        "    model.eval()\n",
        "    train_idx = 0\n",
        "\n",
        "    for batch_idx, (x, labels) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        true_bboxes = cellboxes_to_boxes(labels)\n",
        "        bboxes = cellboxes_to_boxes(predictions)\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            nms_boxes = non_max_suppression(\n",
        "                bboxes[idx],\n",
        "                iou_threshold=iou_threshold,\n",
        "                threshold=threshold,\n",
        "                box_format=box_format,\n",
        "            )\n",
        "\n",
        "\n",
        "            #if batch_idx == 0 and idx == 0:\n",
        "            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
        "            #    print(nms_boxes)\n",
        "\n",
        "            for nms_box in nms_boxes:\n",
        "                all_pred_boxes.append([train_idx] + nms_box)\n",
        "\n",
        "            for box in true_bboxes[idx]:\n",
        "                # many will get converted to 0 pred\n",
        "                if box[1] > threshold:\n",
        "                    all_true_boxes.append([train_idx] + box)\n",
        "\n",
        "            train_idx += 1\n",
        "\n",
        "    model.train()\n",
        "    return all_pred_boxes, all_true_boxes\n",
        "\n",
        "\n",
        "\n",
        "def convert_cellboxes(predictions, S=7):\n",
        "    \"\"\"\n",
        "    Converts bounding boxes output from Yolo with\n",
        "    an image split size of S into entire image ratios\n",
        "    rather than relative to cell ratios. Tried to do this\n",
        "    vectorized, but this resulted in quite difficult to read\n",
        "    code... Use as a black box? Or implement a more intuitive,\n",
        "    using 2 for loops iterating range(S) and convert them one\n",
        "    by one, resulting in a slower but more readable implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    predictions = predictions.to(\"cpu\")\n",
        "    batch_size = predictions.shape[0]\n",
        "    predictions = predictions.reshape(batch_size, 7, 7, 30)\n",
        "    bboxes1 = predictions[..., 21:25]\n",
        "    bboxes2 = predictions[..., 26:30]\n",
        "    scores = torch.cat(\n",
        "        (predictions[..., 20].unsqueeze(0), predictions[..., 25].unsqueeze(0)), dim=0\n",
        "    )\n",
        "    best_box = scores.argmax(0).unsqueeze(-1)\n",
        "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
        "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
        "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
        "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
        "    w_y = 1 / S * best_boxes[..., 2:4]\n",
        "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
        "    predicted_class = predictions[..., :20].argmax(-1).unsqueeze(-1)\n",
        "    best_confidence = torch.max(predictions[..., 20], predictions[..., 25]).unsqueeze(\n",
        "        -1\n",
        "    )\n",
        "    converted_preds = torch.cat(\n",
        "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
        "    )\n",
        "\n",
        "    return converted_preds\n",
        "\n",
        "\n",
        "def cellboxes_to_boxes(out, S=7):\n",
        "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
        "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
        "    all_bboxes = []\n",
        "\n",
        "    for ex_idx in range(out.shape[0]):\n",
        "        bboxes = []\n",
        "\n",
        "        for bbox_idx in range(S * S):\n",
        "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
        "        all_bboxes.append(bboxes)\n",
        "\n",
        "    return all_bboxes\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ],
      "metadata": {
        "id": "TAVNuT1bwKWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_HPGeV5G6Ml"
      },
      "source": [
        "# 모델 구조 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7_HfYccG6BT",
        "outputId": "1c925d29-da1a-41e4-a972-32da13797243"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1470])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# 이것은 YOLO 모델 구조 그대로를 순서대로 layer 단위로 나타낸 것이다.\n",
        "# 여기서 tuple은 Conv2d layer를 pytorch Conv2d의 input, output, kernel_size, stride인자와 함께 나타낸 것이다\n",
        "# \"M\" 이라는 string은 Maxpool2d를 나타낸 것이다.\n",
        "# List: 이는 YOLO의 구조에서, conv의 3번째 4번째 block이 반복되는 layer를 가져서 따로 처리하고자 list로 정의함.\n",
        "# List에서 tuple은 위와 마찬가지로 conv2d layer의 정의이고 마지막 integer는 이 conv2d layer가 얼마나 반복되는지를 나타낸다.\n",
        "\n",
        "architecture_config = [\n",
        "    (7, 64, 2, 3),\n",
        "    \"M\",\n",
        "    (3, 192, 1, 1),\n",
        "    \"M\",\n",
        "    (1, 128, 1, 0),\n",
        "    (3, 256, 1, 1),\n",
        "    (1, 256, 1, 0),\n",
        "    (3, 512, 1, 1),\n",
        "    \"M\",\n",
        "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
        "    (1, 512, 1, 0),\n",
        "    (3, 1024, 1, 1),\n",
        "    \"M\",\n",
        "    [(1,512, 1, 0), (3, 1025, 1, 1), 2],\n",
        "    (3, 1024, 1, 1),\n",
        "    (3, 1024, 2, 1),\n",
        "    (3, 1024, 1, 1),\n",
        "    (3, 1024, 1, 1),\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, **kwargs):\n",
        "    super(CNNBlock, self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "    self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "    self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
        "\n",
        "\n",
        "class Yolov1(nn.Module):\n",
        "  def __init__(self, in_channels=3, **kwargs): ## in_channels default가 3인 이유는 RGB 이기때문\n",
        "    super(Yolov1, self).__init__()\n",
        "    self.architecture = architecture_config\n",
        "    self.in_channels = in_channels\n",
        "    self.darknet = self._create_conv_layers(self.architecture)\n",
        "    self.fcs = self._create_fcs(**kwargs)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.darknet(x)\n",
        "    return self.fcs(torch.flatten(x, start_dim=1))\n",
        "\n",
        "  def _create_conv_layers(self, architecture): # 여기서 위에서 정의한 architecture 를 가지고 model을 설계한다.\n",
        "    layers = []\n",
        "    in_channels = self.in_channels\n",
        "\n",
        "    for x in architecture:\n",
        "      if type(x) == tuple:\n",
        "        layers += [\n",
        "            CNNBlock(\n",
        "            in_channels, out_channels=x[1], kernel_size = x[0], stride = x[2], padding = x[3]\n",
        "            # 그런데 이런식이면 1 by 1 convoultion에서 이상하지 않나? 커널을 1로 하면서 input chanel을 받고 output channel을 1로 내뱉는게 아니게 되는데 # 의문점 1 *******************\n",
        "          )\n",
        "        ]\n",
        "        in_channels = x[1] # 다음 input 업데이트\n",
        "\n",
        "\n",
        "      elif type(x) == str:\n",
        "        layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "\n",
        "      elif type(x) == list:\n",
        "        conv1 = x[0] # Tuple: 반복 layer의 첫번째\n",
        "        conv2 = x[1] # Tuple: 반복 layer의 두번째\n",
        "        num_repeats = x[2] # Integer\n",
        "\n",
        "        for _ in range(num_repeats):\n",
        "          layers += [\n",
        "              CNNBlock(\n",
        "                  in_channels,\n",
        "                  conv1[1],\n",
        "                  kernel_size = conv1[0],\n",
        "                  stride = conv1[2],\n",
        "                  padding=conv1[3]\n",
        "              )\n",
        "          ]\n",
        "\n",
        "          layers += [\n",
        "              CNNBlock(\n",
        "                  conv1[1],\n",
        "                  conv2[1],\n",
        "                  kernel_size = conv2[0],\n",
        "                  stride = conv2[2],\n",
        "                  padding = conv2[3]\n",
        "              )\n",
        "          ]\n",
        "\n",
        "          in_channels = conv2[1]\n",
        "\n",
        "    return nn.Sequential(*layers) # Sequential에 넘길 때 각각 layer unpack해서 넣어준다.\n",
        "\n",
        "  def _create_fcs(self, split_size, num_boxes, num_classes):\n",
        "    S, B, C = split_size, num_boxes, num_classes\n",
        "    return nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(1024*S*S,496),  # 원래 paper에서는 output이 4096이다.\n",
        "        nn.Dropout(0.5),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.Linear(496, S * S * (C+B*5)) # 나중에 Reshape가 되어야한다고 한다. (https://youtu.be/n9_XyCGr-MI?t=1697)\n",
        "    )\n",
        "\n",
        "\n",
        "def test(S=7, B=2, C=20): # 모델 잘 작동하는지 shape 확인하는 용도\n",
        "  model = Yolov1(split_size = S, num_boxes = B, num_classes=C)\n",
        "  x = torch.randn((2,3,448,448))\n",
        "  return (model(x).shape)\n",
        "\n",
        "\n",
        "test()\n",
        "\n",
        "\n",
        "# 이해 못한 부분:\n",
        "# 1 by 1 conv는 어디서 구현된건지 모르겠다.\n",
        "# 코드 주의할 점으로 작성해두기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmO9RtU_9NXn"
      },
      "source": [
        "Loss function의 각 term을 보면 알겠지만 모두 Grid cell에 대한 sum 과 Bounding box에 대한 sum에 대해서 연산이 진행된다.\n",
        "- 첫 번째 term인 middle point 관련 loss 값에 대해서, Identity function of $(x_i - x^{hat}_i)^2 + (y_i - y^{hat}_i)^2$ 이 있을 때 이는 모든 cell 에 대해서 object가 있는 cell 에 대해서만? loss를 계산한다 하는듯. 자세한건 더 공부해봐야됨.\n",
        "- 두 번째 term은 width와 height 관련인데 여기는 square root가 씌워져있음. 이는 bounding box가 너무 큰 경우 loss가 매우 커지기 때문이라 함.\n",
        "\n",
        "여기까지, 첫 번쨰와 두 번째 term은 box의 coordinate를 위한 계산이라고 보면 됨\n",
        "\n",
        "\n",
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY1bkPhC_doa"
      },
      "source": [
        "# Loss function 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01zdIPsQ_dUW"
      },
      "outputs": [],
      "source": [
        "# utils 관련 코드는 github 혹은 https://www.youtube.com/playlist?list=PLhhyoLH6Ijfw0TpCTVTNk42NN08H6UvNq 를 참고하면 된다.\n",
        "\n",
        "\n",
        "class YoloLoss(nn.Module):\n",
        "  def __init__(self, S=7, B=2, C=20):\n",
        "    super(YoloLoss, self).__init__()\n",
        "    self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "    self.S = S\n",
        "    self.B = B\n",
        "    self.C = C\n",
        "    self.lambda_noobj = 0.5\n",
        "    self.lambda_coord = 5\n",
        "\n",
        "  def forward(self, predictions, target):\n",
        "    prediction = predictions.reshape(-1, self.S, self.S, self.C + self.B*5)\n",
        "\n",
        "    iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25]) # prediction의 0부터 19까지는 class probability 값이다. 20은 class score이다. 21부터 24는 네 개의 bounding box 값이다.\n",
        "    iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25]) # 타겟은 하나 뿐이라 여기서 target은 26:30이 아니라 21:25 라고 한다.\n",
        "    ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
        "    iou_maxes, best_box = torch.max(ious, dim=0) # 첫번째 것은 max값, 두 번쨰 것은 argmax로 max값의 index를 나타낸다.\n",
        "    exists_box = target[..., 20].unsqueeze(3) # object가 있냐 없냐에 따라 0, 혹은 1 값이다.\n",
        "    # exists_box의 역할은 논문 loss function에서 iobj_i 에 해당한다.\n",
        "\n",
        "    # =================== #\n",
        "    # For Box Coordinates #\n",
        "    # =================== #\n",
        "    # mid point와 width, height에 관한 loss이다.\n",
        "\n",
        "    box_predictions = exists_box * (\n",
        "        (\n",
        "            best_box * predictions[..., 26:30] # best_box=0 이라면 첫번째 box, 즉 iou_b1 =  prediction[..., 21:25] 가 더 크다는건데, 그래서 0*[26:30] 꼴로 해서 지우는듯.\n",
        "        + (1 - best_box) * predictions[..., 21:25]\n",
        "        )\n",
        "    )\n",
        "\n",
        "    box_targets = exists_box * target[..., :21:25]\n",
        "\n",
        "    box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
        "          torch.abs(box_predictions[..., 2:4]) + 1e-6 # 만약 0 가까이 값이 나오면 square root 의 derivative가 infinite가 될 수 있어서 stability를 위해 1e-6을 더했다고 한다.\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    box_targets[..., 2:4] = torch.sqrt[box_targets[..., 2:4]]  # label은 음수 값이 없다.\n",
        "\n",
        "\n",
        "    # end_dim의 역할: (N, S, S, 4) -> (N*S*S,4) 로 변환\n",
        "    box_loss = self.mse(\n",
        "        torch.flatten(box_predictions, end_dim=-2),\n",
        "        torch.flatten(box_targets, end_dim = -2)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # =================== #\n",
        "    # For Object Loss #\n",
        "    # =================== #\n",
        "    # 만약 object가 있으면 probability 가 1이 된다.\n",
        "\n",
        "    pred_box = (\n",
        "        best_box * predictions[..., 25:26] + (1-best_box) * predictions[..., 20:21]\n",
        "    )\n",
        "\n",
        "    # (N*S*S, 1)\n",
        "    object_loss = self.mse(\n",
        "        torch.flatten(exists_box * pred_box),\n",
        "        torch.flatten(exists_box * target[..., 20:21])\n",
        "    )\n",
        "\n",
        "    # =================== #\n",
        "    # For No Object Loss #\n",
        "    # =================== #\n",
        "\n",
        "    # (N, S, S, 1) -> (N, S*S)\n",
        "    no_object_loss = self.mse(\n",
        "        torch.flatten( (1-exists_box) * predictions[..., 20:21], start_dim=1),\n",
        "        torch.flatten( (1-exists_box) * target[..., 20:21], start_dim = 1)\n",
        "    )\n",
        "\n",
        "    no_object_loss += self.mse(\n",
        "        torch.flatten( (1-exists_box) * predictions[..., 25:26], start_dim=1),\n",
        "        torch.flatten( (1-exists_box) * target[..., 20:21], start_dim = 1)\n",
        "    )\n",
        "\n",
        "\n",
        "    # =================== #\n",
        "    # For Class Loss #\n",
        "    # =================== #\n",
        "    # object가 어떤 class에 속하는지.\n",
        "\n",
        "    class_loss = self.mse(\n",
        "        torch.flatten(exists_box * predictions[..., :20], end_dim=-2),\n",
        "        torch.flatten(exists_box * target[..., :20], end_dim = -2)\n",
        "    )\n",
        "\n",
        "\n",
        "    # =================== #\n",
        "    # Final Loss #\n",
        "    # =================== #\n",
        "\n",
        "    loss = (self.lambda_coord * box_loss\n",
        "            + object_loss\n",
        "            + self.lambda_noobj * no_object_loss\n",
        "            + class_loss)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OKyFk6ORd0U"
      },
      "source": [
        "# Dataset 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u3znfXUbm711",
        "outputId": "1322cb47-174b-4498-8002-05a1afbfe5e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWtJy3z3DoDO"
      },
      "outputs": [],
      "source": [
        "class VOCDataset(torch.utils.data.Dataset):\n",
        "  def __init__(\n",
        "      self, csv_file, img_dir, label_dir, S=7, B=2, C=20, transform=None # csv파일에는 image 파일과 txt(Label) 파일이 있다.\n",
        "  ):\n",
        "    self.annotations = pd.read_csv(csv_file)\n",
        "    self.img_dir = img_dir\n",
        "    self.label_dir = label_dir\n",
        "    self.transform = transform\n",
        "    self.S = S\n",
        "    self.B = B\n",
        "    self.C = C\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1]) # csv파일에 image와 text 중에서 text의 이름정보를\n",
        "    boxes = []\n",
        "    with open(label_path) as f:\n",
        "      for label in f.readlines():\n",
        "        class_label, x, y, width, height = [\n",
        "            float(x) if float(x) != int(float(x)) else int(x)\n",
        "            for x in label.replace(\"\\n\",\"\").split()\n",
        "        ]\n",
        "\n",
        "        boxes.append([class_label, x, y, width, height])\n",
        "\n",
        "    img_path = os.path.join(self.img_dir, self.annotations.iloc[index,0]) # image의 경로를 얻어내는 것\n",
        "    image = Image.open(img_path)\n",
        "    boxes = torch.tensor(boxes)\n",
        "\n",
        "\n",
        "\n",
        "    if self.transform:\n",
        "      image, boxes = self.transform(image, boxes) # object Detection의 경우, Rotation과 같은 transform을 하면 bonding box의 위치정보도 바뀌게되어 같이 고려해야한다. 뒤에서 이 부분을 고려한다고 한다.\n",
        "\n",
        "\n",
        "    label_matrix = torch.zeros((self.S, self.S, self.C + 5*self.B))\n",
        "    for box in boxes:\n",
        "      class_label, x, y, width, height = box.tolist()\n",
        "      class_label = int(class_label)\n",
        "\n",
        "      i, j = int(self.S * y), int(self.S * x)\n",
        "      x_cell, y_cell = self.S * x - j, self.S * y - i\n",
        "\n",
        "\n",
        "      width_cell, height_cell = (\n",
        "          width * self.S,\n",
        "          height * self.S\n",
        "      )\n",
        "\n",
        "      if label_matrix[i, j, 20] == 0:\n",
        "        label_matrix[i, j, 20] = 1\n",
        "        box_coordinates = torch.tensor(\n",
        "            [x_cell, y_cell, width_cell, height_cell]\n",
        "        )\n",
        "        label_matrix[i,j, 21:25] = box_coordinates\n",
        "        label_matrix[i,j, class_label] = 1\n",
        "\n",
        "    return image, label_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training setup & Evaluation 구현"
      ],
      "metadata": {
        "id": "NDZ0sSzuew9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c_iLF2bDn-7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as FutureWarning\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Hyperparameters etc\n",
        "LEARNING_RATE = 2e-5\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 16 # 이미지가 매우 크고, 모델이 그걸 다 받아들여야 하므로, 모델의 크기를 원본 논문보다 줄였어도, 원본의 batch_size인 64를 하긴 힘들다.\n",
        "WEIGHT_DECAY = 0\n",
        "\n",
        "EPOCHS = 100\n",
        "NUM_WORKERS = 2\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "LOAD_MODEL_FILE = \"overfit.pth.tar\" # 이는 overfit한 모델 저장\n",
        "IMG_DIR = \"./images\"\n",
        "LABEL_DIR = \"./labels\"\n",
        "\n",
        "# 먼저, image resize를 할 필요가 있다. 이를 위해서는 transform.compose를 해볼 수 있다.\n",
        "\n",
        "class Compose(object):\n",
        "  def __init__(self, transforms):\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __call__(self, img, bboxes):\n",
        "    for t in self.transforms:\n",
        "      img, bboxes = t(img), bboxes # transforms가 image data에만 적용되는 것이라 bboxes에는 적용이 힘들다.\n",
        "\n",
        "    return img, bboxes\n",
        "\n",
        "transform = Compose([transforms.Resize((448,448)), transforms.ToTensor()]) # 여기서 더 improve할 수 있는 것은 normalization을 하는 것인데, 여기서는 괜찮을 것 같다함.\n",
        "\n",
        "def train_fn(train_loader, model ,optimizer, loss_fn):\n",
        "  loop = tqdm(train_loader, leav=True) # for loop 할 때, progree bar 표시하기\n",
        "  mean_loss = []\n",
        "\n",
        "  for batch_idx, (x,y) in enumerate(loop):\n",
        "    x,y = x.to(device), y.to(device)\n",
        "    out = model(x)\n",
        "    loss = loss_fn(out,y)\n",
        "    mean_loss.append(loss.item())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update the progress bar\n",
        "    loop.set_postfix(loss = loss.item())\n",
        "\n",
        "  print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(device)\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "    loss_fn = YoloLoss()\n",
        "\n",
        "    if LOAD_MODEL:\n",
        "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
        "\n",
        "    train_dataset = VOCDataset(\n",
        "        \"./100examples.csv\",\n",
        "        transform=transform,\n",
        "        img_dir=IMG_DIR,\n",
        "        label_dir=LABEL_DIR,\n",
        "    )\n",
        "\n",
        "    test_dataset = VOCDataset(\n",
        "        \"./test.csv\", transform=transform, img_dir=IMG_DIR, label_dir=LABEL_DIR,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "\n",
        "      for x, y in train_loader:\n",
        "        x = x.to(device)\n",
        "        for idx in range(8):\n",
        "          bboxes = cellboxes_to_boxes(model(x))\n",
        "          bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
        "          plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
        "\n",
        "        import sys\n",
        "        sys.exit()\n",
        "\n",
        "      pred_boxes, target_boxes = get_bboxes(\n",
        "          train_loader, model, iou_threshold=0.5, threshold=0.4\n",
        "      )\n",
        "\n",
        "      mean_avg_prec = mean_average_precision(\n",
        "          pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
        "      )\n",
        "      print(f\"Train mAP: {mean_avg_prec}\")\n",
        "\n",
        "      if mean_avg_prec > 0.9:\n",
        "          checkpoint = {\n",
        "              \"state_dict\": model.state_dict(),\n",
        "              \"optimizer\": optimizer.state_dict(),\n",
        "          }\n",
        "          save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n",
        "          import time\n",
        "          time.sleep(10)\n",
        "\n",
        "      train_fn(train_loader, model, optimizer, loss_fn)\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "wqNMosmSguTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aT6D8QlTguc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zqg-fw6uguhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2mOAYEgJguk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nk1rHtGeguoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NxnhYde7gusN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l_FdmCRiguzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-zCwiAsGgu3d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
