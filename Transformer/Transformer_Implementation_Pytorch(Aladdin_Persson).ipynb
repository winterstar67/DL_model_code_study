{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aladdin Persson의 scratch부터 구현하는 Transformer를 보면서 그대로 작성한 코드이다.\n",
        "\n"
      ],
      "metadata": {
        "id": "Z_CmpgNF3COh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "장점:\n",
        "- 논문 모델 figure의 어느부분에 해당하는지 설명하면서 코드 구현을 진행하여 이해가 쉬웁. 직관적으로 코드를 논문과 비교하며 보기가 수월했다고 생각함.\n",
        "\n",
        "단점:\n",
        "- 논문을 그대로 구현하지 않음. 예를 들어, Positional encoding의 경우는 sin, cos 기반의 PE를 적용한게 아니라 그냥 Embedding 레이어 사용함.\n",
        "- 실제 데이터로 모델을 돌리기까진 안갔음.\n",
        "- torch.einsum 과 같은, 바로 와닿지 않는 함수가 있음 (하지만 이는 코딩 공부겸 장점이 될 수도 있다.)\n",
        "\n",
        "\n",
        "일단 시간이 넉넉하면 구현해보는 것도 괜찮다고 생각함.\n",
        "\n",
        "개인적으로, `논문(이론)으로 이해 - 적당한 구현 - 구체적인 구현` 에서 중간단계인 적당한 구현 정도로 생각한다."
      ],
      "metadata": {
        "id": "0u1Jculj3I17"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zeou4RkH3IgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsGhftuXQJhN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Attention"
      ],
      "metadata": {
        "id": "UWthfDeHQKku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embed_size, heads):\n",
        "    super(SelfAttention. self).__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.heads = heads\n",
        "    self.head_dim = embed_size//heads\n",
        "\n",
        "    assert (self.head_dim * heads == embed_size), \"Embedsize needs to be div by heads\"\n",
        "\n",
        "    self.values = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "    self.keys = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "    self.queries = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "    # 이 셋을 정의해놓고 사용은 안함. 원래 input word하나에 대해서 input -> value, input -> key, input -> query 이런식으로 변환 하는건데, 여기선 애초에 query, key, value를 처음부터 받는다고 가정하고 전개함.\n",
        "\n",
        "\n",
        "    self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
        "  \n",
        "  def forward(self, values, keys, query, mask):\n",
        "\n",
        "    ############################## 영상에 없는 것 추가한 부분 ***\n",
        "\n",
        "    N = query.shape[0] # training example의 개수, 배치 사이즈로 보면 될듯. https://youtu.be/U0s0f995w14?t=940\n",
        "    value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "    # 이제 multi head 각각의 head들로 values, keys, queries를 나눈다.\n",
        "    values = values.reshape(N, value_len, self.heads, self.head_dim) # 원래는 나눠지기 전의 상태로 입력을 받는다 가정하는듯 하다.\n",
        "    keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "    queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "    \n",
        "    values = self.values(values)\n",
        "    keys = self.keys(keys)\n",
        "    query = self.query(query)\n",
        "\n",
        "    energy = torch.einsum(\"nqhd,nkhd -> nhqk\", [queries, keys]) # 각각 query를 keys와 곱해서 target word인 query를 다른 word들인 keys에 얼마나 attention 시킬건지. dot product attention을 여기선 사용함.\n",
        "    # einsum은 batch를 고려한 matrix multiplication을 해야하는 상황에서 복잡하게 안하고 einsum으로 간단히 처리하는 것이다.\n",
        "    # 의미는 다음과 같다.\n",
        "    # queries의 shape는 (N, query_len, heads, heads_dim), einsum에선 이 각각을 n, q, h, d로 임의로 표현하여 nqhd로 작성한다.\n",
        "    # keys의 shape는 (N, key_len, heads, heads_dim), einsum에선 이 각각을 n, k, h, d로 임의로 표현하여 nqhd로 작성한다.\n",
        "    # -> nhqk는 저 shape를 nhqk와 같은 shape로 연산하고 싶다는 의미이다.\n",
        "      # 연산의 의미는 N은 배치니까 없다고 치고 (query_len, heads, heads_dim). (key_len, heads, heads_dim)만 고려하자.\n",
        "      # 여기서, multiplication이 (heads, heads_dim) (heads,heads_dim)끼리 된다면 각각의 query, key에 대해서 될 것이다.\n",
        "      # (첫번째 쿼리(word)에 대한, head들의 query, 각 head의 dim), (첫번째 key(word)에 대한, head들의 key, 각 head의 dim)의 행렬곱이 될거다.\n",
        "      \n",
        "\n",
        "\n",
        "    if mask is not None: # 만약 mask 적용을 하면\n",
        "      energy = energy.masked_fill(mask == 0, float(\"-1e20\")) # 여기선 패딩을 0으로 표현했다. 즉, 패딩된 부분은 값을 모두 -infinit로 하여, 해당 부분에 대한 attention은 0으로 만든다.\n",
        "\n",
        "    attention = torch.softmax(energy/(self.embed_size**(1/2)), dim=3) # (N,H,Q,K) shape이다 즉, K부분에 대한 softmax이고, 이는 하나의 query에 대한 모든 key에 대해서 softmax를 구하겠다는 의미이다.\n",
        "\n",
        "    out = torch.einsum(\"nhql, nlhd -> nqhd\", [attention, values]).reshape(\n",
        "        N, query_len, self.heads*self.head_dim\n",
        "    )\n",
        "\n",
        "    out = self.fc_out(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.attention = SelfAttention(embed_size, heads)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(embed_size)\n",
        "    self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(embed_size, forward_expansion*embed_size), # Feed forward 파트보면, 중간 layer에서 노드가 더 많아지는 파트있음. 그거 의미하는거. \n",
        "        nn.ReLU(),\n",
        "        nn.Linear(forward_expansion*embed_size, embed_size)\n",
        "    )\n",
        "    self.dropout= nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, value, key, query, mask):\n",
        "    attention = self.attention(value, key, query, mask)\n",
        "\n",
        "    x = self.dropout(self.norm1(attention + query)) # 이 부분은 오류인듯하다, 원래 input받고 query, key, value로 변환하는 식으로 하고 이거자체는 그냥 onput을 줘야되는데 여기선 그렇게 안했다.\n",
        "    forwards = self.feed_forward(x)\n",
        "    out = self.dropout(self.norm2(forwards+x))\n",
        "    return out\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      src_vocab_size,\n",
        "      embed_size,\n",
        "      num_layers,\n",
        "      heads,\n",
        "      device,\n",
        "      forward_expansion,\n",
        "      dropout,\n",
        "      max_length\n",
        "  ):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.device = device\n",
        "    self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "    self.position_embedding = nn.Embedding(max_length, embed_size) # 여기선 sin, cos을 사용 안했다.\n",
        "\n",
        "    self.layers = nn.ModuleList(\n",
        "        [\n",
        "            TransformerBlock(\n",
        "                embed_size,\n",
        "                heads,\n",
        "                dropout = dropout,\n",
        "                forward_expansion = forward_expansion\n",
        "            )\n",
        "        for _ in range(num_layers)]\n",
        "    )\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "      N, seq_length = x.shape\n",
        "      positions = torch.arange(0, seq_length).exapnd(N, seq_length).to(self.device)\n",
        "      out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
        "\n",
        "      for layer in self.layers:\n",
        "        out = layer(out, out, out, mask) # 이 코드에서 구현한 Transformer안에선 각각을 key query value로 변환하는 과정을 안거친다. -> 원래 거치게 하면 out out out 넣는게 맞다, 댓글보니까 query, key, value 변환 코드를 넣어야 하는듯\n",
        "\n",
        "      return out\n",
        "\n",
        "class DecoderBlock(nn.Module)::\n",
        "  def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "    super(DecoderBlock,self).__init__()\n",
        "    self.attention = SelfAttention(embed_size, heads)\n",
        "    self.norm = nn.LayerNorm(embed_size)\n",
        "    self.transformer_block = TransformerBlock(\n",
        "        embed_size, heads, dropout, forward_expansion\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forwar(self, x, value, key, src_mask, trg_mask):  # src mask는 옵션이지만, trg mask는 반드시 decoder에서 가져야한다. src mask는 예제들 몇개 넣었을 때 모두 같은 길이 가지도록 패딩함. 그 후에 그 패딩된 애들에 대해서는 계산 딱히 안하도록 src mask 사용하는거 (https://youtu.be/U0s0f995w14?t=2438)\n",
        "    attention = self.attention(x,x,x, trg_mask) # decoder에선 mask 필요함.\n",
        "    query = self.dropout(self.norm(attention + x))\n",
        "    out = self.transformer_block(value, key, query, src_mask)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               trg_vocal_size,\n",
        "               embed_size,\n",
        "               num_layers,\n",
        "               heads,\n",
        "               forward_expansion,\n",
        "               dropout,\n",
        "               device,\n",
        "               max_length):\n",
        "    super(Decoder,self).__init__() # 상속받고자 하는 상위 모듈의 init를 그대로 불러오는 역할을 한다.  \n",
        "    self.device = device\n",
        "    self.word_embedding = nn.Embedding(tag_vocab_size, embed_size)\n",
        "    self.position_embedding = nn.Embeding(max_length, embed_size)\n",
        "\n",
        "    self.layers = nn.ModuleList(\n",
        "        [DecoderBlock(embed_size, heads, forward_expansion, dropout, device) for _ in range(num_layers)]\n",
        "    )\n",
        "    self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "    N, seq_length = x.shape\n",
        "    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "    X = self.dropout(self.word_embedding(x) + self.position_embedding(x))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, enc_out, enc_out)\n",
        "    \n",
        "    out = self.fc_out(x)\n",
        "    return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      src_vocab_size,\n",
        "      trg_vocab_size,\n",
        "      src_pad_idx,\n",
        "      trg_pad_idx,\n",
        "      embed_size=256,\n",
        "      num_layers=6,\n",
        "      forward_expansion=4,\n",
        "      heads=8,\n",
        "      dropout=0,\n",
        "      device=\"cuda\",\n",
        "      max_length=100      \n",
        "  ):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder = Encoder(\n",
        "        src_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        device,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length\n",
        "    )\n",
        "\n",
        "    self.decoder = Decoder(\n",
        "        trg_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        device,\n",
        "        max_length\n",
        "    )\n",
        "\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.trg_pad_idx = trg_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "  def make_src_mask(self, src):\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    # (N, 1, 1, src_len)\n",
        "    return src_mask.to(self.device)\n",
        " \n",
        "  def make_trg_mask(self,trg):\n",
        "    N, trg_len = trg.shape\n",
        "    trg_mask = torch.trill(torch.ones((trg_len, trg_len))).expand(\n",
        "        N, 1, trg_len, trg_len\n",
        "    )\n",
        "    return trg_mask.to(self.device)\n",
        "\n",
        "  def forward(self, src, trg):\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    trg_mask = self.make_trg_mask(trg)\n",
        "    enc_src = self.encoder(src, src_mask)\n",
        "    out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "j8UaXnJMQKK-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}