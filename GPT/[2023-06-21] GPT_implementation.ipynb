{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 이 코드는 Andrej Karpathy님의 코드임을 밝힙니다.\n",
        "출처: https://www.youtube.com/watch?v=kCc8FmEb1nY"
      ],
      "metadata": {
        "id": "os6piFYwn4LS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32"
      ],
      "metadata": {
        "id": "Ujqxx5lIDxta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "ggoOe6dtLejw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt','r', encoding = 'utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "hvNNLNBQwfKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFCsnIhpwfIo",
        "outputId": "2bff11d4-91f5-4927-a985-90579c9ebd8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Bbe6eSgwfHe",
        "outputId": "78ac5090-d27a-43db-e870-185128532315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqviLaBtwfGS",
        "outputId": "73fa6f7f-b374-4d00-bd69-7a566aa55dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # 새로운 방식의 함수 정의법, lambda l 에서 l이 함수 입력값으로 받아짐.\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cpMHWZxwfE6",
        "outputId": "a3d91619-fdee-4168-97d5-82cb700a4365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae-XdRuswfDm",
        "outputId": "432b292d-13b5-4578-bb04-bd9a87df33bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "F0ewMrpLwfA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 training을 할 때, 모든 데이터를 한번에 넣어서 training을 하진 않는다. (계산 비용이 매우 높다.)\n",
        "대신 랜덤하게 chunk를 뽑아내서 training에 각각 사용한다. (https://youtu.be/kCc8FmEb1nY?t=894)\n",
        "여기서는 block_size라는 이름으로 maximum_length를 지정한다."
      ],
      "metadata": {
        "id": "kzllNL7a6pZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 길이 8의 훈련데이터를 설정하고 샘플링을 할 때는 길이 9를 샘플링 한다. 이는 autoregressive에서 흔하게 사용하는 teacher forcing 방식을 사용하기 위해 하는 것이다.\n",
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY3sjBrbwe_w",
        "outputId": "c8b1fd8d-cf62-4175-afc2-e3b3b016db3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이제 입력값과 label값을 정의해보자\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt3vbDguwe-o",
        "outputId": "32c40bab-ad23-4b2e-c1c7-0e54575a0c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서, Transformer가 inference 하는 방식은 다음과 같다.\n",
        "- 일단 block size 크기 전까지 계속 예측할 때는 이전의 예측 token들을 기반으로 다음 것을 예측한다.\n",
        "- 그러나 block size만큼 예측을 하면, 그 후에는 truncate(이전 것을 잘라냄) 하면서 길이 8을 유지하며, 다음 토큰을 예측하는 식으로 예측이 진행된다."
      ],
      "metadata": {
        "id": "sPXR3Gt_8lWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "여태까지는 single data에 대해서 데이터를 만든 것이였다.\n",
        "이번에는 배치를 고려하며 다시 코드를 작성해보자."
      ],
      "metadata": {
        "id": "e7_KYYI29Lnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size, )) # 마지막 8개(block_size=8)는 그 다음 것을 예측할 때 사용하지 못한다.\n",
        "\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x, y\n",
        "\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZQzgRlQwe9D",
        "outputId": "13c1a8b8-a095-45cd-862e-af383c2da50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_l_zeAswe6i",
        "outputId": "4fef7da2-a2c3-4037-bfd2-bbfa24359d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ※ 1. BigramLanguageModel 기반 문자열 생성"
      ],
      "metadata": {
        "id": "tSPc3TXKJb4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337) # pytorch의 random seed 고정. 재구현성을 위해 설정\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    # 여기서 idx와 targets의 shape는 (Batch size, Time(문장 하나의 단어 개수) ) 이다.\n",
        "    tok_emb = self.token_embedding_table(idx) # 이것의 반환 결과 shape는 (Batch, Time, Channel(단어,토큰 하나의 임베딩 결과 벡터) ) 이다.\n",
        "    # loss2 = F.cross_entropy(logits, targets) # 이 코드는 작동 안한다. 왜냐하면 F.cross entropy는 input을 (B,C,T)로 받기 때문이다. (https://youtu.be/kCc8FmEb1nY?t=1597)\n",
        "    logits = self.lm_head(tok_emb)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C) # make sense한 shape임.\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx) # idx의 shape는 (B,T)이다.  여기서 self()는 forward를 부르는 함수라 한다. https://youtu.be/kCc8FmEb1nY?t=1886\n",
        "\n",
        "      logits = logits[:, -1, :] # 예측한 것의 마지막 토큰(즉, 다음 예측, autoregressive model로 치면 바로 다음 예측한 토큰)에 대한 확률값만 가져온다.\n",
        "\n",
        "      probs = F.softmax(logits, dim=-1) # softmax적용\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # softmax로 나온 다음 토큰에 대한 확률을 기반으로 다음 토크을 뽑는다.\n",
        "\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # 기존 idx를 새로 뽑은 토큰과 concat해서 shape가 (B,T+1)인 idx를 새로 만든다.\n",
        "\n",
        "    return idx # 매 스텝마다 sampling해서 token들이 concat된 최종 예측 sequence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss  = m(xb, yb)\n",
        "print(logits.shape, loss)\n",
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZgOusi7we5M",
        "outputId": "cc33baab-82b1-4994-8bb2-10c503d63849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65]) tensor(4.4922, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "lN!BJ'kysLCMFJPKOL?DP-QWwrEoL?jLDJQOL.f'RIHD'Hdhs Yv,wxatnscMZwtEOS'palkq3ssZeAvzF-QT;eMk;x.gQSFCLgx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재는 bigram으로 앞의 하나 단어를 보고 다음 토큰을 유추하는 방식이다."
      ],
      "metadata": {
        "id": "dNI2uo9h0GlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "bigram model을 학습시켜보자.\n",
        "(물론, 셰익스피어를 잘 학습하진 않았을 것이다. 하지만 어떠한 일정 패턴은 보일 것이다.)"
      ],
      "metadata": {
        "id": "kQb9css30Do2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "oaqv-KVSwefy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  # set_to_none 관련해서는 https://velog.io/@kjb0531/zerograd%EC%9D%98-%EC%9D%B4%ED%95%B4 참고\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  print(loss.item())"
      ],
      "metadata": {
        "id": "d3jNOKUiweeU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c72b5b45-a3cf-4342-d421-11b1fd26a99a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "2.5015437602996826\n",
            "2.4089622497558594\n",
            "2.5160410404205322\n",
            "2.4741573333740234\n",
            "2.4618895053863525\n",
            "2.422511339187622\n",
            "2.4093828201293945\n",
            "2.548694133758545\n",
            "2.4375903606414795\n",
            "2.4635019302368164\n",
            "2.362743616104126\n",
            "2.4755489826202393\n",
            "2.357532024383545\n",
            "2.376260280609131\n",
            "2.444220542907715\n",
            "2.4985249042510986\n",
            "2.4815733432769775\n",
            "2.5948750972747803\n",
            "2.4701743125915527\n",
            "2.4592843055725098\n",
            "2.438476324081421\n",
            "2.4719128608703613\n",
            "2.575519561767578\n",
            "2.4527246952056885\n",
            "2.482231855392456\n",
            "2.4283199310302734\n",
            "2.522643804550171\n",
            "2.531187057495117\n",
            "2.4472765922546387\n",
            "2.44993257522583\n",
            "2.439965009689331\n",
            "2.4108545780181885\n",
            "2.4720044136047363\n",
            "2.4077725410461426\n",
            "2.530170440673828\n",
            "2.5499625205993652\n",
            "2.4030582904815674\n",
            "2.3826732635498047\n",
            "2.4314067363739014\n",
            "2.5083019733428955\n",
            "2.5102102756500244\n",
            "2.5320401191711426\n",
            "2.453016996383667\n",
            "2.480130672454834\n",
            "2.488076686859131\n",
            "2.369173049926758\n",
            "2.5427792072296143\n",
            "2.441377639770508\n",
            "2.5128674507141113\n",
            "2.4079766273498535\n",
            "2.57381272315979\n",
            "2.463320255279541\n",
            "2.5200355052948\n",
            "2.4784512519836426\n",
            "2.3554482460021973\n",
            "2.4259932041168213\n",
            "2.3940422534942627\n",
            "2.654323101043701\n",
            "2.3900625705718994\n",
            "2.415501356124878\n",
            "2.4680941104888916\n",
            "2.5263924598693848\n",
            "2.4874863624572754\n",
            "2.4889934062957764\n",
            "2.51607608795166\n",
            "2.4508731365203857\n",
            "2.4762940406799316\n",
            "2.50679349899292\n",
            "2.485539197921753\n",
            "2.3778960704803467\n",
            "2.425680160522461\n",
            "2.4570424556732178\n",
            "2.2714738845825195\n",
            "2.534252643585205\n",
            "2.4726364612579346\n",
            "2.5523176193237305\n",
            "2.340888738632202\n",
            "2.5320286750793457\n",
            "2.4721314907073975\n",
            "2.5405020713806152\n",
            "2.5321261882781982\n",
            "2.481900930404663\n",
            "2.4692466259002686\n",
            "2.524975061416626\n",
            "2.5517635345458984\n",
            "2.49277400970459\n",
            "2.444578170776367\n",
            "2.5893468856811523\n",
            "2.4774162769317627\n",
            "2.350804328918457\n",
            "2.5231802463531494\n",
            "2.523986577987671\n",
            "2.485365629196167\n",
            "2.418509006500244\n",
            "2.5328047275543213\n",
            "2.4475202560424805\n",
            "2.6110799312591553\n",
            "2.516425848007202\n",
            "2.465639591217041\n",
            "2.238736629486084\n",
            "2.4160609245300293\n",
            "2.522519588470459\n",
            "2.5111825466156006\n",
            "2.446824550628662\n",
            "2.478724956512451\n",
            "2.450547456741333\n",
            "2.4277420043945312\n",
            "2.3583052158355713\n",
            "2.481682300567627\n",
            "2.5170795917510986\n",
            "2.5384304523468018\n",
            "2.601374864578247\n",
            "2.44157075881958\n",
            "2.52689790725708\n",
            "2.527073860168457\n",
            "2.419424057006836\n",
            "2.5301241874694824\n",
            "2.4667723178863525\n",
            "2.4487082958221436\n",
            "2.412771701812744\n",
            "2.530611515045166\n",
            "2.488992691040039\n",
            "2.3552260398864746\n",
            "2.453911542892456\n",
            "2.550403118133545\n",
            "2.458750009536743\n",
            "2.4122061729431152\n",
            "2.5271270275115967\n",
            "2.4780170917510986\n",
            "2.5452637672424316\n",
            "2.4987659454345703\n",
            "2.4151463508605957\n",
            "2.5699071884155273\n",
            "2.657928228378296\n",
            "2.540278673171997\n",
            "2.530200481414795\n",
            "2.3697805404663086\n",
            "2.5533692836761475\n",
            "2.2748208045959473\n",
            "2.376246452331543\n",
            "2.501981019973755\n",
            "2.3937158584594727\n",
            "2.3742594718933105\n",
            "2.456920862197876\n",
            "2.4967563152313232\n",
            "2.5036110877990723\n",
            "2.4313764572143555\n",
            "2.4222235679626465\n",
            "2.434361219406128\n",
            "2.6864681243896484\n",
            "2.5340850353240967\n",
            "2.5043718814849854\n",
            "2.460852861404419\n",
            "2.383455753326416\n",
            "2.4742190837860107\n",
            "2.5573441982269287\n",
            "2.420137405395508\n",
            "2.5289523601531982\n",
            "2.4876697063446045\n",
            "2.5016121864318848\n",
            "2.6602237224578857\n",
            "2.4174466133117676\n",
            "2.5357959270477295\n",
            "2.4524753093719482\n",
            "2.5005595684051514\n",
            "2.339991807937622\n",
            "2.45133900642395\n",
            "2.489997148513794\n",
            "2.4864327907562256\n",
            "2.5568649768829346\n",
            "2.487365484237671\n",
            "2.445266008377075\n",
            "2.321906805038452\n",
            "2.581015110015869\n",
            "2.531697988510132\n",
            "2.551307439804077\n",
            "2.4772841930389404\n",
            "2.5221335887908936\n",
            "2.410386323928833\n",
            "2.4987499713897705\n",
            "2.4561872482299805\n",
            "2.4546422958374023\n",
            "2.5602920055389404\n",
            "2.409231662750244\n",
            "2.4526638984680176\n",
            "2.3177289962768555\n",
            "2.4711697101593018\n",
            "2.3659493923187256\n",
            "2.444279670715332\n",
            "2.482743263244629\n",
            "2.4102118015289307\n",
            "2.5489296913146973\n",
            "2.5787854194641113\n",
            "2.526684522628784\n",
            "2.5591230392456055\n",
            "2.364583730697632\n",
            "2.4739279747009277\n",
            "2.5271599292755127\n",
            "2.566338062286377\n",
            "2.581667184829712\n",
            "2.4925572872161865\n",
            "2.407107353210449\n",
            "2.4150376319885254\n",
            "2.4755654335021973\n",
            "2.4348642826080322\n",
            "2.4604268074035645\n",
            "2.3924736976623535\n",
            "2.3095455169677734\n",
            "2.426323413848877\n",
            "2.45428729057312\n",
            "2.4799368381500244\n",
            "2.395705223083496\n",
            "2.5460023880004883\n",
            "2.570647954940796\n",
            "2.4907145500183105\n",
            "2.324761390686035\n",
            "2.4716689586639404\n",
            "2.661176919937134\n",
            "2.597216844558716\n",
            "2.3833935260772705\n",
            "2.49763822555542\n",
            "2.533006191253662\n",
            "2.48240065574646\n",
            "2.4045395851135254\n",
            "2.433784008026123\n",
            "2.4763543605804443\n",
            "2.667775869369507\n",
            "2.468489170074463\n",
            "2.434649705886841\n",
            "2.5113205909729004\n",
            "2.4486608505249023\n",
            "2.5312116146087646\n",
            "2.488251209259033\n",
            "2.4331085681915283\n",
            "2.394253969192505\n",
            "2.536801815032959\n",
            "2.4427809715270996\n",
            "2.439302682876587\n",
            "2.3858590126037598\n",
            "2.444528341293335\n",
            "2.590728759765625\n",
            "2.485090970993042\n",
            "2.449632167816162\n",
            "2.4868693351745605\n",
            "2.5021250247955322\n",
            "2.574338912963867\n",
            "2.461560010910034\n",
            "2.4837422370910645\n",
            "2.4658491611480713\n",
            "2.3277335166931152\n",
            "2.4583845138549805\n",
            "2.4620165824890137\n",
            "2.4998414516448975\n",
            "2.4009225368499756\n",
            "2.5841503143310547\n",
            "2.5083043575286865\n",
            "2.45859694480896\n",
            "2.381391763687134\n",
            "2.446735143661499\n",
            "2.560084581375122\n",
            "2.5337228775024414\n",
            "2.395458936691284\n",
            "2.391939401626587\n",
            "2.413541316986084\n",
            "2.4300992488861084\n",
            "2.483647346496582\n",
            "2.3908274173736572\n",
            "2.5809829235076904\n",
            "2.4361722469329834\n",
            "2.481940746307373\n",
            "2.4344611167907715\n",
            "2.343177556991577\n",
            "2.369640350341797\n",
            "2.5145280361175537\n",
            "2.5037572383880615\n",
            "2.410341501235962\n",
            "2.438380002975464\n",
            "2.512556314468384\n",
            "2.4318699836730957\n",
            "2.5396595001220703\n",
            "2.492410182952881\n",
            "2.543546438217163\n",
            "2.4102346897125244\n",
            "2.394291400909424\n",
            "2.493358850479126\n",
            "2.441394090652466\n",
            "2.4383366107940674\n",
            "2.457874298095703\n",
            "2.470973253250122\n",
            "2.496044158935547\n",
            "2.483511447906494\n",
            "2.3373775482177734\n",
            "2.3940343856811523\n",
            "2.411553382873535\n",
            "2.5154354572296143\n",
            "2.5552985668182373\n",
            "2.407665967941284\n",
            "2.457515239715576\n",
            "2.3876824378967285\n",
            "2.464002847671509\n",
            "2.509197473526001\n",
            "2.3209166526794434\n",
            "2.5786495208740234\n",
            "2.5389668941497803\n",
            "2.498932361602783\n",
            "2.422935724258423\n",
            "2.5574557781219482\n",
            "2.5073792934417725\n",
            "2.53366756439209\n",
            "2.2840652465820312\n",
            "2.4703240394592285\n",
            "2.4732825756073\n",
            "2.452977180480957\n",
            "2.429628610610962\n",
            "2.2847065925598145\n",
            "2.5126214027404785\n",
            "2.495377779006958\n",
            "2.662686824798584\n",
            "2.4832422733306885\n",
            "2.5218636989593506\n",
            "2.5692503452301025\n",
            "2.484137773513794\n",
            "2.4528534412384033\n",
            "2.411196708679199\n",
            "2.5541224479675293\n",
            "2.398286819458008\n",
            "2.4436593055725098\n",
            "2.353940486907959\n",
            "2.34321928024292\n",
            "2.44153094291687\n",
            "2.5970003604888916\n",
            "2.4466872215270996\n",
            "2.511667013168335\n",
            "2.448204278945923\n",
            "2.557291269302368\n",
            "2.4570963382720947\n",
            "2.4410531520843506\n",
            "2.555143117904663\n",
            "2.41078519821167\n",
            "2.4378364086151123\n",
            "2.564074993133545\n",
            "2.4301300048828125\n",
            "2.5436980724334717\n",
            "2.4764249324798584\n",
            "2.4766061305999756\n",
            "2.43513822555542\n",
            "2.3639633655548096\n",
            "2.522937536239624\n",
            "2.523423194885254\n",
            "2.3282761573791504\n",
            "2.423640489578247\n",
            "2.3606555461883545\n",
            "2.4586503505706787\n",
            "2.567087411880493\n",
            "2.470825433731079\n",
            "2.4542925357818604\n",
            "2.3858554363250732\n",
            "2.4422566890716553\n",
            "2.5834856033325195\n",
            "2.4553451538085938\n",
            "2.386397123336792\n",
            "2.4234988689422607\n",
            "2.3518757820129395\n",
            "2.6233084201812744\n",
            "2.4572696685791016\n",
            "2.4514009952545166\n",
            "2.470046281814575\n",
            "2.4329113960266113\n",
            "2.446751356124878\n",
            "2.421532392501831\n",
            "2.6613709926605225\n",
            "2.5325429439544678\n",
            "2.579101800918579\n",
            "2.4891703128814697\n",
            "2.3638741970062256\n",
            "2.3717968463897705\n",
            "2.3907933235168457\n",
            "2.3193013668060303\n",
            "2.4925615787506104\n",
            "2.4449374675750732\n",
            "2.4606823921203613\n",
            "2.4694125652313232\n",
            "2.422400951385498\n",
            "2.512925863265991\n",
            "2.5828280448913574\n",
            "2.5022943019866943\n",
            "2.4381844997406006\n",
            "2.4269566535949707\n",
            "2.512827157974243\n",
            "2.3334054946899414\n",
            "2.489482879638672\n",
            "2.4789507389068604\n",
            "2.5580637454986572\n",
            "2.5254979133605957\n",
            "2.35960054397583\n",
            "2.5088274478912354\n",
            "2.395711898803711\n",
            "2.3990654945373535\n",
            "2.5125529766082764\n",
            "2.3235862255096436\n",
            "2.4743363857269287\n",
            "2.5286359786987305\n",
            "2.4596402645111084\n",
            "2.4146852493286133\n",
            "2.355905771255493\n",
            "2.4949803352355957\n",
            "2.5518531799316406\n",
            "2.539428949356079\n",
            "2.327791929244995\n",
            "2.4828176498413086\n",
            "2.4890456199645996\n",
            "2.481830358505249\n",
            "2.4086153507232666\n",
            "2.4219632148742676\n",
            "2.5099432468414307\n",
            "2.360969305038452\n",
            "2.4730238914489746\n",
            "2.5645394325256348\n",
            "2.504286766052246\n",
            "2.480912923812866\n",
            "2.4589269161224365\n",
            "2.4648149013519287\n",
            "2.435786008834839\n",
            "2.4228336811065674\n",
            "2.4489972591400146\n",
            "2.5107266902923584\n",
            "2.49489426612854\n",
            "2.411904811859131\n",
            "2.5291082859039307\n",
            "2.4446544647216797\n",
            "2.484480142593384\n",
            "2.652940273284912\n",
            "2.576477289199829\n",
            "2.3732402324676514\n",
            "2.435529947280884\n",
            "2.539008378982544\n",
            "2.368457078933716\n",
            "2.4949591159820557\n",
            "2.5659496784210205\n",
            "2.4832074642181396\n",
            "2.4774322509765625\n",
            "2.4973158836364746\n",
            "2.4064135551452637\n",
            "2.4467360973358154\n",
            "2.4394545555114746\n",
            "2.617337226867676\n",
            "2.4590792655944824\n",
            "2.5509684085845947\n",
            "2.588209390640259\n",
            "2.503628730773926\n",
            "2.500288963317871\n",
            "2.433372974395752\n",
            "2.49601149559021\n",
            "2.4649300575256348\n",
            "2.4618425369262695\n",
            "2.499021530151367\n",
            "2.601628303527832\n",
            "2.4741196632385254\n",
            "2.3918018341064453\n",
            "2.471710205078125\n",
            "2.5015439987182617\n",
            "2.462360382080078\n",
            "2.5253727436065674\n",
            "2.406834840774536\n",
            "2.46991229057312\n",
            "2.4647433757781982\n",
            "2.439304828643799\n",
            "2.433985710144043\n",
            "2.4050469398498535\n",
            "2.51509690284729\n",
            "2.391963481903076\n",
            "2.4858977794647217\n",
            "2.524726390838623\n",
            "2.5719988346099854\n",
            "2.4938628673553467\n",
            "2.372462272644043\n",
            "2.600205659866333\n",
            "2.5289156436920166\n",
            "2.4770236015319824\n",
            "2.5203373432159424\n",
            "2.643555164337158\n",
            "2.44382905960083\n",
            "2.4805827140808105\n",
            "2.4739959239959717\n",
            "2.433495283126831\n",
            "2.4203405380249023\n",
            "2.458993434906006\n",
            "2.5584046840667725\n",
            "2.5334274768829346\n",
            "2.366828680038452\n",
            "2.4434781074523926\n",
            "2.427276134490967\n",
            "2.5488288402557373\n",
            "2.6405625343322754\n",
            "2.597625732421875\n",
            "2.4183802604675293\n",
            "2.5154759883880615\n",
            "2.4553720951080322\n",
            "2.5687599182128906\n",
            "2.5006110668182373\n",
            "2.434685230255127\n",
            "2.42783260345459\n",
            "2.343719244003296\n",
            "2.3981826305389404\n",
            "2.344876527786255\n",
            "2.4240927696228027\n",
            "2.5306880474090576\n",
            "2.4587910175323486\n",
            "2.5209949016571045\n",
            "2.3464722633361816\n",
            "2.4212136268615723\n",
            "2.4905102252960205\n",
            "2.327951431274414\n",
            "2.4200310707092285\n",
            "2.40128755569458\n",
            "2.4296655654907227\n",
            "2.461899757385254\n",
            "2.4556286334991455\n",
            "2.432129144668579\n",
            "2.335798978805542\n",
            "2.3676881790161133\n",
            "2.420510768890381\n",
            "2.4709551334381104\n",
            "2.4949631690979004\n",
            "2.3726797103881836\n",
            "2.478666305541992\n",
            "2.4019367694854736\n",
            "2.5859644412994385\n",
            "2.5968663692474365\n",
            "2.5858945846557617\n",
            "2.510563611984253\n",
            "2.425351858139038\n",
            "2.5627176761627197\n",
            "2.3211746215820312\n",
            "2.4352948665618896\n",
            "2.4500668048858643\n",
            "2.412529468536377\n",
            "2.420707941055298\n",
            "2.4337215423583984\n",
            "2.6238303184509277\n",
            "2.528216600418091\n",
            "2.646623373031616\n",
            "2.362806797027588\n",
            "2.4599127769470215\n",
            "2.3756542205810547\n",
            "2.431438446044922\n",
            "2.4746105670928955\n",
            "2.456395387649536\n",
            "2.6407060623168945\n",
            "2.3784735202789307\n",
            "2.4875330924987793\n",
            "2.3890151977539062\n",
            "2.4717330932617188\n",
            "2.573770046234131\n",
            "2.4619789123535156\n",
            "2.5024776458740234\n",
            "2.4420387744903564\n",
            "2.508274555206299\n",
            "2.5246191024780273\n",
            "2.530200719833374\n",
            "2.480325937271118\n",
            "2.475459575653076\n",
            "2.4400405883789062\n",
            "2.3567662239074707\n",
            "2.4125986099243164\n",
            "2.428529739379883\n",
            "2.496124744415283\n",
            "2.5661113262176514\n",
            "2.6012461185455322\n",
            "2.4379220008850098\n",
            "2.37965989112854\n",
            "2.457129955291748\n",
            "2.44927716255188\n",
            "2.42191743850708\n",
            "2.547757148742676\n",
            "2.5058422088623047\n",
            "2.347484588623047\n",
            "2.5785880088806152\n",
            "2.6476223468780518\n",
            "2.6225173473358154\n",
            "2.5592539310455322\n",
            "2.5039288997650146\n",
            "2.3378443717956543\n",
            "2.376020669937134\n",
            "2.48307204246521\n",
            "2.519322633743286\n",
            "2.5065741539001465\n",
            "2.645730495452881\n",
            "2.440688133239746\n",
            "2.4477906227111816\n",
            "2.6875195503234863\n",
            "2.3797245025634766\n",
            "2.572802782058716\n",
            "2.6175262928009033\n",
            "2.4804513454437256\n",
            "2.427910089492798\n",
            "2.4342153072357178\n",
            "2.4706597328186035\n",
            "2.424206495285034\n",
            "2.4735400676727295\n",
            "2.5484726428985596\n",
            "2.4887940883636475\n",
            "2.4353644847869873\n",
            "2.4727463722229004\n",
            "2.6360504627227783\n",
            "2.3854422569274902\n",
            "2.486067771911621\n",
            "2.4648597240448\n",
            "2.429884672164917\n",
            "2.4999289512634277\n",
            "2.4382171630859375\n",
            "2.4332454204559326\n",
            "2.4555540084838867\n",
            "2.548720359802246\n",
            "2.451385259628296\n",
            "2.4028072357177734\n",
            "2.458366632461548\n",
            "2.4534289836883545\n",
            "2.3930580615997314\n",
            "2.536480188369751\n",
            "2.3234329223632812\n",
            "2.552243947982788\n",
            "2.3179800510406494\n",
            "2.3615758419036865\n",
            "2.3374571800231934\n",
            "2.5186944007873535\n",
            "2.4742307662963867\n",
            "2.4744656085968018\n",
            "2.4018263816833496\n",
            "2.528934955596924\n",
            "2.5128724575042725\n",
            "2.4735183715820312\n",
            "2.470655918121338\n",
            "2.5303585529327393\n",
            "2.483983278274536\n",
            "2.485773801803589\n",
            "2.455441474914551\n",
            "2.451413869857788\n",
            "2.458016872406006\n",
            "2.4540820121765137\n",
            "2.447103261947632\n",
            "2.41759991645813\n",
            "2.5311896800994873\n",
            "2.4831113815307617\n",
            "2.5025699138641357\n",
            "2.442617893218994\n",
            "2.3784000873565674\n",
            "2.4343814849853516\n",
            "2.458333730697632\n",
            "2.449526309967041\n",
            "2.4861128330230713\n",
            "2.458017110824585\n",
            "2.4383931159973145\n",
            "2.453352928161621\n",
            "2.343153476715088\n",
            "2.47385573387146\n",
            "2.460148572921753\n",
            "2.490190029144287\n",
            "2.4117166996002197\n",
            "2.543224811553955\n",
            "2.5411250591278076\n",
            "2.6675021648406982\n",
            "2.470313549041748\n",
            "2.5966060161590576\n",
            "2.435472011566162\n",
            "2.5487656593322754\n",
            "2.4322760105133057\n",
            "2.442340850830078\n",
            "2.378324031829834\n",
            "2.4975621700286865\n",
            "2.3959362506866455\n",
            "2.4844014644622803\n",
            "2.5366263389587402\n",
            "2.5597243309020996\n",
            "2.437404155731201\n",
            "2.4829699993133545\n",
            "2.513925313949585\n",
            "2.404348850250244\n",
            "2.5528738498687744\n",
            "2.470087766647339\n",
            "2.558140277862549\n",
            "2.570988655090332\n",
            "2.6032252311706543\n",
            "2.5019659996032715\n",
            "2.5849316120147705\n",
            "2.5095739364624023\n",
            "2.4993629455566406\n",
            "2.4232990741729736\n",
            "2.662203311920166\n",
            "2.4504477977752686\n",
            "2.4096524715423584\n",
            "2.4781367778778076\n",
            "2.3856289386749268\n",
            "2.4009430408477783\n",
            "2.406196117401123\n",
            "2.4818763732910156\n",
            "2.416280508041382\n",
            "2.5361623764038086\n",
            "2.4570822715759277\n",
            "2.3328592777252197\n",
            "2.576591968536377\n",
            "2.5684328079223633\n",
            "2.549927234649658\n",
            "2.495976448059082\n",
            "2.497812271118164\n",
            "2.4461073875427246\n",
            "2.3906471729278564\n",
            "2.516814947128296\n",
            "2.4804117679595947\n",
            "2.4372241497039795\n",
            "2.435742139816284\n",
            "2.549131155014038\n",
            "2.514680862426758\n",
            "2.4901673793792725\n",
            "2.5346362590789795\n",
            "2.4369750022888184\n",
            "2.485072612762451\n",
            "2.4551475048065186\n",
            "2.390889883041382\n",
            "2.407522201538086\n",
            "2.490565538406372\n",
            "2.427656650543213\n",
            "2.4886138439178467\n",
            "2.6414804458618164\n",
            "2.542109727859497\n",
            "2.3436503410339355\n",
            "2.5487866401672363\n",
            "2.399336099624634\n",
            "2.446129560470581\n",
            "2.4967494010925293\n",
            "2.357821226119995\n",
            "2.511281967163086\n",
            "2.43381404876709\n",
            "2.409611225128174\n",
            "2.4748926162719727\n",
            "2.406904458999634\n",
            "2.442042112350464\n",
            "2.4093968868255615\n",
            "2.524441957473755\n",
            "2.61240553855896\n",
            "2.648552656173706\n",
            "2.313817024230957\n",
            "2.422773599624634\n",
            "2.6410253047943115\n",
            "2.4222986698150635\n",
            "2.406137704849243\n",
            "2.515925884246826\n",
            "2.364309549331665\n",
            "2.491689920425415\n",
            "2.5474131107330322\n",
            "2.4902114868164062\n",
            "2.475482940673828\n",
            "2.42244029045105\n",
            "2.386786699295044\n",
            "2.5114219188690186\n",
            "2.567920446395874\n",
            "2.492219924926758\n",
            "2.450315475463867\n",
            "2.5619962215423584\n",
            "2.3089890480041504\n",
            "2.367194175720215\n",
            "2.3196237087249756\n",
            "2.4088523387908936\n",
            "2.4779937267303467\n",
            "2.4648866653442383\n",
            "2.4649343490600586\n",
            "2.4794135093688965\n",
            "2.4400813579559326\n",
            "2.4481232166290283\n",
            "2.3752081394195557\n",
            "2.5576436519622803\n",
            "2.5021042823791504\n",
            "2.336416244506836\n",
            "2.4542694091796875\n",
            "2.551051139831543\n",
            "2.484234571456909\n",
            "2.4525270462036133\n",
            "2.590576648712158\n",
            "2.559250593185425\n",
            "2.4890828132629395\n",
            "2.4463911056518555\n",
            "2.4696826934814453\n",
            "2.430824041366577\n",
            "2.414053440093994\n",
            "2.509793996810913\n",
            "2.508725881576538\n",
            "2.359248399734497\n",
            "2.517174243927002\n",
            "2.4466190338134766\n",
            "2.497532606124878\n",
            "2.567600965499878\n",
            "2.4884111881256104\n",
            "2.428093194961548\n",
            "2.4660494327545166\n",
            "2.558734893798828\n",
            "2.3635306358337402\n",
            "2.4105541706085205\n",
            "2.4197025299072266\n",
            "2.3960447311401367\n",
            "2.6238842010498047\n",
            "2.5253326892852783\n",
            "2.2887508869171143\n",
            "2.5368895530700684\n",
            "2.619398832321167\n",
            "2.4173998832702637\n",
            "2.549476385116577\n",
            "2.4653968811035156\n",
            "2.3896195888519287\n",
            "2.4393773078918457\n",
            "2.426485061645508\n",
            "2.644117593765259\n",
            "2.387983560562134\n",
            "2.45029354095459\n",
            "2.3670337200164795\n",
            "2.393690347671509\n",
            "2.4192447662353516\n",
            "2.408865451812744\n",
            "2.4860546588897705\n",
            "2.3546342849731445\n",
            "2.353104829788208\n",
            "2.447753667831421\n",
            "2.4006850719451904\n",
            "2.465926170349121\n",
            "2.4272360801696777\n",
            "2.491281747817993\n",
            "2.4520318508148193\n",
            "2.5231919288635254\n",
            "2.3908512592315674\n",
            "2.539029836654663\n",
            "2.4414761066436768\n",
            "2.4630675315856934\n",
            "2.4001924991607666\n",
            "2.6128036975860596\n",
            "2.6233930587768555\n",
            "2.48551082611084\n",
            "2.509685516357422\n",
            "2.3558003902435303\n",
            "2.5903923511505127\n",
            "2.545724630355835\n",
            "2.4987854957580566\n",
            "2.474722385406494\n",
            "2.3505306243896484\n",
            "2.352071523666382\n",
            "2.5239675045013428\n",
            "2.5370233058929443\n",
            "2.5635998249053955\n",
            "2.393843412399292\n",
            "2.4062917232513428\n",
            "2.4542763233184814\n",
            "2.4204342365264893\n",
            "2.491680860519409\n",
            "2.510612964630127\n",
            "2.4209094047546387\n",
            "2.4648938179016113\n",
            "2.3847873210906982\n",
            "2.512279987335205\n",
            "2.489440441131592\n",
            "2.5569045543670654\n",
            "2.4374442100524902\n",
            "2.5508079528808594\n",
            "2.3974525928497314\n",
            "2.4343268871307373\n",
            "2.4460127353668213\n",
            "2.620919704437256\n",
            "2.479395866394043\n",
            "2.391366958618164\n",
            "2.491487741470337\n",
            "2.474870443344116\n",
            "2.385946750640869\n",
            "2.4853603839874268\n",
            "2.470820188522339\n",
            "2.4875659942626953\n",
            "2.5328845977783203\n",
            "2.5023739337921143\n",
            "2.4000370502471924\n",
            "2.401120901107788\n",
            "2.464231491088867\n",
            "2.458202838897705\n",
            "2.3686561584472656\n",
            "2.372171401977539\n",
            "2.4268288612365723\n",
            "2.3315086364746094\n",
            "2.504497766494751\n",
            "2.4543564319610596\n",
            "2.6237576007843018\n",
            "2.374433755874634\n",
            "2.4993655681610107\n",
            "2.564439058303833\n",
            "2.515634536743164\n",
            "2.4159324169158936\n",
            "2.4792401790618896\n",
            "2.40761137008667\n",
            "2.47190523147583\n",
            "2.479919195175171\n",
            "2.463761329650879\n",
            "2.412489891052246\n",
            "2.4949121475219727\n",
            "2.5129594802856445\n",
            "2.55582594871521\n",
            "2.459397792816162\n",
            "2.5202934741973877\n",
            "2.4136147499084473\n",
            "2.449206590652466\n",
            "2.4118385314941406\n",
            "2.473003387451172\n",
            "2.4550886154174805\n",
            "2.3895959854125977\n",
            "2.5626566410064697\n",
            "2.446399688720703\n",
            "2.4973437786102295\n",
            "2.309662103652954\n",
            "2.4902870655059814\n",
            "2.4750747680664062\n",
            "2.51521372795105\n",
            "2.5758774280548096\n",
            "2.461082935333252\n",
            "2.5177786350250244\n",
            "2.5076115131378174\n",
            "2.4995172023773193\n",
            "2.3736209869384766\n",
            "2.6682212352752686\n",
            "2.481133460998535\n",
            "2.5581247806549072\n",
            "2.4869515895843506\n",
            "2.415231943130493\n",
            "2.5014140605926514\n",
            "2.566011905670166\n",
            "2.4352822303771973\n",
            "2.485370635986328\n",
            "2.3510375022888184\n",
            "2.455235004425049\n",
            "2.523141384124756\n",
            "2.454634666442871\n",
            "2.484959840774536\n",
            "2.3904178142547607\n",
            "2.5520756244659424\n",
            "2.5326004028320312\n",
            "2.467949628829956\n",
            "2.410358428955078\n",
            "2.3916845321655273\n",
            "2.2954180240631104\n",
            "2.476212978363037\n",
            "2.503394842147827\n",
            "2.435999870300293\n",
            "2.4059929847717285\n",
            "2.3716204166412354\n",
            "2.365496873855591\n",
            "2.4919257164001465\n",
            "2.374945640563965\n",
            "2.6352438926696777\n",
            "2.379974603652954\n",
            "2.402289628982544\n",
            "2.4561924934387207\n",
            "2.4611618518829346\n",
            "2.3598368167877197\n",
            "2.448213577270508\n",
            "2.468695640563965\n",
            "2.417534828186035\n",
            "2.565744161605835\n",
            "2.4206292629241943\n",
            "2.3605926036834717\n",
            "2.357017993927002\n",
            "2.4546170234680176\n",
            "2.576505422592163\n",
            "2.4207327365875244\n",
            "2.497610330581665\n",
            "2.490535020828247\n",
            "2.4430489540100098\n",
            "2.503359079360962\n",
            "2.4518392086029053\n",
            "2.4822795391082764\n",
            "2.444577693939209\n",
            "2.4649529457092285\n",
            "2.456747055053711\n",
            "2.4025065898895264\n",
            "2.298659563064575\n",
            "2.562182903289795\n",
            "2.3468456268310547\n",
            "2.4100382328033447\n",
            "2.5809242725372314\n",
            "2.55291485786438\n",
            "2.3689043521881104\n",
            "2.4230682849884033\n",
            "2.412046432495117\n",
            "2.55141019821167\n",
            "2.484238386154175\n",
            "2.4015653133392334\n",
            "2.503530979156494\n",
            "2.3795063495635986\n",
            "2.4261794090270996\n",
            "2.4823415279388428\n",
            "2.4832913875579834\n",
            "2.520125150680542\n",
            "2.4298243522644043\n",
            "2.3744046688079834\n",
            "2.388333320617676\n",
            "2.435920000076294\n",
            "2.5037660598754883\n",
            "2.544565439224243\n",
            "2.4566478729248047\n",
            "2.4234304428100586\n",
            "2.4472692012786865\n",
            "2.4618778228759766\n",
            "2.4302549362182617\n",
            "2.4397811889648438\n",
            "2.5670409202575684\n",
            "2.5406382083892822\n",
            "2.3123888969421387\n",
            "2.535618782043457\n",
            "2.5070295333862305\n",
            "2.395293951034546\n",
            "2.5693178176879883\n",
            "2.4848415851593018\n",
            "2.4432947635650635\n",
            "2.3327527046203613\n",
            "2.311035394668579\n",
            "2.5360090732574463\n",
            "2.396533966064453\n",
            "2.417579174041748\n",
            "2.5178427696228027\n",
            "2.464611530303955\n",
            "2.476665735244751\n",
            "2.401604413986206\n",
            "2.467017412185669\n",
            "2.4457552433013916\n",
            "2.493844985961914\n",
            "2.5133843421936035\n",
            "2.4449501037597656\n",
            "2.5123069286346436\n",
            "2.3211448192596436\n",
            "2.5303497314453125\n",
            "2.581116199493408\n",
            "2.3742644786834717\n",
            "2.5248348712921143\n",
            "2.4528145790100098\n",
            "2.5553276538848877\n",
            "2.547402858734131\n",
            "2.5768980979919434\n",
            "2.506558656692505\n",
            "2.4784066677093506\n",
            "2.4960625171661377\n",
            "2.396027088165283\n",
            "2.450005531311035\n",
            "2.4419875144958496\n",
            "2.5551812648773193\n",
            "2.4248592853546143\n",
            "2.4948644638061523\n",
            "2.5670604705810547\n",
            "2.4345805644989014\n",
            "2.47998309135437\n",
            "2.5348329544067383\n",
            "2.5110645294189453\n",
            "2.330479621887207\n",
            "2.4016542434692383\n",
            "2.458362340927124\n",
            "2.5316672325134277\n",
            "2.4301154613494873\n",
            "2.5415778160095215\n",
            "2.438558340072632\n",
            "2.378282308578491\n",
            "2.4837300777435303\n",
            "2.3865416049957275\n",
            "2.3729665279388428\n",
            "2.36875581741333\n",
            "2.5231943130493164\n",
            "2.3638312816619873\n",
            "2.3720710277557373\n",
            "2.4286880493164062\n",
            "2.409593105316162\n",
            "2.5493104457855225\n",
            "2.4898016452789307\n",
            "2.600792169570923\n",
            "2.5211551189422607\n",
            "2.4941091537475586\n",
            "2.5031635761260986\n",
            "2.4929778575897217\n",
            "2.556143045425415\n",
            "2.4793078899383545\n",
            "2.4155757427215576\n",
            "2.577773332595825\n",
            "2.539278745651245\n",
            "2.549974203109741\n",
            "2.3581178188323975\n",
            "2.4878220558166504\n",
            "2.3374040126800537\n",
            "2.4082677364349365\n",
            "2.4995505809783936\n",
            "2.3225691318511963\n",
            "2.4465878009796143\n",
            "2.52392840385437\n",
            "2.5681569576263428\n",
            "2.4036192893981934\n",
            "2.4325625896453857\n",
            "2.3027400970458984\n",
            "2.5507495403289795\n",
            "2.437349557876587\n",
            "2.5026421546936035\n",
            "2.4320907592773438\n",
            "2.478806495666504\n",
            "2.468503952026367\n",
            "2.4565505981445312\n",
            "2.424198627471924\n",
            "2.420900344848633\n",
            "2.429816246032715\n",
            "2.474036693572998\n",
            "2.504711151123047\n",
            "2.47025990486145\n",
            "2.498182773590088\n",
            "2.477073907852173\n",
            "2.4334964752197266\n",
            "2.380218744277954\n",
            "2.5380094051361084\n",
            "2.4553022384643555\n",
            "2.5106899738311768\n",
            "2.4085419178009033\n",
            "2.3537862300872803\n",
            "2.5473644733428955\n",
            "2.569944381713867\n",
            "2.40384578704834\n",
            "2.3289453983306885\n",
            "2.4674997329711914\n",
            "2.49580454826355\n",
            "2.5801968574523926\n",
            "2.5745506286621094\n",
            "2.4425222873687744\n",
            "2.4284701347351074\n",
            "2.5462841987609863\n",
            "2.56036639213562\n",
            "2.4601993560791016\n",
            "2.5048668384552\n",
            "2.438774347305298\n",
            "2.564124584197998\n",
            "2.490297794342041\n",
            "2.4375786781311035\n",
            "2.441370725631714\n",
            "2.4816226959228516\n",
            "2.5597829818725586\n",
            "2.4028677940368652\n",
            "2.4382894039154053\n",
            "2.537384033203125\n",
            "2.622715950012207\n",
            "2.3946614265441895\n",
            "2.5693793296813965\n",
            "2.3625664710998535\n",
            "2.50807785987854\n",
            "2.6195242404937744\n",
            "2.3821732997894287\n",
            "2.438368320465088\n",
            "2.425405740737915\n",
            "2.5491583347320557\n",
            "2.3955459594726562\n",
            "2.372770309448242\n",
            "2.4624624252319336\n",
            "2.5591559410095215\n",
            "2.494609832763672\n",
            "2.552640914916992\n",
            "2.4597065448760986\n",
            "2.4571409225463867\n",
            "2.4295949935913086\n",
            "2.4692418575286865\n",
            "2.485548257827759\n",
            "2.5266101360321045\n",
            "2.3552727699279785\n",
            "2.401157855987549\n",
            "2.3414766788482666\n",
            "2.4191253185272217\n",
            "2.433495283126831\n",
            "2.4139344692230225\n",
            "2.4244649410247803\n",
            "2.561357259750366\n",
            "2.507620096206665\n",
            "2.395040512084961\n",
            "2.4497287273406982\n",
            "2.5094666481018066\n",
            "2.486868381500244\n",
            "2.4932799339294434\n",
            "2.4922313690185547\n",
            "2.6519994735717773\n",
            "2.454296588897705\n",
            "2.4661996364593506\n",
            "2.6232900619506836\n",
            "2.520569086074829\n",
            "2.3993659019470215\n",
            "2.415332317352295\n",
            "2.457190990447998\n",
            "2.333407402038574\n",
            "2.3872883319854736\n",
            "2.3633182048797607\n",
            "2.4586760997772217\n",
            "2.474114179611206\n",
            "2.3992624282836914\n",
            "2.415759563446045\n",
            "2.3905959129333496\n",
            "2.5007681846618652\n",
            "2.3241937160491943\n",
            "2.383859395980835\n",
            "2.3981356620788574\n",
            "2.4607694149017334\n",
            "2.4900224208831787\n",
            "2.4578909873962402\n",
            "2.4720401763916016\n",
            "2.3483998775482178\n",
            "2.499624252319336\n",
            "2.4598312377929688\n",
            "2.4032461643218994\n",
            "2.488159656524658\n",
            "2.4891979694366455\n",
            "2.543983221054077\n",
            "2.3978288173675537\n",
            "2.3397862911224365\n",
            "2.394651174545288\n",
            "2.5347938537597656\n",
            "2.46451473236084\n",
            "2.3728208541870117\n",
            "2.3702642917633057\n",
            "2.40193772315979\n",
            "2.3637096881866455\n",
            "2.425662040710449\n",
            "2.568427324295044\n",
            "2.4165332317352295\n",
            "2.3892810344696045\n",
            "2.4633564949035645\n",
            "2.4611902236938477\n",
            "2.5394692420959473\n",
            "2.410956621170044\n",
            "2.505584716796875\n",
            "2.344438076019287\n",
            "2.520963191986084\n",
            "2.465808391571045\n",
            "2.388672351837158\n",
            "2.3953511714935303\n",
            "2.443108081817627\n",
            "2.437762975692749\n",
            "2.4398841857910156\n",
            "2.503754138946533\n",
            "2.5532281398773193\n",
            "2.381261110305786\n",
            "2.3904364109039307\n",
            "2.503666877746582\n",
            "2.4209578037261963\n",
            "2.343113899230957\n",
            "2.3668527603149414\n",
            "2.544719696044922\n",
            "2.568429470062256\n",
            "2.409492015838623\n",
            "2.570237159729004\n",
            "2.470672130584717\n",
            "2.441702127456665\n",
            "2.5103535652160645\n",
            "2.5362796783447266\n",
            "2.521235704421997\n",
            "2.4195783138275146\n",
            "2.4980621337890625\n",
            "2.5032973289489746\n",
            "2.4178104400634766\n",
            "2.433875799179077\n",
            "2.3616042137145996\n",
            "2.602478265762329\n",
            "2.6336653232574463\n",
            "2.436859607696533\n",
            "2.5064451694488525\n",
            "2.3949573040008545\n",
            "2.4278595447540283\n",
            "2.4479928016662598\n",
            "2.3765594959259033\n",
            "2.458026885986328\n",
            "2.459136486053467\n",
            "2.4259231090545654\n",
            "2.5040717124938965\n",
            "2.514084815979004\n",
            "2.412590503692627\n",
            "2.4717860221862793\n",
            "2.3325130939483643\n",
            "2.432976722717285\n",
            "2.608677864074707\n",
            "2.594649314880371\n",
            "2.4094228744506836\n",
            "2.3750686645507812\n",
            "2.338620662689209\n",
            "2.565269947052002\n",
            "2.4183101654052734\n",
            "2.5253472328186035\n",
            "2.372727632522583\n",
            "2.5443785190582275\n",
            "2.531414270401001\n",
            "2.5734493732452393\n",
            "2.39491605758667\n",
            "2.447951316833496\n",
            "2.5371010303497314\n",
            "2.4847967624664307\n",
            "2.432185649871826\n",
            "2.330756187438965\n",
            "2.468702793121338\n",
            "2.4337897300720215\n",
            "2.3394227027893066\n",
            "2.4179534912109375\n",
            "2.4196150302886963\n",
            "2.622769594192505\n",
            "2.3442869186401367\n",
            "2.5534253120422363\n",
            "2.464925765991211\n",
            "2.4735448360443115\n",
            "2.503159999847412\n",
            "2.553347587585449\n",
            "2.4186792373657227\n",
            "2.6159586906433105\n",
            "2.5091304779052734\n",
            "2.523954153060913\n",
            "2.4957516193389893\n",
            "2.488614797592163\n",
            "2.4102272987365723\n",
            "2.629908323287964\n",
            "2.5029666423797607\n",
            "2.5093259811401367\n",
            "2.4237945079803467\n",
            "2.4833741188049316\n",
            "2.371380090713501\n",
            "2.4631543159484863\n",
            "2.541846513748169\n",
            "2.5825963020324707\n",
            "2.431004047393799\n",
            "2.5222816467285156\n",
            "2.5400822162628174\n",
            "2.556201934814453\n",
            "2.393646478652954\n",
            "2.410172939300537\n",
            "2.482091188430786\n",
            "2.536822557449341\n",
            "2.5762455463409424\n",
            "2.374152421951294\n",
            "2.439857244491577\n",
            "2.3246347904205322\n",
            "2.537339687347412\n",
            "2.451303720474243\n",
            "2.6588382720947266\n",
            "2.413551092147827\n",
            "2.4849233627319336\n",
            "2.5812888145446777\n",
            "2.4935688972473145\n",
            "2.5675208568573\n",
            "2.510035276412964\n",
            "2.3992817401885986\n",
            "2.559278726577759\n",
            "2.402015447616577\n",
            "2.445755958557129\n",
            "2.5935044288635254\n",
            "2.449803113937378\n",
            "2.343998908996582\n",
            "2.4356791973114014\n",
            "2.447108745574951\n",
            "2.572505235671997\n",
            "2.515733480453491\n",
            "2.5002410411834717\n",
            "2.469818592071533\n",
            "2.555051326751709\n",
            "2.5851800441741943\n",
            "2.4101345539093018\n",
            "2.473804473876953\n",
            "2.372860908508301\n",
            "2.453733205795288\n",
            "2.605073928833008\n",
            "2.4826786518096924\n",
            "2.5231378078460693\n",
            "2.543416738510132\n",
            "2.5185539722442627\n",
            "2.550226926803589\n",
            "2.563922643661499\n",
            "2.328526258468628\n",
            "2.4729795455932617\n",
            "2.509688377380371\n",
            "2.4733364582061768\n",
            "2.4238522052764893\n",
            "2.6056926250457764\n",
            "2.6480138301849365\n",
            "2.4080262184143066\n",
            "2.434863567352295\n",
            "2.4529848098754883\n",
            "2.5177981853485107\n",
            "2.4521687030792236\n",
            "2.4285902976989746\n",
            "2.492647647857666\n",
            "2.5163347721099854\n",
            "2.578115463256836\n",
            "2.3079237937927246\n",
            "2.4153857231140137\n",
            "2.608154773712158\n",
            "2.5546038150787354\n",
            "2.424107074737549\n",
            "2.4483842849731445\n",
            "2.4567277431488037\n",
            "2.4822371006011963\n",
            "2.4990200996398926\n",
            "2.5550310611724854\n",
            "2.5512731075286865\n",
            "2.518679618835449\n",
            "2.4800028800964355\n",
            "2.5392379760742188\n",
            "2.45863938331604\n",
            "2.4150102138519287\n",
            "2.4735751152038574\n",
            "2.5310678482055664\n",
            "2.4257540702819824\n",
            "2.4794130325317383\n",
            "2.5433032512664795\n",
            "2.3760178089141846\n",
            "2.463677167892456\n",
            "2.5153894424438477\n",
            "2.6545939445495605\n",
            "2.468691825866699\n",
            "2.470510482788086\n",
            "2.435729742050171\n",
            "2.4545650482177734\n",
            "2.4744157791137695\n",
            "2.549420118331909\n",
            "2.4429738521575928\n",
            "2.4381065368652344\n",
            "2.443626880645752\n",
            "2.393287420272827\n",
            "2.4949426651000977\n",
            "2.5737621784210205\n",
            "2.533543586730957\n",
            "2.5124969482421875\n",
            "2.4958016872406006\n",
            "2.4225502014160156\n",
            "2.399052143096924\n",
            "2.518303155899048\n",
            "2.4245660305023193\n",
            "2.492727041244507\n",
            "2.504206895828247\n",
            "2.3961069583892822\n",
            "2.4522814750671387\n",
            "2.35756516456604\n",
            "2.493208408355713\n",
            "2.5433433055877686\n",
            "2.499228000640869\n",
            "2.4788553714752197\n",
            "2.585097074508667\n",
            "2.5289063453674316\n",
            "2.446528196334839\n",
            "2.482229232788086\n",
            "2.434300661087036\n",
            "2.530454158782959\n",
            "2.445737838745117\n",
            "2.540555715560913\n",
            "2.512009620666504\n",
            "2.461228847503662\n",
            "2.479421854019165\n",
            "2.431817054748535\n",
            "2.407755136489868\n",
            "2.4019007682800293\n",
            "2.459902048110962\n",
            "2.5292954444885254\n",
            "2.4577994346618652\n",
            "2.5573782920837402\n",
            "2.431605100631714\n",
            "2.4565634727478027\n",
            "2.4671549797058105\n",
            "2.3896408081054688\n",
            "2.3943300247192383\n",
            "2.311472177505493\n",
            "2.6644322872161865\n",
            "2.5238876342773438\n",
            "2.480429172515869\n",
            "2.51958966255188\n",
            "2.5475733280181885\n",
            "2.343947649002075\n",
            "2.5084710121154785\n",
            "2.356414318084717\n",
            "2.319547653198242\n",
            "2.3700573444366455\n",
            "2.5507314205169678\n",
            "2.485196590423584\n",
            "2.389225721359253\n",
            "2.5656137466430664\n",
            "2.4375784397125244\n",
            "2.4228971004486084\n",
            "2.506958246231079\n",
            "2.4480981826782227\n",
            "2.5090346336364746\n",
            "2.551626205444336\n",
            "2.4740641117095947\n",
            "2.3242571353912354\n",
            "2.572265863418579\n",
            "2.492307662963867\n",
            "2.418038845062256\n",
            "2.485841989517212\n",
            "2.4769792556762695\n",
            "2.496089458465576\n",
            "2.528988838195801\n",
            "2.362931728363037\n",
            "2.5350303649902344\n",
            "2.469261407852173\n",
            "2.4507243633270264\n",
            "2.399291753768921\n",
            "2.4146199226379395\n",
            "2.6443445682525635\n",
            "2.4508049488067627\n",
            "2.433945894241333\n",
            "2.4364404678344727\n",
            "2.442647695541382\n",
            "2.531111240386963\n",
            "2.4486167430877686\n",
            "2.5420475006103516\n",
            "2.391090154647827\n",
            "2.4741711616516113\n",
            "2.5838496685028076\n",
            "2.502471685409546\n",
            "2.4780242443084717\n",
            "2.486189365386963\n",
            "2.3409857749938965\n",
            "2.4553306102752686\n",
            "2.4635870456695557\n",
            "2.471724510192871\n",
            "2.4437122344970703\n",
            "2.432319164276123\n",
            "2.5264205932617188\n",
            "2.381742000579834\n",
            "2.4220635890960693\n",
            "2.481518268585205\n",
            "2.4928369522094727\n",
            "2.501645565032959\n",
            "2.3560068607330322\n",
            "2.3990814685821533\n",
            "2.5045249462127686\n",
            "2.3798165321350098\n",
            "2.5338783264160156\n",
            "2.442525863647461\n",
            "2.453784465789795\n",
            "2.5176703929901123\n",
            "2.4066927433013916\n",
            "2.4198737144470215\n",
            "2.550875186920166\n",
            "2.3126027584075928\n",
            "2.5483696460723877\n",
            "2.532562494277954\n",
            "2.3311922550201416\n",
            "2.3768537044525146\n",
            "2.4002902507781982\n",
            "2.3842337131500244\n",
            "2.3933515548706055\n",
            "2.4680724143981934\n",
            "2.3470120429992676\n",
            "2.4488935470581055\n",
            "2.564664840698242\n",
            "2.5211398601531982\n",
            "2.43495512008667\n",
            "2.338421583175659\n",
            "2.4337399005889893\n",
            "2.6420702934265137\n",
            "2.5402281284332275\n",
            "2.414785146713257\n",
            "2.5148189067840576\n",
            "2.402468681335449\n",
            "2.4481542110443115\n",
            "2.4930174350738525\n",
            "2.512664318084717\n",
            "2.464799642562866\n",
            "2.520653247833252\n",
            "2.602405309677124\n",
            "2.3638086318969727\n",
            "2.3975558280944824\n",
            "2.545111656188965\n",
            "2.4559037685394287\n",
            "2.4432363510131836\n",
            "2.534315347671509\n",
            "2.313258647918701\n",
            "2.4014389514923096\n",
            "2.4884138107299805\n",
            "2.505084991455078\n",
            "2.32619571685791\n",
            "2.463798999786377\n",
            "2.4246602058410645\n",
            "2.6499555110931396\n",
            "2.326856851577759\n",
            "2.426842212677002\n",
            "2.299203395843506\n",
            "2.442704916000366\n",
            "2.4221110343933105\n",
            "2.49959397315979\n",
            "2.497650623321533\n",
            "2.2702155113220215\n",
            "2.4853270053863525\n",
            "2.5235705375671387\n",
            "2.392181873321533\n",
            "2.4882822036743164\n",
            "2.386805772781372\n",
            "2.4131290912628174\n",
            "2.519613265991211\n",
            "2.4591643810272217\n",
            "2.49979305267334\n",
            "2.421386957168579\n",
            "2.563598871231079\n",
            "2.4792399406433105\n",
            "2.4267661571502686\n",
            "2.4685113430023193\n",
            "2.3802483081817627\n",
            "2.4335010051727295\n",
            "2.371965169906616\n",
            "2.4198057651519775\n",
            "2.440565586090088\n",
            "2.4464898109436035\n",
            "2.4811127185821533\n",
            "2.4866604804992676\n",
            "2.5019264221191406\n",
            "2.4699716567993164\n",
            "2.4595441818237305\n",
            "2.5681169033050537\n",
            "2.4042136669158936\n",
            "2.4607961177825928\n",
            "2.4406239986419678\n",
            "2.5542306900024414\n",
            "2.466203212738037\n",
            "2.498652935028076\n",
            "2.4398293495178223\n",
            "2.4300014972686768\n",
            "2.547138214111328\n",
            "2.5722599029541016\n",
            "2.5156564712524414\n",
            "2.453174591064453\n",
            "2.4264185428619385\n",
            "2.449859857559204\n",
            "2.439075231552124\n",
            "2.528393268585205\n",
            "2.369459390640259\n",
            "2.3300650119781494\n",
            "2.4094431400299072\n",
            "2.4381115436553955\n",
            "2.4085323810577393\n",
            "2.3513941764831543\n",
            "2.586422920227051\n",
            "2.478677988052368\n",
            "2.5075950622558594\n",
            "2.6144802570343018\n",
            "2.43963885307312\n",
            "2.463768243789673\n",
            "2.51910400390625\n",
            "2.414393663406372\n",
            "2.471022367477417\n",
            "2.4818661212921143\n",
            "2.3381898403167725\n",
            "2.3557865619659424\n",
            "2.293242931365967\n",
            "2.4572291374206543\n",
            "2.4310765266418457\n",
            "2.4639055728912354\n",
            "2.5381531715393066\n",
            "2.532775640487671\n",
            "2.591639757156372\n",
            "2.513139009475708\n",
            "2.4777824878692627\n",
            "2.3460872173309326\n",
            "2.473529577255249\n",
            "2.561790704727173\n",
            "2.385026454925537\n",
            "2.3068792819976807\n",
            "2.480499744415283\n",
            "2.561160087585449\n",
            "2.401580572128296\n",
            "2.4352762699127197\n",
            "2.4777169227600098\n",
            "2.524988889694214\n",
            "2.4021096229553223\n",
            "2.6348209381103516\n",
            "2.5123965740203857\n",
            "2.398375988006592\n",
            "2.4083826541900635\n",
            "2.4837381839752197\n",
            "2.4069035053253174\n",
            "2.5049121379852295\n",
            "2.5251269340515137\n",
            "2.4717559814453125\n",
            "2.388019323348999\n",
            "2.376901388168335\n",
            "2.4160497188568115\n",
            "2.400078058242798\n",
            "2.5386486053466797\n",
            "2.560115098953247\n",
            "2.571669816970825\n",
            "2.5277271270751953\n",
            "2.6069231033325195\n",
            "2.5246031284332275\n",
            "2.3354179859161377\n",
            "2.5338668823242188\n",
            "2.4632039070129395\n",
            "2.55533766746521\n",
            "2.3634543418884277\n",
            "2.5498225688934326\n",
            "2.572315216064453\n",
            "2.4707727432250977\n",
            "2.5146825313568115\n",
            "2.4357306957244873\n",
            "2.3691766262054443\n",
            "2.4977526664733887\n",
            "2.5212745666503906\n",
            "2.321523666381836\n",
            "2.3161461353302\n",
            "2.543851137161255\n",
            "2.5887229442596436\n",
            "2.4275407791137695\n",
            "2.4310851097106934\n",
            "2.442960500717163\n",
            "2.5223913192749023\n",
            "2.404233694076538\n",
            "2.522278308868408\n",
            "2.445660352706909\n",
            "2.3491294384002686\n",
            "2.471553087234497\n",
            "2.389031171798706\n",
            "2.5612785816192627\n",
            "2.4058375358581543\n",
            "2.516733169555664\n",
            "2.414930582046509\n",
            "2.494685649871826\n",
            "2.473771572113037\n",
            "2.5425527095794678\n",
            "2.375225305557251\n",
            "2.4375102519989014\n",
            "2.495875597000122\n",
            "2.5169923305511475\n",
            "2.662914514541626\n",
            "2.4891598224639893\n",
            "2.4010283946990967\n",
            "2.4002997875213623\n",
            "2.518028974533081\n",
            "2.398705244064331\n",
            "2.5836029052734375\n",
            "2.4555580615997314\n",
            "2.450167417526245\n",
            "2.4345719814300537\n",
            "2.47355580329895\n",
            "2.563764810562134\n",
            "2.4989709854125977\n",
            "2.5207695960998535\n",
            "2.5178823471069336\n",
            "2.4878222942352295\n",
            "2.328291177749634\n",
            "2.472022533416748\n",
            "2.4093990325927734\n",
            "2.3844687938690186\n",
            "2.5577445030212402\n",
            "2.516355276107788\n",
            "2.4412858486175537\n",
            "2.4083309173583984\n",
            "2.5437886714935303\n",
            "2.4750373363494873\n",
            "2.686204433441162\n",
            "2.639214515686035\n",
            "2.5536954402923584\n",
            "2.602083921432495\n",
            "2.480520248413086\n",
            "2.522305488586426\n",
            "2.4666924476623535\n",
            "2.4377946853637695\n",
            "2.423511505126953\n",
            "2.4793355464935303\n",
            "2.4100334644317627\n",
            "2.4504401683807373\n",
            "2.536848306655884\n",
            "2.396146297454834\n",
            "2.4424691200256348\n",
            "2.467679977416992\n",
            "2.411026954650879\n",
            "2.4600071907043457\n",
            "2.4213273525238037\n",
            "2.4702186584472656\n",
            "2.515547037124634\n",
            "2.5224523544311523\n",
            "2.523329496383667\n",
            "2.466590166091919\n",
            "2.420092821121216\n",
            "2.3992249965667725\n",
            "2.4053587913513184\n",
            "2.3986003398895264\n",
            "2.474663257598877\n",
            "2.357802391052246\n",
            "2.39184308052063\n",
            "2.461472272872925\n",
            "2.5648889541625977\n",
            "2.488166332244873\n",
            "2.596374273300171\n",
            "2.5468831062316895\n",
            "2.430234670639038\n",
            "2.552962064743042\n",
            "2.330415725708008\n",
            "2.61006236076355\n",
            "2.4460721015930176\n",
            "2.505112886428833\n",
            "2.5511038303375244\n",
            "2.450051784515381\n",
            "2.472757577896118\n",
            "2.379965305328369\n",
            "2.3034138679504395\n",
            "2.4920568466186523\n",
            "2.4250519275665283\n",
            "2.4084293842315674\n",
            "2.592264175415039\n",
            "2.5370466709136963\n",
            "2.445369243621826\n",
            "2.5244176387786865\n",
            "2.3796448707580566\n",
            "2.5650475025177\n",
            "2.4811859130859375\n",
            "2.483865737915039\n",
            "2.572685956954956\n",
            "2.4210879802703857\n",
            "2.415877103805542\n",
            "2.4514293670654297\n",
            "2.3985862731933594\n",
            "2.545949935913086\n",
            "2.3796780109405518\n",
            "2.423177719116211\n",
            "2.4189648628234863\n",
            "2.4065921306610107\n",
            "2.4411776065826416\n",
            "2.3729512691497803\n",
            "2.417295217514038\n",
            "2.4757609367370605\n",
            "2.4731390476226807\n",
            "2.572618007659912\n",
            "2.4248602390289307\n",
            "2.481323719024658\n",
            "2.510664463043213\n",
            "2.3096067905426025\n",
            "2.521430730819702\n",
            "2.4362967014312744\n",
            "2.5726449489593506\n",
            "2.4410345554351807\n",
            "2.3711788654327393\n",
            "2.503481149673462\n",
            "2.4598803520202637\n",
            "2.5245139598846436\n",
            "2.453174114227295\n",
            "2.4411754608154297\n",
            "2.337263822555542\n",
            "2.4604227542877197\n",
            "2.3895528316497803\n",
            "2.551778554916382\n",
            "2.4741575717926025\n",
            "2.5045759677886963\n",
            "2.431947708129883\n",
            "2.6095123291015625\n",
            "2.3676259517669678\n",
            "2.5222649574279785\n",
            "2.4991352558135986\n",
            "2.554184675216675\n",
            "2.575711727142334\n",
            "2.4676246643066406\n",
            "2.525970458984375\n",
            "2.5380332469940186\n",
            "2.4299023151397705\n",
            "2.502716541290283\n",
            "2.665743350982666\n",
            "2.437656879425049\n",
            "2.525904655456543\n",
            "2.417442560195923\n",
            "2.4228453636169434\n",
            "2.381957530975342\n",
            "2.4618446826934814\n",
            "2.3907735347747803\n",
            "2.4144551753997803\n",
            "2.541590452194214\n",
            "2.4571115970611572\n",
            "2.4875099658966064\n",
            "2.4582579135894775\n",
            "2.4479470252990723\n",
            "2.495556354522705\n",
            "2.502084255218506\n",
            "2.445136785507202\n",
            "2.421370506286621\n",
            "2.4488723278045654\n",
            "2.5028700828552246\n",
            "2.3664333820343018\n",
            "2.5422682762145996\n",
            "2.6043004989624023\n",
            "2.5067858695983887\n",
            "2.4499192237854004\n",
            "2.500678777694702\n",
            "2.6065189838409424\n",
            "2.5520730018615723\n",
            "2.477303981781006\n",
            "2.4081921577453613\n",
            "2.4056193828582764\n",
            "2.490683078765869\n",
            "2.574139356613159\n",
            "2.4226274490356445\n",
            "2.5222856998443604\n",
            "2.409390687942505\n",
            "2.4533820152282715\n",
            "2.5432307720184326\n",
            "2.6027326583862305\n",
            "2.4217097759246826\n",
            "2.589694023132324\n",
            "2.5107688903808594\n",
            "2.4930837154388428\n",
            "2.4382739067077637\n",
            "2.3277883529663086\n",
            "2.319115400314331\n",
            "2.459535598754883\n",
            "2.4997642040252686\n",
            "2.518897533416748\n",
            "2.4740190505981445\n",
            "2.509577512741089\n",
            "2.460832357406616\n",
            "2.3837528228759766\n",
            "2.4670779705047607\n",
            "2.4598655700683594\n",
            "2.5461173057556152\n",
            "2.4774041175842285\n",
            "2.3702754974365234\n",
            "2.474818706512451\n",
            "2.4755702018737793\n",
            "2.471592664718628\n",
            "2.540316581726074\n",
            "2.4295623302459717\n",
            "2.41597318649292\n",
            "2.413300037384033\n",
            "2.4670825004577637\n",
            "2.380169153213501\n",
            "2.4129133224487305\n",
            "2.4437036514282227\n",
            "2.491713047027588\n",
            "2.456258535385132\n",
            "2.435514450073242\n",
            "2.550626754760742\n",
            "2.412126064300537\n",
            "2.492769956588745\n",
            "2.570024013519287\n",
            "2.4024910926818848\n",
            "2.5003037452697754\n",
            "2.395185947418213\n",
            "2.4266035556793213\n",
            "2.3143832683563232\n",
            "2.4663164615631104\n",
            "2.454559087753296\n",
            "2.503589630126953\n",
            "2.3523011207580566\n",
            "2.3901917934417725\n",
            "2.3881685733795166\n",
            "2.4700515270233154\n",
            "2.4188663959503174\n",
            "2.52786922454834\n",
            "2.59108304977417\n",
            "2.611870288848877\n",
            "2.5944998264312744\n",
            "2.4253909587860107\n",
            "2.444059133529663\n",
            "2.464606761932373\n",
            "2.478454113006592\n",
            "2.321104049682617\n",
            "2.4421017169952393\n",
            "2.5801424980163574\n",
            "2.4621236324310303\n",
            "2.5349082946777344\n",
            "2.474666118621826\n",
            "2.531024694442749\n",
            "2.6127452850341797\n",
            "2.439180612564087\n",
            "2.6053054332733154\n",
            "2.429969072341919\n",
            "2.4840800762176514\n",
            "2.47609281539917\n",
            "2.4544551372528076\n",
            "2.451610565185547\n",
            "2.505505084991455\n",
            "2.5384328365325928\n",
            "2.3916549682617188\n",
            "2.5132927894592285\n",
            "2.63283109664917\n",
            "2.368494749069214\n",
            "2.4000496864318848\n",
            "2.4066972732543945\n",
            "2.425347328186035\n",
            "2.490453004837036\n",
            "2.432025671005249\n",
            "2.5003607273101807\n",
            "2.582388401031494\n",
            "2.4760284423828125\n",
            "2.408457040786743\n",
            "2.4885454177856445\n",
            "2.5766725540161133\n",
            "2.5032758712768555\n",
            "2.522020101547241\n",
            "2.4163241386413574\n",
            "2.464294195175171\n",
            "2.4324800968170166\n",
            "2.5770788192749023\n",
            "2.5244557857513428\n",
            "2.4795444011688232\n",
            "2.5361924171447754\n",
            "2.419698476791382\n",
            "2.516765594482422\n",
            "2.45823335647583\n",
            "2.484100341796875\n",
            "2.492753744125366\n",
            "2.550271511077881\n",
            "2.52605938911438\n",
            "2.4430484771728516\n",
            "2.57244873046875\n",
            "2.39119029045105\n",
            "2.4901037216186523\n",
            "2.5027318000793457\n",
            "2.439948320388794\n",
            "2.451704978942871\n",
            "2.4610486030578613\n",
            "2.5191047191619873\n",
            "2.5119998455047607\n",
            "2.385336399078369\n",
            "2.4095160961151123\n",
            "2.4504363536834717\n",
            "2.5099048614501953\n",
            "2.4969723224639893\n",
            "2.4497663974761963\n",
            "2.4158596992492676\n",
            "2.470099925994873\n",
            "2.400974988937378\n",
            "2.3873372077941895\n",
            "2.5663058757781982\n",
            "2.4033091068267822\n",
            "2.439797878265381\n",
            "2.5205957889556885\n",
            "2.4213411808013916\n",
            "2.4428744316101074\n",
            "2.525189161300659\n",
            "2.402966260910034\n",
            "2.5419516563415527\n",
            "2.509302854537964\n",
            "2.3848299980163574\n",
            "2.4899957180023193\n",
            "2.4408023357391357\n",
            "2.6170294284820557\n",
            "2.497842788696289\n",
            "2.513357639312744\n",
            "2.3997297286987305\n",
            "2.5301342010498047\n",
            "2.406874418258667\n",
            "2.5382773876190186\n",
            "2.3557989597320557\n",
            "2.550293445587158\n",
            "2.436655282974243\n",
            "2.540635347366333\n",
            "2.4669950008392334\n",
            "2.4562554359436035\n",
            "2.3806772232055664\n",
            "2.415494680404663\n",
            "2.4498512744903564\n",
            "2.5425868034362793\n",
            "2.528425931930542\n",
            "2.585625410079956\n",
            "2.539409637451172\n",
            "2.4497909545898438\n",
            "2.4172675609588623\n",
            "2.4265379905700684\n",
            "2.504645586013794\n",
            "2.3647027015686035\n",
            "2.521127700805664\n",
            "2.545731782913208\n",
            "2.451253652572632\n",
            "2.4044718742370605\n",
            "2.505730152130127\n",
            "2.395998477935791\n",
            "2.66294527053833\n",
            "2.480733871459961\n",
            "2.4234619140625\n",
            "2.5525429248809814\n",
            "2.4964752197265625\n",
            "2.4221553802490234\n",
            "2.536127805709839\n",
            "2.4926016330718994\n",
            "2.45408034324646\n",
            "2.4480319023132324\n",
            "2.4557576179504395\n",
            "2.535386800765991\n",
            "2.444922924041748\n",
            "2.5240187644958496\n",
            "2.335822343826294\n",
            "2.448173761367798\n",
            "2.551145076751709\n",
            "2.4921836853027344\n",
            "2.377957344055176\n",
            "2.3490333557128906\n",
            "2.4775466918945312\n",
            "2.4676434993743896\n",
            "2.3972625732421875\n",
            "2.490654468536377\n",
            "2.4636480808258057\n",
            "2.524197578430176\n",
            "2.3795294761657715\n",
            "2.5040972232818604\n",
            "2.453463554382324\n",
            "2.5290164947509766\n",
            "2.3921096324920654\n",
            "2.5419692993164062\n",
            "2.4784483909606934\n",
            "2.2921159267425537\n",
            "2.470370054244995\n",
            "2.577620029449463\n",
            "2.5261590480804443\n",
            "2.472334384918213\n",
            "2.5697031021118164\n",
            "2.3810174465179443\n",
            "2.4318625926971436\n",
            "2.4549624919891357\n",
            "2.3498518466949463\n",
            "2.478452444076538\n",
            "2.4493918418884277\n",
            "2.4537322521209717\n",
            "2.540576219558716\n",
            "2.4839682579040527\n",
            "2.379293441772461\n",
            "2.5027506351470947\n",
            "2.540829658508301\n",
            "2.5162711143493652\n",
            "2.480198621749878\n",
            "2.398085117340088\n",
            "2.5519981384277344\n",
            "2.4847192764282227\n",
            "2.462696075439453\n",
            "2.4803314208984375\n",
            "2.5233497619628906\n",
            "2.448960065841675\n",
            "2.4679512977600098\n",
            "2.441596508026123\n",
            "2.529282808303833\n",
            "2.453364372253418\n",
            "2.4836950302124023\n",
            "2.571622848510742\n",
            "2.479250192642212\n",
            "2.4641103744506836\n",
            "2.3780813217163086\n",
            "2.478874921798706\n",
            "2.4597835540771484\n",
            "2.3909199237823486\n",
            "2.4764373302459717\n",
            "2.3797037601470947\n",
            "2.450070858001709\n",
            "2.4115147590637207\n",
            "2.521463394165039\n",
            "2.4947283267974854\n",
            "2.4078383445739746\n",
            "2.391366958618164\n",
            "2.4230117797851562\n",
            "2.530322551727295\n",
            "2.4515504837036133\n",
            "2.5206758975982666\n",
            "2.452462911605835\n",
            "2.3915140628814697\n",
            "2.485870838165283\n",
            "2.4339916706085205\n",
            "2.484797239303589\n",
            "2.4768576622009277\n",
            "2.355902671813965\n",
            "2.4941012859344482\n",
            "2.4581353664398193\n",
            "2.6011264324188232\n",
            "2.432940721511841\n",
            "2.592095136642456\n",
            "2.3350915908813477\n",
            "2.4846394062042236\n",
            "2.387246608734131\n",
            "2.351407289505005\n",
            "2.5300190448760986\n",
            "2.4767513275146484\n",
            "2.589254379272461\n",
            "2.3063018321990967\n",
            "2.457503080368042\n",
            "2.476043224334717\n",
            "2.4612393379211426\n",
            "2.478013753890991\n",
            "2.443331718444824\n",
            "2.630143165588379\n",
            "2.30713152885437\n",
            "2.55014705657959\n",
            "2.451401948928833\n",
            "2.5001473426818848\n",
            "2.49822735786438\n",
            "2.360470771789551\n",
            "2.4076669216156006\n",
            "2.526369571685791\n",
            "2.5208399295806885\n",
            "2.490551233291626\n",
            "2.4478251934051514\n",
            "2.670719623565674\n",
            "2.5317955017089844\n",
            "2.4314727783203125\n",
            "2.4628968238830566\n",
            "2.4103450775146484\n",
            "2.49047589302063\n",
            "2.478793144226074\n",
            "2.4104771614074707\n",
            "2.428785562515259\n",
            "2.4098753929138184\n",
            "2.451920747756958\n",
            "2.57308292388916\n",
            "2.512338638305664\n",
            "2.4590694904327393\n",
            "2.444427728652954\n",
            "2.5522148609161377\n",
            "2.50530743598938\n",
            "2.469287633895874\n",
            "2.6280055046081543\n",
            "2.4486453533172607\n",
            "2.418189525604248\n",
            "2.4349026679992676\n",
            "2.578968048095703\n",
            "2.5947635173797607\n",
            "2.440869092941284\n",
            "2.407956123352051\n",
            "2.5287842750549316\n",
            "2.4844353199005127\n",
            "2.5094552040100098\n",
            "2.4914722442626953\n",
            "2.54547381401062\n",
            "2.413315534591675\n",
            "2.528200149536133\n",
            "2.463552713394165\n",
            "2.5796844959259033\n",
            "2.5059356689453125\n",
            "2.3054943084716797\n",
            "2.3396873474121094\n",
            "2.52813720703125\n",
            "2.5158934593200684\n",
            "2.4760842323303223\n",
            "2.531616449356079\n",
            "2.395772695541382\n",
            "2.490394353866577\n",
            "2.5152840614318848\n",
            "2.4488632678985596\n",
            "2.464775562286377\n",
            "2.5131313800811768\n",
            "2.4501991271972656\n",
            "2.511674165725708\n",
            "2.428875684738159\n",
            "2.553590774536133\n",
            "2.363825559616089\n",
            "2.6420490741729736\n",
            "2.5399982929229736\n",
            "2.659688711166382\n",
            "2.378910779953003\n",
            "2.5510289669036865\n",
            "2.513805866241455\n",
            "2.5286080837249756\n",
            "2.4711856842041016\n",
            "2.531268358230591\n",
            "2.399867534637451\n",
            "2.4748942852020264\n",
            "2.4858012199401855\n",
            "2.4695839881896973\n",
            "2.4837515354156494\n",
            "2.5009427070617676\n",
            "2.5405993461608887\n",
            "2.5982918739318848\n",
            "2.63547945022583\n",
            "2.554854393005371\n",
            "2.4047982692718506\n",
            "2.4644012451171875\n",
            "2.4241137504577637\n",
            "2.485623598098755\n",
            "2.396267890930176\n",
            "2.55751895904541\n",
            "2.5093560218811035\n",
            "2.4048476219177246\n",
            "2.474459648132324\n",
            "2.4283366203308105\n",
            "2.3276352882385254\n",
            "2.4257824420928955\n",
            "2.537688732147217\n",
            "2.4554343223571777\n",
            "2.570279359817505\n",
            "2.533585786819458\n",
            "2.434199333190918\n",
            "2.590254545211792\n",
            "2.5038578510284424\n",
            "2.331066370010376\n",
            "2.5319745540618896\n",
            "2.452446699142456\n",
            "2.4332711696624756\n",
            "2.5294246673583984\n",
            "2.3161330223083496\n",
            "2.415527820587158\n",
            "2.46191143989563\n",
            "2.37809157371521\n",
            "2.452793598175049\n",
            "2.5657432079315186\n",
            "2.412247896194458\n",
            "2.469850778579712\n",
            "2.4972445964813232\n",
            "2.6167662143707275\n",
            "2.5600154399871826\n",
            "2.5633225440979004\n",
            "2.529287576675415\n",
            "2.5322091579437256\n",
            "2.4666433334350586\n",
            "2.428884267807007\n",
            "2.423743963241577\n",
            "2.520249128341675\n",
            "2.4856491088867188\n",
            "2.4067344665527344\n",
            "2.5075130462646484\n",
            "2.4614884853363037\n",
            "2.436310291290283\n",
            "2.368617296218872\n",
            "2.4709696769714355\n",
            "2.3619883060455322\n",
            "2.342364549636841\n",
            "2.4673197269439697\n",
            "2.4536776542663574\n",
            "2.4390928745269775\n",
            "2.4307868480682373\n",
            "2.47100830078125\n",
            "2.3859710693359375\n",
            "2.427795886993408\n",
            "2.6129443645477295\n",
            "2.4763500690460205\n",
            "2.446575164794922\n",
            "2.508244514465332\n",
            "2.535184621810913\n",
            "2.33249831199646\n",
            "2.6154894828796387\n",
            "2.6233487129211426\n",
            "2.5249979496002197\n",
            "2.5394339561462402\n",
            "2.2860870361328125\n",
            "2.51578426361084\n",
            "2.478999376296997\n",
            "2.5307743549346924\n",
            "2.4178311824798584\n",
            "2.4536523818969727\n",
            "2.4552855491638184\n",
            "2.4449424743652344\n",
            "2.4697952270507812\n",
            "2.5122711658477783\n",
            "2.3799171447753906\n",
            "2.411590337753296\n",
            "2.519138813018799\n",
            "2.546268939971924\n",
            "2.4542007446289062\n",
            "2.455554962158203\n",
            "2.4480361938476562\n",
            "2.478736400604248\n",
            "2.5586814880371094\n",
            "2.577286958694458\n",
            "2.450127601623535\n",
            "2.341841459274292\n",
            "2.3568878173828125\n",
            "2.4132702350616455\n",
            "2.422292709350586\n",
            "2.5285489559173584\n",
            "2.5182762145996094\n",
            "2.590043783187866\n",
            "2.519838571548462\n",
            "2.3766300678253174\n",
            "2.44122052192688\n",
            "2.5600907802581787\n",
            "2.5347611904144287\n",
            "2.4652581214904785\n",
            "2.5229814052581787\n",
            "2.33552885055542\n",
            "2.53715443611145\n",
            "2.515022039413452\n",
            "2.5828728675842285\n",
            "2.510770082473755\n",
            "2.535691976547241\n",
            "2.4902942180633545\n",
            "2.550410032272339\n",
            "2.595036029815674\n",
            "2.3244030475616455\n",
            "2.4210383892059326\n",
            "2.5009377002716064\n",
            "2.448482036590576\n",
            "2.4416778087615967\n",
            "2.711827278137207\n",
            "2.5109288692474365\n",
            "2.4718375205993652\n",
            "2.4921867847442627\n",
            "2.575080394744873\n",
            "2.4031646251678467\n",
            "2.5552334785461426\n",
            "2.491037368774414\n",
            "2.358175754547119\n",
            "2.3380801677703857\n",
            "2.562382936477661\n",
            "2.410576581954956\n",
            "2.412095546722412\n",
            "2.5393011569976807\n",
            "2.392117977142334\n",
            "2.5417652130126953\n",
            "2.42262864112854\n",
            "2.531858205795288\n",
            "2.403343439102173\n",
            "2.5736725330352783\n",
            "2.481484889984131\n",
            "2.5407097339630127\n",
            "2.5238137245178223\n",
            "2.3719260692596436\n",
            "2.4512600898742676\n",
            "2.491454839706421\n",
            "2.4543509483337402\n",
            "2.5235023498535156\n",
            "2.3945934772491455\n",
            "2.360018491744995\n",
            "2.410799264907837\n",
            "2.369265079498291\n",
            "2.6367340087890625\n",
            "2.4035556316375732\n",
            "2.4635870456695557\n",
            "2.557457685470581\n",
            "2.387338399887085\n",
            "2.506504774093628\n",
            "2.3916563987731934\n",
            "2.525031805038452\n",
            "2.51111102104187\n",
            "2.462951183319092\n",
            "2.45843505859375\n",
            "2.5685579776763916\n",
            "2.359604835510254\n",
            "2.3739240169525146\n",
            "2.4939491748809814\n",
            "2.4452948570251465\n",
            "2.4402499198913574\n",
            "2.434493064880371\n",
            "2.2825984954833984\n",
            "2.5397212505340576\n",
            "2.273723840713501\n",
            "2.3288190364837646\n",
            "2.4672443866729736\n",
            "2.513472557067871\n",
            "2.4651074409484863\n",
            "2.3964791297912598\n",
            "2.4549078941345215\n",
            "2.4928319454193115\n",
            "2.411109209060669\n",
            "2.5352234840393066\n",
            "2.408783435821533\n",
            "2.4306113719940186\n",
            "2.515679359436035\n",
            "2.490222215652466\n",
            "2.4854915142059326\n",
            "2.4985477924346924\n",
            "2.413241147994995\n",
            "2.49153733253479\n",
            "2.5446271896362305\n",
            "2.4014689922332764\n",
            "2.435798406600952\n",
            "2.5402817726135254\n",
            "2.484516143798828\n",
            "2.5294620990753174\n",
            "2.46655535697937\n",
            "2.547680139541626\n",
            "2.3119051456451416\n",
            "2.447115182876587\n",
            "2.4730441570281982\n",
            "2.4178378582000732\n",
            "2.4748318195343018\n",
            "2.497218608856201\n",
            "2.458456039428711\n",
            "2.384875535964966\n",
            "2.543393850326538\n",
            "2.547844409942627\n",
            "2.5391387939453125\n",
            "2.4269649982452393\n",
            "2.436594247817993\n",
            "2.550668239593506\n",
            "2.460362672805786\n",
            "2.4125125408172607\n",
            "2.342923164367676\n",
            "2.4677670001983643\n",
            "2.628143787384033\n",
            "2.5416014194488525\n",
            "2.444627046585083\n",
            "2.426473379135132\n",
            "2.4499850273132324\n",
            "2.4363765716552734\n",
            "2.405202865600586\n",
            "2.5266361236572266\n",
            "2.4807446002960205\n",
            "2.3631105422973633\n",
            "2.4867396354675293\n",
            "2.5170528888702393\n",
            "2.478623867034912\n",
            "2.417731761932373\n",
            "2.3553740978240967\n",
            "2.621642589569092\n",
            "2.6582682132720947\n",
            "2.5210301876068115\n",
            "2.438089609146118\n",
            "2.4987947940826416\n",
            "2.4994359016418457\n",
            "2.5442941188812256\n",
            "2.4497456550598145\n",
            "2.4707889556884766\n",
            "2.395472764968872\n",
            "2.4528539180755615\n",
            "2.4543280601501465\n",
            "2.4653942584991455\n",
            "2.470198631286621\n",
            "2.417062520980835\n",
            "2.553680419921875\n",
            "2.477909803390503\n",
            "2.483668327331543\n",
            "2.537235736846924\n",
            "2.3598477840423584\n",
            "2.399047374725342\n",
            "2.4572157859802246\n",
            "2.5447490215301514\n",
            "2.3780531883239746\n",
            "2.439830780029297\n",
            "2.5538787841796875\n",
            "2.2972934246063232\n",
            "2.4730348587036133\n",
            "2.2947499752044678\n",
            "2.4580376148223877\n",
            "2.5433266162872314\n",
            "2.5626916885375977\n",
            "2.5183157920837402\n",
            "2.392958164215088\n",
            "2.4610297679901123\n",
            "2.415719985961914\n",
            "2.4082608222961426\n",
            "2.5447230339050293\n",
            "2.4440183639526367\n",
            "2.4071030616760254\n",
            "2.4301042556762695\n",
            "2.45821475982666\n",
            "2.487055778503418\n",
            "2.657402276992798\n",
            "2.3919594287872314\n",
            "2.4813501834869385\n",
            "2.375941276550293\n",
            "2.4862513542175293\n",
            "2.486469030380249\n",
            "2.4349544048309326\n",
            "2.518632173538208\n",
            "2.456605911254883\n",
            "2.4722824096679688\n",
            "2.3860232830047607\n",
            "2.4315237998962402\n",
            "2.3248751163482666\n",
            "2.535778522491455\n",
            "2.413472890853882\n",
            "2.517207384109497\n",
            "2.39616060256958\n",
            "2.309422731399536\n",
            "2.4034931659698486\n",
            "2.588937282562256\n",
            "2.5203754901885986\n",
            "2.436464309692383\n",
            "2.5924179553985596\n",
            "2.5034258365631104\n",
            "2.4656565189361572\n",
            "2.578028440475464\n",
            "2.310314655303955\n",
            "2.5332186222076416\n",
            "2.44498348236084\n",
            "2.327418565750122\n",
            "2.470176935195923\n",
            "2.5549263954162598\n",
            "2.351921319961548\n",
            "2.465765953063965\n",
            "2.5804855823516846\n",
            "2.539659261703491\n",
            "2.4038219451904297\n",
            "2.5177388191223145\n",
            "2.4720585346221924\n",
            "2.43327260017395\n",
            "2.520414113998413\n",
            "2.441837787628174\n",
            "2.4792182445526123\n",
            "2.432189464569092\n",
            "2.5629210472106934\n",
            "2.577072858810425\n",
            "2.4747209548950195\n",
            "2.4438889026641846\n",
            "2.4769108295440674\n",
            "2.3958683013916016\n",
            "2.584838628768921\n",
            "2.5178823471069336\n",
            "2.369312047958374\n",
            "2.48001766204834\n",
            "2.369980573654175\n",
            "2.36372971534729\n",
            "2.43536639213562\n",
            "2.4030113220214844\n",
            "2.469022035598755\n",
            "2.6020023822784424\n",
            "2.4427616596221924\n",
            "2.5380313396453857\n",
            "2.538560152053833\n",
            "2.430769443511963\n",
            "2.263768434524536\n",
            "2.561861276626587\n",
            "2.4085755348205566\n",
            "2.5000975131988525\n",
            "2.4629344940185547\n",
            "2.4395759105682373\n",
            "2.5636942386627197\n",
            "2.406040906906128\n",
            "2.5574424266815186\n",
            "2.4157192707061768\n",
            "2.517648458480835\n",
            "2.4886412620544434\n",
            "2.4246091842651367\n",
            "2.4123973846435547\n",
            "2.487293004989624\n",
            "2.508267879486084\n",
            "2.4586868286132812\n",
            "2.454282760620117\n",
            "2.50406813621521\n",
            "2.530245304107666\n",
            "2.581646203994751\n",
            "2.4861154556274414\n",
            "2.369382858276367\n",
            "2.4853930473327637\n",
            "2.424227476119995\n",
            "2.585028648376465\n",
            "2.475867509841919\n",
            "2.443819284439087\n",
            "2.4783051013946533\n",
            "2.5692524909973145\n",
            "2.5980353355407715\n",
            "2.559852123260498\n",
            "2.3402771949768066\n",
            "2.5696842670440674\n",
            "2.4236085414886475\n",
            "2.551722526550293\n",
            "2.4201817512512207\n",
            "2.4804415702819824\n",
            "2.3846068382263184\n",
            "2.4354827404022217\n",
            "2.522620916366577\n",
            "2.427708625793457\n",
            "2.3327229022979736\n",
            "2.508401870727539\n",
            "2.464059829711914\n",
            "2.6509039402008057\n",
            "2.60969877243042\n",
            "2.5471744537353516\n",
            "2.2811601161956787\n",
            "2.4120724201202393\n",
            "2.448598861694336\n",
            "2.4553098678588867\n",
            "2.475266456604004\n",
            "2.5044469833374023\n",
            "2.6319053173065186\n",
            "2.4500513076782227\n",
            "2.482189178466797\n",
            "2.554375171661377\n",
            "2.4969534873962402\n",
            "2.380574941635132\n",
            "2.3704655170440674\n",
            "2.5407423973083496\n",
            "2.432983636856079\n",
            "2.377311944961548\n",
            "2.4623656272888184\n",
            "2.5842981338500977\n",
            "2.5672402381896973\n",
            "2.380425214767456\n",
            "2.564211845397949\n",
            "2.2997751235961914\n",
            "2.450096845626831\n",
            "2.5171031951904297\n",
            "2.5526514053344727\n",
            "2.3692715167999268\n",
            "2.4831295013427734\n",
            "2.4933676719665527\n",
            "2.5141632556915283\n",
            "2.457275629043579\n",
            "2.473315477371216\n",
            "2.464219570159912\n",
            "2.416240692138672\n",
            "2.6542837619781494\n",
            "2.4691405296325684\n",
            "2.5275678634643555\n",
            "2.380669116973877\n",
            "2.4810197353363037\n",
            "2.420370578765869\n",
            "2.5139753818511963\n",
            "2.553697109222412\n",
            "2.4836909770965576\n",
            "2.5793869495391846\n",
            "2.450343132019043\n",
            "2.4171841144561768\n",
            "2.5359079837799072\n",
            "2.5333802700042725\n",
            "2.4716970920562744\n",
            "2.4305949211120605\n",
            "2.4254212379455566\n",
            "2.4588983058929443\n",
            "2.5177083015441895\n",
            "2.62294340133667\n",
            "2.574404001235962\n",
            "2.368479013442993\n",
            "2.470723867416382\n",
            "2.4088072776794434\n",
            "2.4848875999450684\n",
            "2.5477941036224365\n",
            "2.466593027114868\n",
            "2.3699121475219727\n",
            "2.5444517135620117\n",
            "2.5103163719177246\n",
            "2.482348918914795\n",
            "2.483642578125\n",
            "2.5094285011291504\n",
            "2.529294729232788\n",
            "2.4262936115264893\n",
            "2.539632558822632\n",
            "2.4804117679595947\n",
            "2.4964375495910645\n",
            "2.5407001972198486\n",
            "2.5545620918273926\n",
            "2.361295700073242\n",
            "2.6480891704559326\n",
            "2.4422223567962646\n",
            "2.5509727001190186\n",
            "2.5191538333892822\n",
            "2.436216354370117\n",
            "2.484868049621582\n",
            "2.480964183807373\n",
            "2.4894776344299316\n",
            "2.4897570610046387\n",
            "2.4939348697662354\n",
            "2.4942994117736816\n",
            "2.3874247074127197\n",
            "2.5030152797698975\n",
            "2.515059471130371\n",
            "2.339367151260376\n",
            "2.4594147205352783\n",
            "2.418414354324341\n",
            "2.403106927871704\n",
            "2.692845344543457\n",
            "2.4805409908294678\n",
            "2.5234546661376953\n",
            "2.525179386138916\n",
            "2.5682504177093506\n",
            "2.4852218627929688\n",
            "2.5409345626831055\n",
            "2.451240301132202\n",
            "2.5036349296569824\n",
            "2.4405159950256348\n",
            "2.5942463874816895\n",
            "2.4600460529327393\n",
            "2.4842867851257324\n",
            "2.403186559677124\n",
            "2.3822414875030518\n",
            "2.440138578414917\n",
            "2.4407808780670166\n",
            "2.529784679412842\n",
            "2.5405361652374268\n",
            "2.4253852367401123\n",
            "2.418391227722168\n",
            "2.4674346446990967\n",
            "2.488513708114624\n",
            "2.48628830909729\n",
            "2.3294765949249268\n",
            "2.4399232864379883\n",
            "2.3784914016723633\n",
            "2.3156838417053223\n",
            "2.4640185832977295\n",
            "2.5001680850982666\n",
            "2.439756393432617\n",
            "2.5668160915374756\n",
            "2.52101469039917\n",
            "2.494285821914673\n",
            "2.5962331295013428\n",
            "2.462634801864624\n",
            "2.4927327632904053\n",
            "2.570254325866699\n",
            "2.4042859077453613\n",
            "2.5812623500823975\n",
            "2.357513427734375\n",
            "2.570523500442505\n",
            "2.493318796157837\n",
            "2.4921963214874268\n",
            "2.4456021785736084\n",
            "2.5001330375671387\n",
            "2.438646078109741\n",
            "2.4747142791748047\n",
            "2.5104379653930664\n",
            "2.3990397453308105\n",
            "2.5235657691955566\n",
            "2.4958086013793945\n",
            "2.4353280067443848\n",
            "2.5023727416992188\n",
            "2.355051040649414\n",
            "2.549009323120117\n",
            "2.449939489364624\n",
            "2.6133575439453125\n",
            "2.3344879150390625\n",
            "2.53482723236084\n",
            "2.547403573989868\n",
            "2.4348695278167725\n",
            "2.5686020851135254\n",
            "2.4899675846099854\n",
            "2.4392714500427246\n",
            "2.5607352256774902\n",
            "2.3956406116485596\n",
            "2.4358932971954346\n",
            "2.4866809844970703\n",
            "2.3537368774414062\n",
            "2.5255069732666016\n",
            "2.495779037475586\n",
            "2.440723419189453\n",
            "2.4098236560821533\n",
            "2.4110167026519775\n",
            "2.3993372917175293\n",
            "2.44551157951355\n",
            "2.434581995010376\n",
            "2.51743221282959\n",
            "2.456242799758911\n",
            "2.5472545623779297\n",
            "2.4248459339141846\n",
            "2.633625030517578\n",
            "2.4892101287841797\n",
            "2.3700575828552246\n",
            "2.434999942779541\n",
            "2.572073459625244\n",
            "2.4996726512908936\n",
            "2.4712839126586914\n",
            "2.4862892627716064\n",
            "2.427516460418701\n",
            "2.541354179382324\n",
            "2.4649078845977783\n",
            "2.4141623973846436\n",
            "2.59839129447937\n",
            "2.4880495071411133\n",
            "2.5077872276306152\n",
            "2.3198177814483643\n",
            "2.576390504837036\n",
            "2.5466959476470947\n",
            "2.3452765941619873\n",
            "2.4455208778381348\n",
            "2.4176979064941406\n",
            "2.5148918628692627\n",
            "2.6172709465026855\n",
            "2.3905763626098633\n",
            "2.5513768196105957\n",
            "2.639920949935913\n",
            "2.389688014984131\n",
            "2.5310323238372803\n",
            "2.364912271499634\n",
            "2.482886791229248\n",
            "2.5303149223327637\n",
            "2.4960691928863525\n",
            "2.4212543964385986\n",
            "2.3882224559783936\n",
            "2.4429798126220703\n",
            "2.336652994155884\n",
            "2.3910253047943115\n",
            "2.447291612625122\n",
            "2.4297409057617188\n",
            "2.523686408996582\n",
            "2.463466167449951\n",
            "2.4392166137695312\n",
            "2.503676176071167\n",
            "2.4537181854248047\n",
            "2.5898802280426025\n",
            "2.441838026046753\n",
            "2.4634335041046143\n",
            "2.6554088592529297\n",
            "2.382290840148926\n",
            "2.509168863296509\n",
            "2.4684078693389893\n",
            "2.504842519760132\n",
            "2.5315699577331543\n",
            "2.5192506313323975\n",
            "2.4542582035064697\n",
            "2.5270557403564453\n",
            "2.3664493560791016\n",
            "2.4993414878845215\n",
            "2.486341714859009\n",
            "2.481616258621216\n",
            "2.4368438720703125\n",
            "2.382685661315918\n",
            "2.4601969718933105\n",
            "2.5268704891204834\n",
            "2.5007026195526123\n",
            "2.4872634410858154\n",
            "2.3700785636901855\n",
            "2.53402042388916\n",
            "2.477421283721924\n",
            "2.5162131786346436\n",
            "2.529785394668579\n",
            "2.4227190017700195\n",
            "2.5127930641174316\n",
            "2.531045436859131\n",
            "2.386806011199951\n",
            "2.383988857269287\n",
            "2.357980489730835\n",
            "2.420238733291626\n",
            "2.5745174884796143\n",
            "2.488098382949829\n",
            "2.4841575622558594\n",
            "2.478320360183716\n",
            "2.401470422744751\n",
            "2.466918468475342\n",
            "2.43142032623291\n",
            "2.45768404006958\n",
            "2.486487627029419\n",
            "2.4339733123779297\n",
            "2.4926135540008545\n",
            "2.345811128616333\n",
            "2.4434890747070312\n",
            "2.5021986961364746\n",
            "2.488481044769287\n",
            "2.4240500926971436\n",
            "2.6193127632141113\n",
            "2.478145122528076\n",
            "2.468543291091919\n",
            "2.5153872966766357\n",
            "2.475182056427002\n",
            "2.6068480014801025\n",
            "2.5780739784240723\n",
            "2.616171360015869\n",
            "2.398796558380127\n",
            "2.585219383239746\n",
            "2.5840277671813965\n",
            "2.4400510787963867\n",
            "2.565185308456421\n",
            "2.5265235900878906\n",
            "2.532952070236206\n",
            "2.3852739334106445\n",
            "2.5048632621765137\n",
            "2.5096395015716553\n",
            "2.5370404720306396\n",
            "2.563318967819214\n",
            "2.578683614730835\n",
            "2.495042324066162\n",
            "2.4393105506896973\n",
            "2.458005905151367\n",
            "2.5771725177764893\n",
            "2.398454427719116\n",
            "2.292670726776123\n",
            "2.4617931842803955\n",
            "2.509373426437378\n",
            "2.456958770751953\n",
            "2.370368719100952\n",
            "2.430002450942993\n",
            "2.5714025497436523\n",
            "2.537348508834839\n",
            "2.536881923675537\n",
            "2.518099069595337\n",
            "2.389230728149414\n",
            "2.4456160068511963\n",
            "2.479376792907715\n",
            "2.471510410308838\n",
            "2.525552272796631\n",
            "2.408543825149536\n",
            "2.569983959197998\n",
            "2.4236562252044678\n",
            "2.4485318660736084\n",
            "2.351138114929199\n",
            "2.3964645862579346\n",
            "2.3975815773010254\n",
            "2.5138988494873047\n",
            "2.4970877170562744\n",
            "2.480341911315918\n",
            "2.516822338104248\n",
            "2.5591554641723633\n",
            "2.4467291831970215\n",
            "2.4363460540771484\n",
            "2.508174419403076\n",
            "2.4503002166748047\n",
            "2.551251173019409\n",
            "2.4714090824127197\n",
            "2.543142795562744\n",
            "2.398430347442627\n",
            "2.4788615703582764\n",
            "2.5212695598602295\n",
            "2.4192392826080322\n",
            "2.374811887741089\n",
            "2.5227553844451904\n",
            "2.415501117706299\n",
            "2.423954725265503\n",
            "2.3890163898468018\n",
            "2.365659236907959\n",
            "2.379662036895752\n",
            "2.4852733612060547\n",
            "2.4513180255889893\n",
            "2.369342803955078\n",
            "2.420927047729492\n",
            "2.580862283706665\n",
            "2.4120934009552\n",
            "2.428459405899048\n",
            "2.6018996238708496\n",
            "2.4105067253112793\n",
            "2.476301908493042\n",
            "2.4803965091705322\n",
            "2.495352029800415\n",
            "2.464725971221924\n",
            "2.457400321960449\n",
            "2.53044056892395\n",
            "2.5533556938171387\n",
            "2.525212526321411\n",
            "2.4924330711364746\n",
            "2.463979721069336\n",
            "2.5379340648651123\n",
            "2.5279836654663086\n",
            "2.338073253631592\n",
            "2.4023826122283936\n",
            "2.4685330390930176\n",
            "2.48713755607605\n",
            "2.517482042312622\n",
            "2.4481124877929688\n",
            "2.57525897026062\n",
            "2.4806926250457764\n",
            "2.416538715362549\n",
            "2.592467784881592\n",
            "2.5916073322296143\n",
            "2.6322708129882812\n",
            "2.4131152629852295\n",
            "2.4610462188720703\n",
            "2.5195882320404053\n",
            "2.51224684715271\n",
            "2.413949489593506\n",
            "2.4454455375671387\n",
            "2.5280630588531494\n",
            "2.5024216175079346\n",
            "2.4997029304504395\n",
            "2.5355875492095947\n",
            "2.431443929672241\n",
            "2.4773712158203125\n",
            "2.5579025745391846\n",
            "2.426987648010254\n",
            "2.4064111709594727\n",
            "2.5168137550354004\n",
            "2.4835126399993896\n",
            "2.410831928253174\n",
            "2.4676852226257324\n",
            "2.45497465133667\n",
            "2.3422176837921143\n",
            "2.40785813331604\n",
            "2.4799001216888428\n",
            "2.5377073287963867\n",
            "2.4590694904327393\n",
            "2.4926037788391113\n",
            "2.4221973419189453\n",
            "2.6332321166992188\n",
            "2.624488115310669\n",
            "2.50394344329834\n",
            "2.454918384552002\n",
            "2.4776124954223633\n",
            "2.6482596397399902\n",
            "2.4470937252044678\n",
            "2.454939603805542\n",
            "2.4630930423736572\n",
            "2.4928693771362305\n",
            "2.587637424468994\n",
            "2.507702350616455\n",
            "2.5893912315368652\n",
            "2.4816019535064697\n",
            "2.661853790283203\n",
            "2.4856183528900146\n",
            "2.4768741130828857\n",
            "2.4735898971557617\n",
            "2.412271499633789\n",
            "2.415487289428711\n",
            "2.424057722091675\n",
            "2.492100715637207\n",
            "2.474660873413086\n",
            "2.523257255554199\n",
            "2.402402639389038\n",
            "2.547262668609619\n",
            "2.4994728565216064\n",
            "2.583164691925049\n",
            "2.4935452938079834\n",
            "2.3998937606811523\n",
            "2.4933016300201416\n",
            "2.5978844165802\n",
            "2.583326816558838\n",
            "2.421398162841797\n",
            "2.4576032161712646\n",
            "2.5453243255615234\n",
            "2.426254987716675\n",
            "2.439281702041626\n",
            "2.4656100273132324\n",
            "2.3743691444396973\n",
            "2.457672357559204\n",
            "2.4639320373535156\n",
            "2.4649345874786377\n",
            "2.5423343181610107\n",
            "2.335017681121826\n",
            "2.612663745880127\n",
            "2.478174924850464\n",
            "2.4929730892181396\n",
            "2.415804862976074\n",
            "2.4016265869140625\n",
            "2.535945177078247\n",
            "2.3968887329101562\n",
            "2.4973769187927246\n",
            "2.6395270824432373\n",
            "2.453258991241455\n",
            "2.3904149532318115\n",
            "2.424821376800537\n",
            "2.396479845046997\n",
            "2.344484806060791\n",
            "2.465642213821411\n",
            "2.467866897583008\n",
            "2.466939687728882\n",
            "2.5445542335510254\n",
            "2.3549609184265137\n",
            "2.3817760944366455\n",
            "2.4097506999969482\n",
            "2.3959779739379883\n",
            "2.4491989612579346\n",
            "2.5021331310272217\n",
            "2.41043758392334\n",
            "2.573604106903076\n",
            "2.565793514251709\n",
            "2.4532337188720703\n",
            "2.4980485439300537\n",
            "2.6294453144073486\n",
            "2.442866086959839\n",
            "2.440173864364624\n",
            "2.4343624114990234\n",
            "2.4248669147491455\n",
            "2.597764015197754\n",
            "2.4803290367126465\n",
            "2.408388614654541\n",
            "2.528409242630005\n",
            "2.54504656791687\n",
            "2.551074266433716\n",
            "2.3158655166625977\n",
            "2.4388175010681152\n",
            "2.418581962585449\n",
            "2.445789337158203\n",
            "2.565768241882324\n",
            "2.5066165924072266\n",
            "2.376941680908203\n",
            "2.6932332515716553\n",
            "2.528567314147949\n",
            "2.5242714881896973\n",
            "2.5757458209991455\n",
            "2.4035134315490723\n",
            "2.3780641555786133\n",
            "2.5816292762756348\n",
            "2.3602280616760254\n",
            "2.5330379009246826\n",
            "2.4519827365875244\n",
            "2.5039801597595215\n",
            "2.6089766025543213\n",
            "2.5391576290130615\n",
            "2.5451972484588623\n",
            "2.4721949100494385\n",
            "2.4222466945648193\n",
            "2.3855419158935547\n",
            "2.4472594261169434\n",
            "2.617657423019409\n",
            "2.56002140045166\n",
            "2.4011669158935547\n",
            "2.4388225078582764\n",
            "2.509899616241455\n",
            "2.55413818359375\n",
            "2.4086313247680664\n",
            "2.6199967861175537\n",
            "2.4138216972351074\n",
            "2.4935946464538574\n",
            "2.3927359580993652\n",
            "2.5783941745758057\n",
            "2.474470615386963\n",
            "2.467622756958008\n",
            "2.390385389328003\n",
            "2.4730477333068848\n",
            "2.4437170028686523\n",
            "2.4842145442962646\n",
            "2.46419358253479\n",
            "2.4682703018188477\n",
            "2.3564834594726562\n",
            "2.4960193634033203\n",
            "2.495645046234131\n",
            "2.3628101348876953\n",
            "2.3957040309906006\n",
            "2.466963291168213\n",
            "2.5015976428985596\n",
            "2.475100040435791\n",
            "2.4905548095703125\n",
            "2.463186740875244\n",
            "2.407588243484497\n",
            "2.521141767501831\n",
            "2.5572314262390137\n",
            "2.5129966735839844\n",
            "2.517573118209839\n",
            "2.5152413845062256\n",
            "2.526283025741577\n",
            "2.478745222091675\n",
            "2.432651996612549\n",
            "2.4795665740966797\n",
            "2.527127981185913\n",
            "2.364398956298828\n",
            "2.57698130607605\n",
            "2.547757387161255\n",
            "2.503488063812256\n",
            "2.3209457397460938\n",
            "2.6227715015411377\n",
            "2.5419065952301025\n",
            "2.3872034549713135\n",
            "2.4688820838928223\n",
            "2.4162752628326416\n",
            "2.4520766735076904\n",
            "2.4449405670166016\n",
            "2.493513584136963\n",
            "2.4045729637145996\n",
            "2.460071325302124\n",
            "2.3441808223724365\n",
            "2.2882254123687744\n",
            "2.6340432167053223\n",
            "2.480001449584961\n",
            "2.4449102878570557\n",
            "2.533855438232422\n",
            "2.507261037826538\n",
            "2.4880924224853516\n",
            "2.588120460510254\n",
            "2.4391045570373535\n",
            "2.3953895568847656\n",
            "2.558298110961914\n",
            "2.444242000579834\n",
            "2.4996888637542725\n",
            "2.4189043045043945\n",
            "2.527017831802368\n",
            "2.406895875930786\n",
            "2.4077813625335693\n",
            "2.5588631629943848\n",
            "2.545048236846924\n",
            "2.6130359172821045\n",
            "2.4396584033966064\n",
            "2.3295764923095703\n",
            "2.4715163707733154\n",
            "2.454394578933716\n",
            "2.751866579055786\n",
            "2.479404926300049\n",
            "2.4140427112579346\n",
            "2.432321548461914\n",
            "2.597259521484375\n",
            "2.4254720211029053\n",
            "2.390320301055908\n",
            "2.466327428817749\n",
            "2.4436891078948975\n",
            "2.4177374839782715\n",
            "2.5070419311523438\n",
            "2.4942381381988525\n",
            "2.4628989696502686\n",
            "2.5920581817626953\n",
            "2.496523857116699\n",
            "2.5214762687683105\n",
            "2.544020414352417\n",
            "2.476135730743408\n",
            "2.326256036758423\n",
            "2.3411149978637695\n",
            "2.346498489379883\n",
            "2.422147035598755\n",
            "2.510843276977539\n",
            "2.5359275341033936\n",
            "2.4410853385925293\n",
            "2.4719135761260986\n",
            "2.321869134902954\n",
            "2.4227569103240967\n",
            "2.4810657501220703\n",
            "2.3944644927978516\n",
            "2.462326765060425\n",
            "2.4443018436431885\n",
            "2.4572043418884277\n",
            "2.3512606620788574\n",
            "2.460740804672241\n",
            "2.456990957260132\n",
            "2.5369958877563477\n",
            "2.6298351287841797\n",
            "2.570397138595581\n",
            "2.5562615394592285\n",
            "2.3559372425079346\n",
            "2.5605063438415527\n",
            "2.399005174636841\n",
            "2.4595799446105957\n",
            "2.528265953063965\n",
            "2.4559690952301025\n",
            "2.3525123596191406\n",
            "2.4488139152526855\n",
            "2.4500110149383545\n",
            "2.4918034076690674\n",
            "2.5153415203094482\n",
            "2.315825939178467\n",
            "2.4093432426452637\n",
            "2.435488700866699\n",
            "2.586007833480835\n",
            "2.5390167236328125\n",
            "2.5161805152893066\n",
            "2.538661003112793\n",
            "2.4915637969970703\n",
            "2.431443929672241\n",
            "2.4453439712524414\n",
            "2.425676107406616\n",
            "2.510828971862793\n",
            "2.483825445175171\n",
            "2.4432339668273926\n",
            "2.479372262954712\n",
            "2.3488709926605225\n",
            "2.3522138595581055\n",
            "2.480621337890625\n",
            "2.5539045333862305\n",
            "2.525378465652466\n",
            "2.4402341842651367\n",
            "2.3675880432128906\n",
            "2.428668737411499\n",
            "2.5788490772247314\n",
            "2.4378576278686523\n",
            "2.591017007827759\n",
            "2.387786388397217\n",
            "2.480717897415161\n",
            "2.5802361965179443\n",
            "2.4702489376068115\n",
            "2.508134365081787\n",
            "2.6032907962799072\n",
            "2.4867324829101562\n",
            "2.4263358116149902\n",
            "2.3600668907165527\n",
            "2.474998712539673\n",
            "2.440264940261841\n",
            "2.5018577575683594\n",
            "2.4790585041046143\n",
            "2.42299747467041\n",
            "2.4678585529327393\n",
            "2.4426968097686768\n",
            "2.45802903175354\n",
            "2.453031539916992\n",
            "2.5336005687713623\n",
            "2.357414960861206\n",
            "2.5082600116729736\n",
            "2.3548519611358643\n",
            "2.426046848297119\n",
            "2.5552830696105957\n",
            "2.4870033264160156\n",
            "2.4732229709625244\n",
            "2.4391069412231445\n",
            "2.5774128437042236\n",
            "2.3678061962127686\n",
            "2.3911798000335693\n",
            "2.578768730163574\n",
            "2.4050309658050537\n",
            "2.3568496704101562\n",
            "2.4922478199005127\n",
            "2.4274837970733643\n",
            "2.609063148498535\n",
            "2.4139771461486816\n",
            "2.428779125213623\n",
            "2.516758441925049\n",
            "2.3438432216644287\n",
            "2.3537189960479736\n",
            "2.505223035812378\n",
            "2.4171969890594482\n",
            "2.44522762298584\n",
            "2.514294147491455\n",
            "2.4897775650024414\n",
            "2.570761203765869\n",
            "2.50585675239563\n",
            "2.4398231506347656\n",
            "2.50551438331604\n",
            "2.5377376079559326\n",
            "2.4248178005218506\n",
            "2.3998029232025146\n",
            "2.5934481620788574\n",
            "2.512390375137329\n",
            "2.4614713191986084\n",
            "2.569655418395996\n",
            "2.476088762283325\n",
            "2.4892382621765137\n",
            "2.45176362991333\n",
            "2.4678802490234375\n",
            "2.521411418914795\n",
            "2.406153678894043\n",
            "2.469965934753418\n",
            "2.393458843231201\n",
            "2.5507936477661133\n",
            "2.459683418273926\n",
            "2.536285161972046\n",
            "2.333705186843872\n",
            "2.6325535774230957\n",
            "2.4203455448150635\n",
            "2.5067813396453857\n",
            "2.459979295730591\n",
            "2.6248950958251953\n",
            "2.4707350730895996\n",
            "2.452212333679199\n",
            "2.5440995693206787\n",
            "2.588353157043457\n",
            "2.5094127655029297\n",
            "2.4499707221984863\n",
            "2.4503629207611084\n",
            "2.4488930702209473\n",
            "2.4559311866760254\n",
            "2.6145572662353516\n",
            "2.3637855052948\n",
            "2.475501775741577\n",
            "2.491472005844116\n",
            "2.5502140522003174\n",
            "2.385946035385132\n",
            "2.417046546936035\n",
            "2.4994313716888428\n",
            "2.5165793895721436\n",
            "2.5041208267211914\n",
            "2.543447732925415\n",
            "2.432924270629883\n",
            "2.4958343505859375\n",
            "2.6343836784362793\n",
            "2.5114033222198486\n",
            "2.445624589920044\n",
            "2.43740177154541\n",
            "2.422802448272705\n",
            "2.3881020545959473\n",
            "2.5798192024230957\n",
            "2.357475757598877\n",
            "2.380971908569336\n",
            "2.4366369247436523\n",
            "2.4884402751922607\n",
            "2.4427263736724854\n",
            "2.501453161239624\n",
            "2.5228724479675293\n",
            "2.5452425479888916\n",
            "2.5246336460113525\n",
            "2.4616408348083496\n",
            "2.508967161178589\n",
            "2.3833484649658203\n",
            "2.570711851119995\n",
            "2.407837390899658\n",
            "2.3775339126586914\n",
            "2.449631929397583\n",
            "2.369786024093628\n",
            "2.4730794429779053\n",
            "2.4454429149627686\n",
            "2.5513112545013428\n",
            "2.495467185974121\n",
            "2.319661855697632\n",
            "2.4943196773529053\n",
            "2.417534589767456\n",
            "2.438563823699951\n",
            "2.409519910812378\n",
            "2.419738531112671\n",
            "2.5354163646698\n",
            "2.3882105350494385\n",
            "2.337022304534912\n",
            "2.3659138679504395\n",
            "2.3895480632781982\n",
            "2.4300496578216553\n",
            "2.41165828704834\n",
            "2.4201815128326416\n",
            "2.5346570014953613\n",
            "2.5505058765411377\n",
            "2.4753894805908203\n",
            "2.392845869064331\n",
            "2.388463258743286\n",
            "2.510830879211426\n",
            "2.4877586364746094\n",
            "2.3023829460144043\n",
            "2.460034132003784\n",
            "2.5650155544281006\n",
            "2.3870394229888916\n",
            "2.3555564880371094\n",
            "2.426213026046753\n",
            "2.426454544067383\n",
            "2.4844138622283936\n",
            "2.4723243713378906\n",
            "2.4218757152557373\n",
            "2.5322158336639404\n",
            "2.4643540382385254\n",
            "2.4822802543640137\n",
            "2.5051474571228027\n",
            "2.3999085426330566\n",
            "2.412607431411743\n",
            "2.486640691757202\n",
            "2.363604784011841\n",
            "2.3634700775146484\n",
            "2.4331846237182617\n",
            "2.3899002075195312\n",
            "2.5545647144317627\n",
            "2.450140953063965\n",
            "2.44179368019104\n",
            "2.3442625999450684\n",
            "2.4314768314361572\n",
            "2.4079320430755615\n",
            "2.5144810676574707\n",
            "2.5318655967712402\n",
            "2.3659274578094482\n",
            "2.4239816665649414\n",
            "2.4652316570281982\n",
            "2.5153350830078125\n",
            "2.456149101257324\n",
            "2.4740045070648193\n",
            "2.478059768676758\n",
            "2.497103691101074\n",
            "2.6189255714416504\n",
            "2.4165234565734863\n",
            "2.5530641078948975\n",
            "2.5920705795288086\n",
            "2.4936962127685547\n",
            "2.5267832279205322\n",
            "2.2990243434906006\n",
            "2.451200008392334\n",
            "2.4306888580322266\n",
            "2.4700841903686523\n",
            "2.6003828048706055\n",
            "2.431641101837158\n",
            "2.4614007472991943\n",
            "2.479661226272583\n",
            "2.530876636505127\n",
            "2.382087469100952\n",
            "2.409799337387085\n",
            "2.53173828125\n",
            "2.416753053665161\n",
            "2.57949161529541\n",
            "2.54187273979187\n",
            "2.5386860370635986\n",
            "2.449537515640259\n",
            "2.486727237701416\n",
            "2.4906511306762695\n",
            "2.574204444885254\n",
            "2.4276459217071533\n",
            "2.364488363265991\n",
            "2.496940851211548\n",
            "2.410087823867798\n",
            "2.5234439373016357\n",
            "2.6294171810150146\n",
            "2.592034101486206\n",
            "2.4545235633850098\n",
            "2.375614881515503\n",
            "2.4980521202087402\n",
            "2.4514408111572266\n",
            "2.3891441822052\n",
            "2.436509847640991\n",
            "2.536548137664795\n",
            "2.365773916244507\n",
            "2.4805774688720703\n",
            "2.4087705612182617\n",
            "2.3365674018859863\n",
            "2.556295871734619\n",
            "2.4761931896209717\n",
            "2.512139320373535\n",
            "2.5014705657958984\n",
            "2.6258621215820312\n",
            "2.4913480281829834\n",
            "2.4691598415374756\n",
            "2.446319580078125\n",
            "2.3499512672424316\n",
            "2.542369842529297\n",
            "2.548456907272339\n",
            "2.4850196838378906\n",
            "2.509291648864746\n",
            "2.5905895233154297\n",
            "2.3458073139190674\n",
            "2.458498001098633\n",
            "2.4899582862854004\n",
            "2.4479007720947266\n",
            "2.402060031890869\n",
            "2.4690799713134766\n",
            "2.5231435298919678\n",
            "2.424818754196167\n",
            "2.3733906745910645\n",
            "2.4550094604492188\n",
            "2.459096670150757\n",
            "2.4586281776428223\n",
            "2.451472759246826\n",
            "2.509455919265747\n",
            "2.587118148803711\n",
            "2.4662508964538574\n",
            "2.414536714553833\n",
            "2.4456210136413574\n",
            "2.4908199310302734\n",
            "2.5025224685668945\n",
            "2.350252866744995\n",
            "2.437445640563965\n",
            "2.516817092895508\n",
            "2.4808878898620605\n",
            "2.47214412689209\n",
            "2.3381872177124023\n",
            "2.363830089569092\n",
            "2.4152584075927734\n",
            "2.4542081356048584\n",
            "2.4467689990997314\n",
            "2.5441219806671143\n",
            "2.429983139038086\n",
            "2.400799512863159\n",
            "2.5284903049468994\n",
            "2.3757174015045166\n",
            "2.5383102893829346\n",
            "2.4977190494537354\n",
            "2.496157169342041\n",
            "2.460947036743164\n",
            "2.4841485023498535\n",
            "2.374281883239746\n",
            "2.5353643894195557\n",
            "2.464238166809082\n",
            "2.4555513858795166\n",
            "2.4606521129608154\n",
            "2.393561363220215\n",
            "2.4812707901000977\n",
            "2.332028388977051\n",
            "2.4476897716522217\n",
            "2.518984794616699\n",
            "2.515336036682129\n",
            "2.4131226539611816\n",
            "2.425222158432007\n",
            "2.4538774490356445\n",
            "2.4893219470977783\n",
            "2.4885449409484863\n",
            "2.427065849304199\n",
            "2.407186985015869\n",
            "2.462686538696289\n",
            "2.539341688156128\n",
            "2.4399218559265137\n",
            "2.479107618331909\n",
            "2.5133514404296875\n",
            "2.5369741916656494\n",
            "2.464714288711548\n",
            "2.5229904651641846\n",
            "2.382143020629883\n",
            "2.463686227798462\n",
            "2.4350016117095947\n",
            "2.4375810623168945\n",
            "2.5079848766326904\n",
            "2.4290575981140137\n",
            "2.426406145095825\n",
            "2.413831949234009\n",
            "2.4890074729919434\n",
            "2.4772250652313232\n",
            "2.5648038387298584\n",
            "2.5337140560150146\n",
            "2.4380667209625244\n",
            "2.5581307411193848\n",
            "2.631535291671753\n",
            "2.5251944065093994\n",
            "2.3686442375183105\n",
            "2.6612050533294678\n",
            "2.349144220352173\n",
            "2.5177857875823975\n",
            "2.4996185302734375\n",
            "2.503221035003662\n",
            "2.378599166870117\n",
            "2.484081745147705\n",
            "2.380709409713745\n",
            "2.3902974128723145\n",
            "2.422769069671631\n",
            "2.62119722366333\n",
            "2.442200183868408\n",
            "2.3521995544433594\n",
            "2.446247100830078\n",
            "2.411573648452759\n",
            "2.272653818130493\n",
            "2.453885793685913\n",
            "2.435009717941284\n",
            "2.4999306201934814\n",
            "2.4987902641296387\n",
            "2.459707260131836\n",
            "2.5385305881500244\n",
            "2.5279335975646973\n",
            "2.3731322288513184\n",
            "2.550774097442627\n",
            "2.538156747817993\n",
            "2.498915433883667\n",
            "2.3643946647644043\n",
            "2.425699472427368\n",
            "2.5133438110351562\n",
            "2.6229193210601807\n",
            "2.4431488513946533\n",
            "2.5435566902160645\n",
            "2.449389696121216\n",
            "2.540569305419922\n",
            "2.5730650424957275\n",
            "2.4056923389434814\n",
            "2.422163248062134\n",
            "2.5161800384521484\n",
            "2.499601364135742\n",
            "2.4362633228302\n",
            "2.4079744815826416\n",
            "2.4821219444274902\n",
            "2.365227699279785\n",
            "2.429266929626465\n",
            "2.4341797828674316\n",
            "2.44447660446167\n",
            "2.469036102294922\n",
            "2.3778445720672607\n",
            "2.469937562942505\n",
            "2.371887445449829\n",
            "2.5265393257141113\n",
            "2.5890495777130127\n",
            "2.5926601886749268\n",
            "2.426919460296631\n",
            "2.5269033908843994\n",
            "2.3917267322540283\n",
            "2.522608518600464\n",
            "2.4285054206848145\n",
            "2.6041014194488525\n",
            "2.3676655292510986\n",
            "2.412417411804199\n",
            "2.470282793045044\n",
            "2.419179677963257\n",
            "2.3859050273895264\n",
            "2.5765445232391357\n",
            "2.469280958175659\n",
            "2.3570704460144043\n",
            "2.4522504806518555\n",
            "2.4682343006134033\n",
            "2.422466278076172\n",
            "2.5337979793548584\n",
            "2.4907522201538086\n",
            "2.3574838638305664\n",
            "2.4814767837524414\n",
            "2.5322799682617188\n",
            "2.4354803562164307\n",
            "2.436223268508911\n",
            "2.5195186138153076\n",
            "2.3814456462860107\n",
            "2.53027606010437\n",
            "2.3043549060821533\n",
            "2.4602839946746826\n",
            "2.4383556842803955\n",
            "2.4701802730560303\n",
            "2.5196876525878906\n",
            "2.5613393783569336\n",
            "2.4627020359039307\n",
            "2.4038774967193604\n",
            "2.4575843811035156\n",
            "2.476001262664795\n",
            "2.420672655105591\n",
            "2.5088696479797363\n",
            "2.3823859691619873\n",
            "2.345937967300415\n",
            "2.4725759029388428\n",
            "2.553133249282837\n",
            "2.5044779777526855\n",
            "2.500828266143799\n",
            "2.4467315673828125\n",
            "2.3994431495666504\n",
            "2.5492825508117676\n",
            "2.4202208518981934\n",
            "2.3459219932556152\n",
            "2.540802240371704\n",
            "2.3874568939208984\n",
            "2.5151519775390625\n",
            "2.440981149673462\n",
            "2.5443601608276367\n",
            "2.462611675262451\n",
            "2.567890167236328\n",
            "2.4331839084625244\n",
            "2.414774179458618\n",
            "2.5801117420196533\n",
            "2.4140820503234863\n",
            "2.439284324645996\n",
            "2.4293694496154785\n",
            "2.45902419090271\n",
            "2.4407567977905273\n",
            "2.5903868675231934\n",
            "2.4308371543884277\n",
            "2.4491896629333496\n",
            "2.40659236907959\n",
            "2.5430264472961426\n",
            "2.4461936950683594\n",
            "2.42549991607666\n",
            "2.475524425506592\n",
            "2.5720126628875732\n",
            "2.3986258506774902\n",
            "2.4975476264953613\n",
            "2.432218074798584\n",
            "2.37601375579834\n",
            "2.5358731746673584\n",
            "2.407224655151367\n",
            "2.563559055328369\n",
            "2.380046844482422\n",
            "2.489671230316162\n",
            "2.4015867710113525\n",
            "2.483858823776245\n",
            "2.4636621475219727\n",
            "2.5861597061157227\n",
            "2.5000226497650146\n",
            "2.349856376647949\n",
            "2.532092571258545\n",
            "2.6099088191986084\n",
            "2.5738208293914795\n",
            "2.466841220855713\n",
            "2.482343912124634\n",
            "2.4715685844421387\n",
            "2.3272995948791504\n",
            "2.5697200298309326\n",
            "2.3331291675567627\n",
            "2.4550459384918213\n",
            "2.453579902648926\n",
            "2.4156670570373535\n",
            "2.468623161315918\n",
            "2.5615127086639404\n",
            "2.653430223464966\n",
            "2.39176607131958\n",
            "2.654101848602295\n",
            "2.391876220703125\n",
            "2.573367118835449\n",
            "2.5286519527435303\n",
            "2.5374867916107178\n",
            "2.553842544555664\n",
            "2.4621975421905518\n",
            "2.431290626525879\n",
            "2.5520565509796143\n",
            "2.4031858444213867\n",
            "2.378857374191284\n",
            "2.5653398036956787\n",
            "2.432640552520752\n",
            "2.439275026321411\n",
            "2.4851436614990234\n",
            "2.638489246368408\n",
            "2.61911678314209\n",
            "2.5236382484436035\n",
            "2.459951400756836\n",
            "2.434037923812866\n",
            "2.493408441543579\n",
            "2.5383434295654297\n",
            "2.446415901184082\n",
            "2.4058611392974854\n",
            "2.5304582118988037\n",
            "2.394700765609741\n",
            "2.5168163776397705\n",
            "2.4492762088775635\n",
            "2.352296829223633\n",
            "2.44085955619812\n",
            "2.556823253631592\n",
            "2.4310145378112793\n",
            "2.333003282546997\n",
            "2.5280871391296387\n",
            "2.439711809158325\n",
            "2.4763712882995605\n",
            "2.4117767810821533\n",
            "2.4883840084075928\n",
            "2.5798428058624268\n",
            "2.492945909500122\n",
            "2.442605495452881\n",
            "2.4768972396850586\n",
            "2.510716199874878\n",
            "2.590503215789795\n",
            "2.4138875007629395\n",
            "2.4390101432800293\n",
            "2.4442222118377686\n",
            "2.496863603591919\n",
            "2.5825095176696777\n",
            "2.4225175380706787\n",
            "2.476074457168579\n",
            "2.531574010848999\n",
            "2.4067375659942627\n",
            "2.269846200942993\n",
            "2.625072479248047\n",
            "2.4821791648864746\n",
            "2.5380923748016357\n",
            "2.5003151893615723\n",
            "2.5969650745391846\n",
            "2.4697556495666504\n",
            "2.4107654094696045\n",
            "2.4878673553466797\n",
            "2.4801480770111084\n",
            "2.496687173843384\n",
            "2.339285373687744\n",
            "2.454129695892334\n",
            "2.4614272117614746\n",
            "2.528440237045288\n",
            "2.4079349040985107\n",
            "2.4801673889160156\n",
            "2.3987531661987305\n",
            "2.3625078201293945\n",
            "2.4225950241088867\n",
            "2.360541582107544\n",
            "2.342151641845703\n",
            "2.435537815093994\n",
            "2.4752931594848633\n",
            "2.6075587272644043\n",
            "2.4524643421173096\n",
            "2.4412879943847656\n",
            "2.4279916286468506\n",
            "2.3403687477111816\n",
            "2.346756935119629\n",
            "2.495842933654785\n",
            "2.5220227241516113\n",
            "2.5053043365478516\n",
            "2.4569785594940186\n",
            "2.4696476459503174\n",
            "2.5093510150909424\n",
            "2.4257726669311523\n",
            "2.3960673809051514\n",
            "2.5467350482940674\n",
            "2.4071903228759766\n",
            "2.4684650897979736\n",
            "2.4792282581329346\n",
            "2.5683696269989014\n",
            "2.496262550354004\n",
            "2.449708938598633\n",
            "2.4840192794799805\n",
            "2.4479176998138428\n",
            "2.443465232849121\n",
            "2.473292589187622\n",
            "2.49466609954834\n",
            "2.6206536293029785\n",
            "2.3246212005615234\n",
            "2.553654193878174\n",
            "2.471862554550171\n",
            "2.351078987121582\n",
            "2.623504161834717\n",
            "2.62933611869812\n",
            "2.510669469833374\n",
            "2.447268486022949\n",
            "2.4940876960754395\n",
            "2.4235408306121826\n",
            "2.5387425422668457\n",
            "2.3473567962646484\n",
            "2.4348275661468506\n",
            "2.4105417728424072\n",
            "2.3780081272125244\n",
            "2.412630319595337\n",
            "2.5123612880706787\n",
            "2.5692636966705322\n",
            "2.474350929260254\n",
            "2.647738456726074\n",
            "2.4915716648101807\n",
            "2.406414270401001\n",
            "2.4312024116516113\n",
            "2.3772358894348145\n",
            "2.412566900253296\n",
            "2.3863630294799805\n",
            "2.495852470397949\n",
            "2.525740623474121\n",
            "2.5538430213928223\n",
            "2.508591890335083\n",
            "2.4816038608551025\n",
            "2.592301607131958\n",
            "2.3859939575195312\n",
            "2.430020332336426\n",
            "2.4533121585845947\n",
            "2.486499547958374\n",
            "2.486201047897339\n",
            "2.4317781925201416\n",
            "2.4243714809417725\n",
            "2.5174639225006104\n",
            "2.408596992492676\n",
            "2.5165915489196777\n",
            "2.469259738922119\n",
            "2.5739715099334717\n",
            "2.4203503131866455\n",
            "2.500661611557007\n",
            "2.430356502532959\n",
            "2.4512696266174316\n",
            "2.3883023262023926\n",
            "2.49713134765625\n",
            "2.461404800415039\n",
            "2.383103847503662\n",
            "2.483844518661499\n",
            "2.456531286239624\n",
            "2.502120018005371\n",
            "2.419438600540161\n",
            "2.541257619857788\n",
            "2.357532501220703\n",
            "2.445469856262207\n",
            "2.515981912612915\n",
            "2.533344030380249\n",
            "2.6707043647766113\n",
            "2.5768990516662598\n",
            "2.435635805130005\n",
            "2.397712230682373\n",
            "2.449747085571289\n",
            "2.523016929626465\n",
            "2.4723918437957764\n",
            "2.4256680011749268\n",
            "2.423753261566162\n",
            "2.5380606651306152\n",
            "2.3951892852783203\n",
            "2.4804725646972656\n",
            "2.5211758613586426\n",
            "2.436699628829956\n",
            "2.5807414054870605\n",
            "2.3454201221466064\n",
            "2.512420177459717\n",
            "2.3261911869049072\n",
            "2.5806140899658203\n",
            "2.3492321968078613\n",
            "2.39428448677063\n",
            "2.4627668857574463\n",
            "2.433356523513794\n",
            "2.30889892578125\n",
            "2.5143585205078125\n",
            "2.364427328109741\n",
            "2.482776165008545\n",
            "2.4942352771759033\n",
            "2.53151798248291\n",
            "2.4961469173431396\n",
            "2.371807336807251\n",
            "2.4050369262695312\n",
            "2.33831787109375\n",
            "2.4176645278930664\n",
            "2.462639570236206\n",
            "2.5527913570404053\n",
            "2.398782730102539\n",
            "2.459007978439331\n",
            "2.4780139923095703\n",
            "2.6106085777282715\n",
            "2.492089033126831\n",
            "2.674243688583374\n",
            "2.436662435531616\n",
            "2.4805662631988525\n",
            "2.4594526290893555\n",
            "2.4755008220672607\n",
            "2.501601457595825\n",
            "2.4251482486724854\n",
            "2.3558852672576904\n",
            "2.3732361793518066\n",
            "2.328643560409546\n",
            "2.475212574005127\n",
            "2.434492349624634\n",
            "2.5572078227996826\n",
            "2.508944272994995\n",
            "2.4589765071868896\n",
            "2.4489364624023438\n",
            "2.4716224670410156\n",
            "2.53926157951355\n",
            "2.5143563747406006\n",
            "2.4463539123535156\n",
            "2.3600027561187744\n",
            "2.389460802078247\n",
            "2.4109370708465576\n",
            "2.5248539447784424\n",
            "2.409127950668335\n",
            "2.417133092880249\n",
            "2.4093339443206787\n",
            "2.4180691242218018\n",
            "2.3215982913970947\n",
            "2.4760448932647705\n",
            "2.42165207862854\n",
            "2.5202715396881104\n",
            "2.4407708644866943\n",
            "2.4434194564819336\n",
            "2.525177240371704\n",
            "2.3724756240844727\n",
            "2.4337210655212402\n",
            "2.4697110652923584\n",
            "2.4690022468566895\n",
            "2.4497766494750977\n",
            "2.341869592666626\n",
            "2.4189298152923584\n",
            "2.4954028129577637\n",
            "2.4055705070495605\n",
            "2.3685531616210938\n",
            "2.5285420417785645\n",
            "2.575455665588379\n",
            "2.4794461727142334\n",
            "2.4437344074249268\n",
            "2.499934434890747\n",
            "2.4459400177001953\n",
            "2.4486277103424072\n",
            "2.5707521438598633\n",
            "2.508038282394409\n",
            "2.5300886631011963\n",
            "2.3623385429382324\n",
            "2.530397415161133\n",
            "2.5018739700317383\n",
            "2.299238443374634\n",
            "2.3860459327697754\n",
            "2.484497308731079\n",
            "2.443047046661377\n",
            "2.6036458015441895\n",
            "2.431328296661377\n",
            "2.4959073066711426\n",
            "2.414935350418091\n",
            "2.4526009559631348\n",
            "2.528968095779419\n",
            "2.505908966064453\n",
            "2.4229586124420166\n",
            "2.5057172775268555\n",
            "2.4356157779693604\n",
            "2.374673843383789\n",
            "2.43558669090271\n",
            "2.5685479640960693\n",
            "2.5662500858306885\n",
            "2.5770325660705566\n",
            "2.5135343074798584\n",
            "2.5360231399536133\n",
            "2.3384597301483154\n",
            "2.394470691680908\n",
            "2.4223878383636475\n",
            "2.463620901107788\n",
            "2.382878303527832\n",
            "2.468273639678955\n",
            "2.4788589477539062\n",
            "2.60679030418396\n",
            "2.4151837825775146\n",
            "2.5521886348724365\n",
            "2.450740098953247\n",
            "2.498769521713257\n",
            "2.464505434036255\n",
            "2.4968349933624268\n",
            "2.471226692199707\n",
            "2.349377155303955\n",
            "2.4962379932403564\n",
            "2.395655870437622\n",
            "2.4249043464660645\n",
            "2.451659679412842\n",
            "2.5428905487060547\n",
            "2.430104970932007\n",
            "2.3436131477355957\n",
            "2.483872413635254\n",
            "2.5573642253875732\n",
            "2.434619903564453\n",
            "2.4937989711761475\n",
            "2.5691144466400146\n",
            "2.4717729091644287\n",
            "2.4620554447174072\n",
            "2.601517677307129\n",
            "2.3906760215759277\n",
            "2.5461857318878174\n",
            "2.4222218990325928\n",
            "2.401744842529297\n",
            "2.483107566833496\n",
            "2.4714951515197754\n",
            "2.5870983600616455\n",
            "2.5045433044433594\n",
            "2.511486053466797\n",
            "2.5419273376464844\n",
            "2.53444766998291\n",
            "2.3921008110046387\n",
            "2.598541021347046\n",
            "2.4909801483154297\n",
            "2.450744390487671\n",
            "2.5196967124938965\n",
            "2.463284492492676\n",
            "2.5116071701049805\n",
            "2.577272891998291\n",
            "2.3519201278686523\n",
            "2.522578239440918\n",
            "2.542137384414673\n",
            "2.461388111114502\n",
            "2.5220792293548584\n",
            "2.5447304248809814\n",
            "2.4462497234344482\n",
            "2.4556996822357178\n",
            "2.3986027240753174\n",
            "2.5128166675567627\n",
            "2.588945150375366\n",
            "2.542752742767334\n",
            "2.3530285358428955\n",
            "2.4147565364837646\n",
            "2.4221198558807373\n",
            "2.249384880065918\n",
            "2.4302451610565186\n",
            "2.4531443119049072\n",
            "2.524284839630127\n",
            "2.5862503051757812\n",
            "2.4548463821411133\n",
            "2.357490062713623\n",
            "2.5615580081939697\n",
            "2.5328757762908936\n",
            "2.545792579650879\n",
            "2.4087796211242676\n",
            "2.4976465702056885\n",
            "2.40220046043396\n",
            "2.4988183975219727\n",
            "2.51755952835083\n",
            "2.4203193187713623\n",
            "2.496095657348633\n",
            "2.4671881198883057\n",
            "2.642910957336426\n",
            "2.340446949005127\n",
            "2.5086803436279297\n",
            "2.469359874725342\n",
            "2.3971707820892334\n",
            "2.384044885635376\n",
            "2.4255075454711914\n",
            "2.415602684020996\n",
            "2.6193881034851074\n",
            "2.400331735610962\n",
            "2.4108681678771973\n",
            "2.604673147201538\n",
            "2.443678855895996\n",
            "2.351475477218628\n",
            "2.571428060531616\n",
            "2.456066846847534\n",
            "2.4184916019439697\n",
            "2.474668025970459\n",
            "2.5067338943481445\n",
            "2.486849784851074\n",
            "2.420369863510132\n",
            "2.4410462379455566\n",
            "2.4224631786346436\n",
            "2.4247889518737793\n",
            "2.4050488471984863\n",
            "2.5498054027557373\n",
            "2.3902335166931152\n",
            "2.549238681793213\n",
            "2.3505442142486572\n",
            "2.4558346271514893\n",
            "2.536912441253662\n",
            "2.489116907119751\n",
            "2.4741604328155518\n",
            "2.477170705795288\n",
            "2.4636340141296387\n",
            "2.458338975906372\n",
            "2.3515331745147705\n",
            "2.545654773712158\n",
            "2.536503791809082\n",
            "2.544672727584839\n",
            "2.5480706691741943\n",
            "2.4735524654388428\n",
            "2.515841484069824\n",
            "2.435171127319336\n",
            "2.4293172359466553\n",
            "2.4289777278900146\n",
            "2.432703733444214\n",
            "2.5488033294677734\n",
            "2.516589879989624\n",
            "2.3295109272003174\n",
            "2.417778968811035\n",
            "2.439387559890747\n",
            "2.46584153175354\n",
            "2.478189468383789\n",
            "2.4607348442077637\n",
            "2.4490036964416504\n",
            "2.5164899826049805\n",
            "2.4197158813476562\n",
            "2.5380778312683105\n",
            "2.4495580196380615\n",
            "2.528289794921875\n",
            "2.3921544551849365\n",
            "2.5470337867736816\n",
            "2.432277202606201\n",
            "2.4891774654388428\n",
            "2.4462552070617676\n",
            "2.5464229583740234\n",
            "2.580554962158203\n",
            "2.5019164085388184\n",
            "2.468324661254883\n",
            "2.5145676136016846\n",
            "2.4392993450164795\n",
            "2.3774139881134033\n",
            "2.4381439685821533\n",
            "2.5719187259674072\n",
            "2.4665231704711914\n",
            "2.5358328819274902\n",
            "2.4551310539245605\n",
            "2.52165150642395\n",
            "2.426086187362671\n",
            "2.5758068561553955\n",
            "2.4179346561431885\n",
            "2.4435324668884277\n",
            "2.365396499633789\n",
            "2.4006757736206055\n",
            "2.4075517654418945\n",
            "2.421809196472168\n",
            "2.5222558975219727\n",
            "2.557356119155884\n",
            "2.3518943786621094\n",
            "2.3591320514678955\n",
            "2.4411656856536865\n",
            "2.5447866916656494\n",
            "2.3485727310180664\n",
            "2.5147104263305664\n",
            "2.5135111808776855\n",
            "2.525768756866455\n",
            "2.4468679428100586\n",
            "2.468343496322632\n",
            "2.4366838932037354\n",
            "2.4594340324401855\n",
            "2.469120979309082\n",
            "2.4519577026367188\n",
            "2.5261898040771484\n",
            "2.4807989597320557\n",
            "2.3826181888580322\n",
            "2.472778081893921\n",
            "2.386568069458008\n",
            "2.593381881713867\n",
            "2.430506467819214\n",
            "2.552124500274658\n",
            "2.6196000576019287\n",
            "2.535468339920044\n",
            "2.4266202449798584\n",
            "2.499842405319214\n",
            "2.5008022785186768\n",
            "2.5481393337249756\n",
            "2.5317609310150146\n",
            "2.4347400665283203\n",
            "2.4375758171081543\n",
            "2.558640956878662\n",
            "2.481743574142456\n",
            "2.490889072418213\n",
            "2.6401960849761963\n",
            "2.533618688583374\n",
            "2.3922483921051025\n",
            "2.548924684524536\n",
            "2.469496011734009\n",
            "2.3935129642486572\n",
            "2.4822921752929688\n",
            "2.5252482891082764\n",
            "2.469513177871704\n",
            "2.485424757003784\n",
            "2.4317104816436768\n",
            "2.4921584129333496\n",
            "2.5642812252044678\n",
            "2.430626153945923\n",
            "2.471285820007324\n",
            "2.573293924331665\n",
            "2.455444574356079\n",
            "2.399399995803833\n",
            "2.3931541442871094\n",
            "2.452958583831787\n",
            "2.37455153465271\n",
            "2.519503355026245\n",
            "2.418961763381958\n",
            "2.4557204246520996\n",
            "2.473782777786255\n",
            "2.4088339805603027\n",
            "2.4560365676879883\n",
            "2.4941611289978027\n",
            "2.3825292587280273\n",
            "2.4715490341186523\n",
            "2.3522520065307617\n",
            "2.5040974617004395\n",
            "2.3903768062591553\n",
            "2.3370134830474854\n",
            "2.467102289199829\n",
            "2.5701873302459717\n",
            "2.5635268688201904\n",
            "2.591848134994507\n",
            "2.4551188945770264\n",
            "2.396313190460205\n",
            "2.4255645275115967\n",
            "2.493023633956909\n",
            "2.5176076889038086\n",
            "2.478947877883911\n",
            "2.422302722930908\n",
            "2.473245859146118\n",
            "2.303635835647583\n",
            "2.3488080501556396\n",
            "2.422405958175659\n",
            "2.5228569507598877\n",
            "2.4603090286254883\n",
            "2.537264347076416\n",
            "2.5102334022521973\n",
            "2.44989275932312\n",
            "2.462327241897583\n",
            "2.4467849731445312\n",
            "2.5499560832977295\n",
            "2.6237385272979736\n",
            "2.5460104942321777\n",
            "2.5121889114379883\n",
            "2.4343321323394775\n",
            "2.5137791633605957\n",
            "2.484096050262451\n",
            "2.357273817062378\n",
            "2.4097647666931152\n",
            "2.4500489234924316\n",
            "2.498534917831421\n",
            "2.5159029960632324\n",
            "2.6368799209594727\n",
            "2.458881378173828\n",
            "2.5149497985839844\n",
            "2.4719831943511963\n",
            "2.3535380363464355\n",
            "2.335045099258423\n",
            "2.465555191040039\n",
            "2.440415382385254\n",
            "2.4299349784851074\n",
            "2.3242812156677246\n",
            "2.3576314449310303\n",
            "2.6164863109588623\n",
            "2.525967836380005\n",
            "2.545898914337158\n",
            "2.4980387687683105\n",
            "2.4762017726898193\n",
            "2.5103492736816406\n",
            "2.4605374336242676\n",
            "2.5393497943878174\n",
            "2.4784326553344727\n",
            "2.420616388320923\n",
            "2.4387431144714355\n",
            "2.5217156410217285\n",
            "2.559213638305664\n",
            "2.4496142864227295\n",
            "2.5301945209503174\n",
            "2.3793816566467285\n",
            "2.553323268890381\n",
            "2.4573917388916016\n",
            "2.3569562435150146\n",
            "2.517118453979492\n",
            "2.461148500442505\n",
            "2.487703800201416\n",
            "2.4186463356018066\n",
            "2.4251978397369385\n",
            "2.5024514198303223\n",
            "2.4131076335906982\n",
            "2.473620653152466\n",
            "2.481316089630127\n",
            "2.4767839908599854\n",
            "2.449965000152588\n",
            "2.484191656112671\n",
            "2.466885805130005\n",
            "2.4965884685516357\n",
            "2.42505145072937\n",
            "2.4534173011779785\n",
            "2.3305811882019043\n",
            "2.4317071437835693\n",
            "2.3898110389709473\n",
            "2.315377712249756\n",
            "2.4804763793945312\n",
            "2.517580986022949\n",
            "2.3001296520233154\n",
            "2.339020013809204\n",
            "2.5916409492492676\n",
            "2.3573150634765625\n",
            "2.5114686489105225\n",
            "2.445103883743286\n",
            "2.45035982131958\n",
            "2.2337992191314697\n",
            "2.4313666820526123\n",
            "2.501938819885254\n",
            "2.4674763679504395\n",
            "2.5199267864227295\n",
            "2.4863522052764893\n",
            "2.4177162647247314\n",
            "2.5389342308044434\n",
            "2.433917760848999\n",
            "2.476539134979248\n",
            "2.4604616165161133\n",
            "2.456188440322876\n",
            "2.5162315368652344\n",
            "2.3345730304718018\n",
            "2.6077795028686523\n",
            "2.4218459129333496\n",
            "2.3540098667144775\n",
            "2.5271384716033936\n",
            "2.5154001712799072\n",
            "2.424469232559204\n",
            "2.4583334922790527\n",
            "2.3319036960601807\n",
            "2.470050096511841\n",
            "2.416301727294922\n",
            "2.523771047592163\n",
            "2.437027931213379\n",
            "2.401972532272339\n",
            "2.493760347366333\n",
            "2.4791011810302734\n",
            "2.539532423019409\n",
            "2.5207364559173584\n",
            "2.515658378601074\n",
            "2.4537124633789062\n",
            "2.365316390991211\n",
            "2.5537164211273193\n",
            "2.454603910446167\n",
            "2.4707272052764893\n",
            "2.476884603500366\n",
            "2.566499948501587\n",
            "2.5322678089141846\n",
            "2.4690701961517334\n",
            "2.5680155754089355\n",
            "2.487053155899048\n",
            "2.3543152809143066\n",
            "2.4080095291137695\n",
            "2.476179838180542\n",
            "2.554478883743286\n",
            "2.407306671142578\n",
            "2.4766972064971924\n",
            "2.404545307159424\n",
            "2.552170515060425\n",
            "2.423868417739868\n",
            "2.42458438873291\n",
            "2.333054304122925\n",
            "2.59538197517395\n",
            "2.432464599609375\n",
            "2.405521869659424\n",
            "2.5697014331817627\n",
            "2.596888780593872\n",
            "2.4114534854888916\n",
            "2.4387848377227783\n",
            "2.47652006149292\n",
            "2.3700923919677734\n",
            "2.5896148681640625\n",
            "2.397665023803711\n",
            "2.4246227741241455\n",
            "2.386937141418457\n",
            "2.3855719566345215\n",
            "2.616046190261841\n",
            "2.4323129653930664\n",
            "2.414309024810791\n",
            "2.4926888942718506\n",
            "2.3532874584198\n",
            "2.49995756149292\n",
            "2.5495378971099854\n",
            "2.386028289794922\n",
            "2.453441858291626\n",
            "2.470597982406616\n",
            "2.3397350311279297\n",
            "2.4682769775390625\n",
            "2.5554144382476807\n",
            "2.5455408096313477\n",
            "2.545905351638794\n",
            "2.657869338989258\n",
            "2.463127851486206\n",
            "2.4098684787750244\n",
            "2.4580254554748535\n",
            "2.4462924003601074\n",
            "2.3913636207580566\n",
            "2.581408739089966\n",
            "2.563055992126465\n",
            "2.622258186340332\n",
            "2.4797163009643555\n",
            "2.5170793533325195\n",
            "2.3975980281829834\n",
            "2.4039504528045654\n",
            "2.604457139968872\n",
            "2.367901086807251\n",
            "2.3630385398864746\n",
            "2.4165337085723877\n",
            "2.5367977619171143\n",
            "2.507633686065674\n",
            "2.394178867340088\n",
            "2.541555643081665\n",
            "2.544025421142578\n",
            "2.3819150924682617\n",
            "2.2754597663879395\n",
            "2.382643461227417\n",
            "2.460700273513794\n",
            "2.428107976913452\n",
            "2.3011903762817383\n",
            "2.4948248863220215\n",
            "2.4164843559265137\n",
            "2.4432711601257324\n",
            "2.4356191158294678\n",
            "2.5512759685516357\n",
            "2.479987144470215\n",
            "2.562915325164795\n",
            "2.479743003845215\n",
            "2.3462696075439453\n",
            "2.446925401687622\n",
            "2.566589832305908\n",
            "2.4261083602905273\n",
            "2.305293321609497\n",
            "2.513331413269043\n",
            "2.41753888130188\n",
            "2.481518030166626\n",
            "2.3579001426696777\n",
            "2.432487964630127\n",
            "2.536107063293457\n",
            "2.4305222034454346\n",
            "2.4543542861938477\n",
            "2.5653367042541504\n",
            "2.5291876792907715\n",
            "2.356257200241089\n",
            "2.4580516815185547\n",
            "2.5649428367614746\n",
            "2.4643678665161133\n",
            "2.3755526542663574\n",
            "2.376511812210083\n",
            "2.474979877471924\n",
            "2.516448497772217\n",
            "2.496340036392212\n",
            "2.453007221221924\n",
            "2.4154436588287354\n",
            "2.442162036895752\n",
            "2.4695041179656982\n",
            "2.6182005405426025\n",
            "2.4332494735717773\n",
            "2.41705322265625\n",
            "2.5039660930633545\n",
            "2.447044610977173\n",
            "2.4454333782196045\n",
            "2.420848846435547\n",
            "2.429335832595825\n",
            "2.4125583171844482\n",
            "2.4414732456207275\n",
            "2.54091477394104\n",
            "2.5766260623931885\n",
            "2.431000232696533\n",
            "2.4302287101745605\n",
            "2.4024569988250732\n",
            "2.5684051513671875\n",
            "2.558062791824341\n",
            "2.4675543308258057\n",
            "2.5587105751037598\n",
            "2.5709176063537598\n",
            "2.422773599624634\n",
            "2.5925333499908447\n",
            "2.562335729598999\n",
            "2.492220163345337\n",
            "2.4484009742736816\n",
            "2.4213321208953857\n",
            "2.3948278427124023\n",
            "2.398529529571533\n",
            "2.5903396606445312\n",
            "2.5469136238098145\n",
            "2.5234570503234863\n",
            "2.4259214401245117\n",
            "2.5393590927124023\n",
            "2.41342830657959\n",
            "2.444755792617798\n",
            "2.41268253326416\n",
            "2.5649311542510986\n",
            "2.477245807647705\n",
            "2.340433120727539\n",
            "2.4761269092559814\n",
            "2.464752435684204\n",
            "2.5921850204467773\n",
            "2.3980464935302734\n",
            "2.5547430515289307\n",
            "2.4650909900665283\n",
            "2.5001220703125\n",
            "2.61541485786438\n",
            "2.4384074211120605\n",
            "2.455613851547241\n",
            "2.4371469020843506\n",
            "2.4982683658599854\n",
            "2.451637029647827\n",
            "2.4049553871154785\n",
            "2.362135648727417\n",
            "2.539161443710327\n",
            "2.465963125228882\n",
            "2.506413698196411\n",
            "2.4837241172790527\n",
            "2.476872444152832\n",
            "2.412503719329834\n",
            "2.4645326137542725\n",
            "2.404021739959717\n",
            "2.3992421627044678\n",
            "2.4121506214141846\n",
            "2.5588326454162598\n",
            "2.5240116119384766\n",
            "2.4025306701660156\n",
            "2.461075782775879\n",
            "2.476982355117798\n",
            "2.4916152954101562\n",
            "2.486166000366211\n",
            "2.4534013271331787\n",
            "2.528240919113159\n",
            "2.4473018646240234\n",
            "2.5865352153778076\n",
            "2.456409215927124\n",
            "2.5352680683135986\n",
            "2.517531633377075\n",
            "2.4195711612701416\n",
            "2.4271552562713623\n",
            "2.565347671508789\n",
            "2.3589396476745605\n",
            "2.369649648666382\n",
            "2.3638088703155518\n",
            "2.4870383739471436\n",
            "2.4059391021728516\n",
            "2.3347127437591553\n",
            "2.3222603797912598\n",
            "2.4740052223205566\n",
            "2.4116945266723633\n",
            "2.6713409423828125\n",
            "2.463542938232422\n",
            "2.452052593231201\n",
            "2.402299165725708\n",
            "2.4044320583343506\n",
            "2.437234878540039\n",
            "2.458202838897705\n",
            "2.553112745285034\n",
            "2.568251609802246\n",
            "2.4547007083892822\n",
            "2.4508841037750244\n",
            "2.3393988609313965\n",
            "2.5618326663970947\n",
            "2.4307925701141357\n",
            "2.5470130443573\n",
            "2.42539381980896\n",
            "2.460500955581665\n",
            "2.3882758617401123\n",
            "2.4479377269744873\n",
            "2.462712526321411\n",
            "2.638780117034912\n",
            "2.3573074340820312\n",
            "2.378696918487549\n",
            "2.3685553073883057\n",
            "2.3753552436828613\n",
            "2.606741189956665\n",
            "2.4222192764282227\n",
            "2.367112874984741\n",
            "2.5059654712677\n",
            "2.4009885787963867\n",
            "2.5209827423095703\n",
            "2.397078037261963\n",
            "2.3897693157196045\n",
            "2.435412883758545\n",
            "2.6173534393310547\n",
            "2.4249701499938965\n",
            "2.4683518409729004\n",
            "2.444666862487793\n",
            "2.396847724914551\n",
            "2.52661395072937\n",
            "2.54801607131958\n",
            "2.4987399578094482\n",
            "2.4446871280670166\n",
            "2.6959891319274902\n",
            "2.4521586894989014\n",
            "2.510660171508789\n",
            "2.433858871459961\n",
            "2.457362413406372\n",
            "2.549257755279541\n",
            "2.4142589569091797\n",
            "2.4835493564605713\n",
            "2.445380210876465\n",
            "2.389057159423828\n",
            "2.575035572052002\n",
            "2.4933922290802\n",
            "2.3476979732513428\n",
            "2.536144256591797\n",
            "2.5372602939605713\n",
            "2.4144341945648193\n",
            "2.534222364425659\n",
            "2.5027129650115967\n",
            "2.5865578651428223\n",
            "2.6402065753936768\n",
            "2.5573747158050537\n",
            "2.53082275390625\n",
            "2.5676825046539307\n",
            "2.496083974838257\n",
            "2.440060615539551\n",
            "2.49161958694458\n",
            "2.478494644165039\n",
            "2.4215006828308105\n",
            "2.5348989963531494\n",
            "2.5506527423858643\n",
            "2.5718541145324707\n",
            "2.4880521297454834\n",
            "2.463456630706787\n",
            "2.5209295749664307\n",
            "2.3792383670806885\n",
            "2.5046229362487793\n",
            "2.561842441558838\n",
            "2.4532411098480225\n",
            "2.379650354385376\n",
            "2.4768569469451904\n",
            "2.5025429725646973\n",
            "2.485255718231201\n",
            "2.508970022201538\n",
            "2.403985023498535\n",
            "2.4073095321655273\n",
            "2.5198307037353516\n",
            "2.5614798069000244\n",
            "2.7074601650238037\n",
            "2.5887467861175537\n",
            "2.42208194732666\n",
            "2.3633553981781006\n",
            "2.460803508758545\n",
            "2.4715828895568848\n",
            "2.39725661277771\n",
            "2.425330877304077\n",
            "2.4897093772888184\n",
            "2.492757797241211\n",
            "2.498476982116699\n",
            "2.4723358154296875\n",
            "2.471731662750244\n",
            "2.3834474086761475\n",
            "2.4526307582855225\n",
            "2.4471230506896973\n",
            "2.4666929244995117\n",
            "2.536439895629883\n",
            "2.5685603618621826\n",
            "2.465883731842041\n",
            "2.4372901916503906\n",
            "2.5831916332244873\n",
            "2.5253195762634277\n",
            "2.4138355255126953\n",
            "2.4082581996917725\n",
            "2.5024590492248535\n",
            "2.4852497577667236\n",
            "2.3453433513641357\n",
            "2.449122428894043\n",
            "2.482154130935669\n",
            "2.441025733947754\n",
            "2.5712244510650635\n",
            "2.4808666706085205\n",
            "2.4585087299346924\n",
            "2.395690679550171\n",
            "2.4752719402313232\n",
            "2.5097174644470215\n",
            "2.523470163345337\n",
            "2.523484468460083\n",
            "2.432813882827759\n",
            "2.424408197402954\n",
            "2.5136935710906982\n",
            "2.414849042892456\n",
            "2.4571433067321777\n",
            "2.491483449935913\n",
            "2.532297372817993\n",
            "2.5287232398986816\n",
            "2.4657959938049316\n",
            "2.3593671321868896\n",
            "2.4552228450775146\n",
            "2.5799787044525146\n",
            "2.435930013656616\n",
            "2.4880428314208984\n",
            "2.4379050731658936\n",
            "2.420753002166748\n",
            "2.4596853256225586\n",
            "2.501520872116089\n",
            "2.5212957859039307\n",
            "2.504242181777954\n",
            "2.457887649536133\n",
            "2.5932538509368896\n",
            "2.433466672897339\n",
            "2.381563186645508\n",
            "2.4059035778045654\n",
            "2.503936767578125\n",
            "2.4370739459991455\n",
            "2.5966665744781494\n",
            "2.4512155055999756\n",
            "2.5246362686157227\n",
            "2.482900381088257\n",
            "2.5554921627044678\n",
            "2.4469282627105713\n",
            "2.600554943084717\n",
            "2.3952813148498535\n",
            "2.367292881011963\n",
            "2.4597136974334717\n",
            "2.3289921283721924\n",
            "2.598008394241333\n",
            "2.424454927444458\n",
            "2.3097984790802\n",
            "2.3900160789489746\n",
            "2.432152271270752\n",
            "2.394374370574951\n",
            "2.469104528427124\n",
            "2.4893252849578857\n",
            "2.528761148452759\n",
            "2.4370927810668945\n",
            "2.490443229675293\n",
            "2.4810166358947754\n",
            "2.5464766025543213\n",
            "2.4796853065490723\n",
            "2.480005979537964\n",
            "2.4840242862701416\n",
            "2.580714464187622\n",
            "2.437354326248169\n",
            "2.519496440887451\n",
            "2.43471097946167\n",
            "2.49523663520813\n",
            "2.488720417022705\n",
            "2.5214643478393555\n",
            "2.4705147743225098\n",
            "2.442258596420288\n",
            "2.492419719696045\n",
            "2.416186809539795\n",
            "2.6295888423919678\n",
            "2.515982151031494\n",
            "2.3821561336517334\n",
            "2.4338536262512207\n",
            "2.376741886138916\n",
            "2.5703587532043457\n",
            "2.37563419342041\n",
            "2.488985300064087\n",
            "2.4510762691497803\n",
            "2.3922739028930664\n",
            "2.6159112453460693\n",
            "2.429257392883301\n",
            "2.4987752437591553\n",
            "2.4553658962249756\n",
            "2.4306981563568115\n",
            "2.4771018028259277\n",
            "2.484973430633545\n",
            "2.4727532863616943\n",
            "2.5089643001556396\n",
            "2.494392156600952\n",
            "2.515174150466919\n",
            "2.432485580444336\n",
            "2.432128667831421\n",
            "2.427995443344116\n",
            "2.539548873901367\n",
            "2.5716958045959473\n",
            "2.4915788173675537\n",
            "2.4059576988220215\n",
            "2.389045238494873\n",
            "2.4423019886016846\n",
            "2.426036834716797\n",
            "2.5895919799804688\n",
            "2.503244638442993\n",
            "2.4751689434051514\n",
            "2.545318126678467\n",
            "2.379183292388916\n",
            "2.4332282543182373\n",
            "2.38395619392395\n",
            "2.524402379989624\n",
            "2.5375332832336426\n",
            "2.393775701522827\n",
            "2.5956432819366455\n",
            "2.5036702156066895\n",
            "2.4981539249420166\n",
            "2.458289384841919\n",
            "2.4161343574523926\n",
            "2.391443967819214\n",
            "2.3447561264038086\n",
            "2.4990627765655518\n",
            "2.491297483444214\n",
            "2.2902791500091553\n",
            "2.461899757385254\n",
            "2.577768087387085\n",
            "2.4581093788146973\n",
            "2.4479305744171143\n",
            "2.6228039264678955\n",
            "2.4981849193573\n",
            "2.513350009918213\n",
            "2.454416513442993\n",
            "2.384894371032715\n",
            "2.6267781257629395\n",
            "2.5119993686676025\n",
            "2.384178400039673\n",
            "2.4446794986724854\n",
            "2.466035842895508\n",
            "2.4225049018859863\n",
            "2.4690327644348145\n",
            "2.4707844257354736\n",
            "2.42781138420105\n",
            "2.472853422164917\n",
            "2.430793285369873\n",
            "2.5253822803497314\n",
            "2.4346227645874023\n",
            "2.4982197284698486\n",
            "2.498098611831665\n",
            "2.500913381576538\n",
            "2.4203808307647705\n",
            "2.412161350250244\n",
            "2.489908456802368\n",
            "2.5489044189453125\n",
            "2.5666332244873047\n",
            "2.4148709774017334\n",
            "2.404341459274292\n",
            "2.416656017303467\n",
            "2.4419949054718018\n",
            "2.418473243713379\n",
            "2.4877078533172607\n",
            "2.511003255844116\n",
            "2.493220567703247\n",
            "2.4401843547821045\n",
            "2.4946141242980957\n",
            "2.4537863731384277\n",
            "2.3415112495422363\n",
            "2.362351417541504\n",
            "2.594344139099121\n",
            "2.5686163902282715\n",
            "2.5158817768096924\n",
            "2.5172319412231445\n",
            "2.4380645751953125\n",
            "2.4372544288635254\n",
            "2.534539222717285\n",
            "2.4619710445404053\n",
            "2.5491044521331787\n",
            "2.3659493923187256\n",
            "2.4603991508483887\n",
            "2.4612574577331543\n",
            "2.4799563884735107\n",
            "2.492960214614868\n",
            "2.544904947280884\n",
            "2.4443471431732178\n",
            "2.3992934226989746\n",
            "2.5057101249694824\n",
            "2.352745532989502\n",
            "2.4498417377471924\n",
            "2.5630552768707275\n",
            "2.43420672416687\n",
            "2.424048662185669\n",
            "2.3556954860687256\n",
            "2.334895133972168\n",
            "2.4383528232574463\n",
            "2.31880259513855\n",
            "2.3614470958709717\n",
            "2.406160831451416\n",
            "2.5417346954345703\n",
            "2.5015225410461426\n",
            "2.5089409351348877\n",
            "2.5038981437683105\n",
            "2.3885598182678223\n",
            "2.6461026668548584\n",
            "2.3975014686584473\n",
            "2.4482080936431885\n",
            "2.432668924331665\n",
            "2.5869345664978027\n",
            "2.390869140625\n",
            "2.5034401416778564\n",
            "2.578152894973755\n",
            "2.3254988193511963\n",
            "2.420557975769043\n",
            "2.3973681926727295\n",
            "2.421046018600464\n",
            "2.5800509452819824\n",
            "2.287327766418457\n",
            "2.384465217590332\n",
            "2.444246292114258\n",
            "2.5394768714904785\n",
            "2.365980625152588\n",
            "2.4912946224212646\n",
            "2.4443325996398926\n",
            "2.3976941108703613\n",
            "2.434494972229004\n",
            "2.455791711807251\n",
            "2.533123016357422\n",
            "2.552633762359619\n",
            "2.6460304260253906\n",
            "2.50886869430542\n",
            "2.449815511703491\n",
            "2.5583295822143555\n",
            "2.579103946685791\n",
            "2.477921962738037\n",
            "2.5243990421295166\n",
            "2.6532132625579834\n",
            "2.477847099304199\n",
            "2.3960719108581543\n",
            "2.514600992202759\n",
            "2.3015530109405518\n",
            "2.5342628955841064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=400)[0].tolist())) ###########"
      ],
      "metadata": {
        "id": "-gpd90eQwec8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be856b9d-f533-41db-ad82-e21cb44fd1e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "No t:\n",
            "in ticothingh, much s ESThondiryrs y ve:\n",
            "\n",
            "The thafenda merar\n",
            "WAng, aothace Pay, ty---\n",
            "Tu whea Why he Gome:\n",
            "HIUSir.\n",
            "SONI INGLor tco anoris thates ulifive hes thes, thacothanent! t male I w at pllar owifer.\n",
            "F anthif.\n",
            "BRII as s horugyouc-rar g wa\n",
            "CagQUCEXle' ure ithe be bone chousse thely w, we tais RDWh e so\n",
            "CARI SIRKINate: odelthr en tsthoond.\n",
            "PUESe tamas he toly myo pe kereyenoidoo, jurderit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ※ 2. Self-attention을 보기 전에 preview를 보자"
      ],
      "metadata": {
        "id": "WMUTNRe4JRCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기 self attention에서 중요한 것은 t 스텝에서 그 이후 time step의 정보는 참고하면 안된다는 것이다. t 스텝의 토큰 예측에는 1,2,3,4, ... , t-1 스텝의 토큰만을 참고하도록 해야한다."
      ],
      "metadata": {
        "id": "njQ3hBiZTskd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "먼저 self attention의 preview로 이전 토큰들의 정보를 평균을 취하는 식으로 가져오는 것을 예제로써 해볼 것이다.\n",
        "- 물론 평균을 취하면 정보손실이 심하여 실제로는 이렇게 하지 않고, 예시정도로 보면 된다."
      ],
      "metadata": {
        "id": "jXI2TM76UM02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "id": "d-wTOZi9webd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64dad717-60a0-4cd1-a1b3-8767a944ce6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros( (B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b, :t+1]\n",
        "    xbow[b, t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "7FTyd3-3weUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 box는 bag of word의 약자로, 단어의 순서를 고려하지 않고 단어의 출현 빈도만을 고려하여 텍스트 데이터를 표현하는 방법을 의미한다.\n",
        "- bag of word 설명 출처: https://wikidocs.net/22650"
      ],
      "metadata": {
        "id": "n7qFyD8qVWnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x[0] # 첫번째 배치에서 각 토큰마다의 channel(임베딩) 값"
      ],
      "metadata": {
        "id": "ZobFBwQ8weTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b9a63b-741c-4721-dfae-64339de95663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.3596, -0.9152],\n",
              "        [ 0.6258,  0.0255],\n",
              "        [ 0.9545,  0.0643],\n",
              "        [ 0.3612,  1.1679],\n",
              "        [-1.3499, -0.5102],\n",
              "        [ 0.2360, -0.2398],\n",
              "        [-0.9211,  1.5433]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0] # 첫번째 배치에 대한 각 time step마다의 average 값"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83Cek53jVSYM",
        "outputId": "0b8984da-9aaa-4317-f2ea-2a9aa3e1b716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.0894, -0.4926],\n",
              "        [ 0.1490, -0.3199],\n",
              "        [ 0.3504, -0.2238],\n",
              "        [ 0.3525,  0.0545],\n",
              "        [ 0.0688, -0.0396],\n",
              "        [ 0.0927, -0.0682],\n",
              "        [-0.0341,  0.1332]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**이번에는 위의 loop 두번 돌리는 것보다 더 효율적으로 만들어보자.**\n",
        "\n",
        "먼저, lower triangular matrix를 만들어보자. 이는 pytorch에서 tril이라는 함수로 구현가능하다.\n",
        "\n"
      ],
      "metadata": {
        "id": "qOu-pFtmWltu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tril(torch.ones(3,3))"
      ],
      "metadata": {
        "id": "2XFecgs8weRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0332d769-f03e-43b3-8b40-19682bbfd989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [1., 1., 0.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 위의 이전 토큰만 고려한 평균 계산을 matrix 기반의 연산으로 수행해보자.\n",
        "- a: 포지션 정보 (이전 토큰만 참고하도록 하기위한 lower triangular matrix)\n",
        "- b: 토큰 정보\n",
        "- c: bow 결과"
      ],
      "metadata": {
        "id": "tby5cKWLXOSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "b = torch.randint(0,10, (3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('b=')\n",
        "print(b)\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "id": "Sx4oditaweP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e304e6f-886b-482e-e1ac-6ea7b3f66bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c=\n",
            "tensor([[ 2.,  7.],\n",
            "        [ 8., 11.],\n",
            "        [14., 16.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "자, 일단 이전 토큰들의 평균은 아니지만, 이전 토큰만 고려해서 summation을 했다는 것을 통해 위의 for loop를 통한 이전 토큰만을 고려하는 메커니즘과 비슷하다는 것은 확인할 수 있다.\n",
        "이제 summation이 아니라 평균을 계산하기 위해 코드를 수정해보자."
      ],
      "metadata": {
        "id": "4FO6lSApXw21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a/torch.sum(a, dim=1, keepdim=True) # 평균 고려를 위해 추가된 코드,  keepdim 인자는 input과 동일한 size의 shape를 유지하도록 도와준다.\n",
        "b = torch.randint(0,10, (3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('b=')\n",
        "print(b)\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "id": "KGDHevdcweOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229bc82d-5703-480c-dad0-612ed11a1684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 동일한 결과를 내지만, 더 계산 효율적인 matrix연산을 통해서 xbow를 다시 계산해보자."
      ],
      "metadata": {
        "id": "AkHOzCzfYxvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# xbow ver 2: use tril function\n",
        "wei = torch.tril(torch.ones(T,T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (T,T) x (B,T,C)인데 여기서 B부분이 wei의 앞에 추가되서 (B,T,T) x (B, T, C) 형태의 multiplication이 된다. 이 곱의 결과는 (B,T,C) 가 된다.\n",
        "print(torch.allclose(xbow2 ,xbow)) # torch.allclose 는 두 텐서가 동일한지 검사하는 함수.\n",
        "print(xbow[0], xbow2[0]) # 이 둘의 결과가 일치함을 확인할 수 있다.\n"
      ],
      "metadata": {
        "id": "2at8hk8tweMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133c75f5-9926-4520-f560-65eab2951896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]]) tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- torch.allclose는 두 텐서가 동일한지 비교하는 데 사용할 수 있는 PyTorch 유틸리티 함수입니다. (https://runebook.dev/ko/docs/pytorch/generated/torch.allclose)"
      ],
      "metadata": {
        "id": "dSBbIdcyZuVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "마지막은 mask_fill 기반의 연산이다.\n",
        "- ※ 이 연산은 transformer 구현에서도 쓰이는 기법으로 알아두면 좋다."
      ],
      "metadata": {
        "id": "efF7Cwemdtx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# xbow ver 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "print(\"tril: \", tril)\n",
        "wei = torch.zeros((T,T))\n",
        "print(\"wei, zeros: \", wei)\n",
        "wei = wei.masked_fill(tril == 0, float('-Inf'))\n",
        "print(\"wei, masked_fill: \", wei)\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(\"wei, masked_fill softmax: \", wei)\n",
        "\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ],
      "metadata": {
        "id": "e6ZQZTclweLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb1530c2-c978-411f-b45e-3b77e5377d27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tril:  tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "wei, zeros:  tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "wei, masked_fill:  tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "wei, masked_fill softmax:  tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "version 3가 version 2보다 번거로워 보이는데, 왜 이 방법을 사용할까?\n",
        "일단, 직관적으로 보면\n",
        "1. 처음 wei, 즉 모든 값이 0인 wei는 서로간의 intercation strength로써 생각할 수 있는데, 처음에는 이전 token에 대해서 intercation은 0이라고 볼 수 있다.\n",
        "2. wei.masked_fill(tril == 0, float('-Inf'))는 과거의 토큰들과는 communicate 할수 없도록 설정한 것이다. (https://youtu.be/kCc8FmEb1nY?t=3442)\n"
      ],
      "metadata": {
        "id": "JYHeAF3Weay5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 위에서 구현했던 BigramLanguageModel에 Self attention을 도입하여 Head라는 클래스와 함께 다시 구현해보자."
      ],
      "metadata": {
        "id": "t3yDtNTdpOFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "\n",
        "torch.manual_seed(1337) # pytorch의 random seed 고정. 재구현성을 위해 설정\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed) # 이것 자체가 바로 logits가 안되고 이는 token embedding이 된다.   shape는 (B, T, C)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed) # 이는 position 값을 임베딩 하는 것인데, 여기서 중요한건 position의 차원 값이 token embedding 의 shape 크기와 같아야한다는 것이다.  shape는 (T, C) -> 이는 toekn embedding과 덧셈 연산을 할 때, (B, T, C) broadcast가 된다.\n",
        "    self.sa_head = Head(n_embed)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size) # 이를 통해서 logits가 계산될 것이다.\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    # 여기서 idx와 targets의 shape는 (Batch size, Time(문장 하나의 단어 개수) ) 이다.\n",
        "    tok_emb = self.token_embedding_table(idx) # 이것의 반환 결과 shape는 (Batch, Time, Channel(단어,토큰 하나의 임베딩 결과 벡터) ) 이다.\n",
        "    pos_emb = self.position_embedding_table(torch.arrange(T, device = device)) # 0부터 T-1 값에 대해서 (입력 sequnece) 이 위치(Time step)값들에 대한 position embedding값을 반환한다.\n",
        "    x = tok_emb + pos_emb # 이것이 최종 입력값이 된다. token에 position을 고려한 input\n",
        "    x = self.sa_head(x)\n",
        "\n",
        "    # toekn + pos_emb를 했을 떄, 아직까진 useful하지 않은데, 왜냐하면 bigram 모델이기 때문이다.\n",
        "\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C) # make sense한 shape\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      idx_cond = idx[:, -block_size:] # 마지막 block size 토큰의 idx를 crop (https://youtu.be/kCc8FmEb1nY?t=4851)\n",
        "\n",
        "      logits, loss = self(idx_cond) # idx의 shape는 (B,T)이다.\n",
        "\n",
        "      logits = logits[:, -1, :]\n",
        "\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # softmax로 나온 다음 토큰에 대한 확률을 기반으로 다음 토크을 뽑는다.\n",
        "\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # 기존 idx를 새로 뽑은 토큰과 concat해서 shape가 (B,T+1)인 idx를 새로 만든다.\n",
        "\n",
        "    return idx # 매 스텝마다 sampling해서 token들이 concat된 최종 예측 sequence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss  = m(xb, yb)\n",
        "print(logits.shape, loss)\n",
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "id": "uikrTSF2XDHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ※ 3. 이제부터 self attention을 보자.\n",
        "https://youtu.be/kCc8FmEb1nY?t=3718\n",
        "\n",
        "- self attention에서 self의 의미는 key와 query, value가 모두 같은 source로부터 나왔다는 것을 의미한다. (여기서는 x라는 동일한 source에서 나온다.)"
      ],
      "metadata": {
        "id": "d-Qe4pvsStGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "먼저, single head인 작은 self attention을 구현해보자."
      ],
      "metadata": {
        "id": "VyWyIeiRStBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self attention\n",
        "# 1. 여기서 달라진 점:\n",
        "# 이전까지는 그저 이전 토큰들의 평균을 취하였다. 여기서는 attention을 적용해서 본다.\n",
        "# 지금은 data dependent하게 보려고 한다. 예: 현재 토큰에 모음이 있다는 것은 그 앞에 토큰을 집중할 때 자음을 더 집중해서 봤다는 의미로 볼 수 있다.\n",
        "\n",
        "# 먼저 query와 key에 대해서 구현할 것이다.\n",
        "# query: 우리가 집중해서 보려고 하는 것\n",
        "# key: 각 토큰이 가지고 있는 content\n",
        "\n",
        "# 2. 이들을 dot product를 할 것이다. 만약 값이 높게 나온다면 해당 token의 query는 해당 토큰의 key에 집중해서 다음 것을 예측한다고 볼 수 있다.\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias = False)\n",
        "query = nn.Linear(C, head_size, bias = False)\n",
        "\n",
        "k = key(x)   # shape: (B, T, 16)\n",
        "q = query(x) # shape: (B, T, 16)\n",
        "# 3. 여기까지, 현재 모든 batch의 각각 token에 대해서 key와 query를 생성했다. 아직 token간의 interaction (self attention)은 계산되지 않았다.\n",
        "\n",
        "wei = q @ k.transpose(-2, -1) * C**-0.5 # 4. k.transpose: 그냥 .T를 하면 batch dimension고려가 힘들다. 저렇게 지정하면 (B, T, 16) @ (B, 16, T) 형식의 곱이 가능하다.   마지막의 C**-0.5는 head_size의 square root를 곱해서 scaled attention을 만드는 것이다.\n",
        "# wei shape: (B, T, T)\n",
        "\n",
        "# 5. 이제 value라는 것을 만들자. 기존에는 wei @ x로 토큰 자체를 attention에 곱하는 형태였다.\n",
        "\n",
        "value = nn.Linear(C, head_size, bias = False)\n",
        "\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-Inf')) # 만약 encoder block이였다면 모든 토큰들이 서로 communicate를 해야되서 이 코드가 없었을 것이다. 하지만 decoder는 autoregressive하게 작동해야해서 이 부분을 넣었다.\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v # 이 v가 aggrigate하는 것이다.\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "id": "OcqbpyQfweIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab039427-d399-45ae-fe57-505e37fe3c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 data dependent하게 출력한 이전 토큰들을 고려하는 aggregation weight를 보자."
      ],
      "metadata": {
        "id": "imIGlAKoYY8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eM34b_YYS-o",
        "outputId": "ca876760-e06d-4b47-d3fc-e43e59d116f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4264, 0.5736, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.3151, 0.3022, 0.3827, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.3007, 0.2272, 0.2467, 0.2253, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1635, 0.2048, 0.1776, 0.1616, 0.2926, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1403, 0.2272, 0.1454, 0.1244, 0.2678, 0.0949, 0.0000, 0.0000],\n",
              "         [0.1554, 0.1815, 0.1224, 0.1213, 0.1428, 0.1603, 0.1164, 0.0000],\n",
              "         [0.0952, 0.1217, 0.1130, 0.1453, 0.1137, 0.1180, 0.1467, 0.1464]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4300, 0.5700, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.3379, 0.2559, 0.4061, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2867, 0.2188, 0.2786, 0.2159, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1297, 0.1813, 0.1683, 0.2990, 0.2217, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1584, 0.1671, 0.1458, 0.2361, 0.1862, 0.1063, 0.0000, 0.0000],\n",
              "         [0.1198, 0.1633, 0.0921, 0.1595, 0.1604, 0.1588, 0.1461, 0.0000],\n",
              "         [0.1607, 0.1120, 0.1220, 0.1029, 0.1560, 0.1224, 0.1112, 0.1128]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4968, 0.5032, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2995, 0.3563, 0.3442, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1551, 0.3508, 0.2158, 0.2782, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.3033, 0.1784, 0.1860, 0.1893, 0.1431, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1587, 0.1747, 0.1542, 0.1577, 0.1708, 0.1839, 0.0000, 0.0000],\n",
              "         [0.0890, 0.1315, 0.1213, 0.1760, 0.1518, 0.1775, 0.1529, 0.0000],\n",
              "         [0.1321, 0.1338, 0.1049, 0.1356, 0.1284, 0.1293, 0.0907, 0.1451]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5248, 0.4752, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.3733, 0.4493, 0.1773, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2909, 0.2753, 0.2298, 0.2040, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2061, 0.2425, 0.1535, 0.2059, 0.1920, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1746, 0.2576, 0.0878, 0.1710, 0.1700, 0.1390, 0.0000, 0.0000],\n",
              "         [0.1281, 0.1279, 0.1427, 0.1209, 0.1443, 0.1957, 0.1403, 0.0000],\n",
              "         [0.1014, 0.1493, 0.1030, 0.1329, 0.1329, 0.1136, 0.1209, 0.1460]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전처럼 평균을 취한 것과 다르게, 모든 배치사이에 weight가 같지 않다.\n",
        "- x에서의 각각의 batch에 대한 값들이 다르기 때문\n"
      ],
      "metadata": {
        "id": "kesn8yQAYgWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "scaled atttention을 보자."
      ],
      "metadata": {
        "id": "6CoDtkIEf9ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B, T, head_size)\n",
        "q = torch.randn(B, T, head_size)\n",
        "wei = q @ k.transpose(-2, -1)"
      ],
      "metadata": {
        "id": "mDcVPZoTYfi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var(), q.var(), wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sFpw5CugHWq",
        "outputId": "e78d6a2e-3a07-4679-b717-d8b85df6f58a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.9487), tensor(1.0449), tensor(14.3682))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "variance가 크게 나왔다.\n",
        "\n",
        "이번엔 head의 square root를 곱해보자."
      ],
      "metadata": {
        "id": "vHcHAGPmgPc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B, T, head_size)\n",
        "q = torch.randn(B, T, head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "1Le2gcZBweG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var(), q.var(), wei.var()"
      ],
      "metadata": {
        "id": "vWV2qnB9weFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8820ac73-64f8-4209-85d5-3a3a0fcd805b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.9416), tensor(1.0104), tensor(1.0879))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "variance가 매우 줄었다."
      ],
      "metadata": {
        "id": "7dJG-Uqlgn6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "attention의 scaling이 중요한 이유는, 우리가 attention의 마지막에 softmax를 적용하기 때문이다. variance가 크다는 말은 벡터 안의 값들 차이가 매우 크다는 의미이다. 너무 negative로 쳐져있거나 postive값이 큰 원소값 하나가 있을것인데, 그렇게 하면 softmax후에 one hot vector 형태의 결과가 나올 것이다."
      ],
      "metadata": {
        "id": "KvfSvF7ogsoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/kCc8FmEb1nY?t=4778 파이토치에서 buffer의 의미"
      ],
      "metadata": {
        "id": "Kac6_55Fhcue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer norm은 batch norm과 비슷하다.\n",
        "차이점: batch norm은 배치단위로, dim = 0 에 대해서 normalize하면, Layer norm은 feature 단위로 dim=1에 대해서 normalize 한다.\n",
        "- https://youtu.be/kCc8FmEb1nY?t=5594\n",
        "- Batch size에 대해 robust 하다."
      ],
      "metadata": {
        "id": "hhAfkrxRopFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. self attention을 고려하여, Head 클래스를 추가한 새로운 버전의 Model을 다시 보자."
      ],
      "metadata": {
        "id": "qJQiIGBL9eGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337) # pytorch의 random seed 고정. 재구현성을 위해 설정\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# estimate_loss는 깊게 안보고 가져온 코드이다.\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):  # 4. 에서 추가된 부분\n",
        "  ### 이 클래스는 하나의 셀프 어텐션 head 단위이다.\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    # register buffer는 gradients를 요구하지 않아서 parameters로써 저장되지 않고 state_dict로 저장된다. 이는 batch norm 레이어의 mean이나 std의 tracking에 용이하다 한다.\n",
        "    # 파이토치에서 state dict는 각 parameter tensor의 레이어에 매핑하는 dictionary object이다.\n",
        "    # Buffer는 Module에 저장해 놓고 사용하는 Tensor의 일종으로 학습을 통해 계산되지 않는 Tensor입니다. (https://teamdable.github.io/techblog/PyTorch-Module)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.key(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-Inf'))\n",
        "    wei = F.softmax(wei, dim = -1)\n",
        "\n",
        "    v = self.value(x)\n",
        "\n",
        "    out = wei @ v # wei는 하나의 row가 하나의 query에 대한 각각의 key에 대한 attention,   v는 하나의 row가 하나의 단어에 대한 value 변환 값. 이 연산을 하면 각 row에 각 attention이 곱해져서 row에 대해서 더해지는 것\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.sa_head = Head(n_embed)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    # 여기서 idx와 targets의 shape는 (Batch size, Time(문장 하나의 단어 개수) ) 이다.\n",
        "    tok_emb = self.token_embedding_table(idx) # 이것의 반환 결과 shape는 (Batch, Time, Channel(단어,토큰 하나의 임베딩 결과 벡터) ) 이다.\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # 0부터 T-1 값에 대해서 (입력 sequnece) 이 위치(Time step)값들에 대한 position embedding값을 반환한다.\n",
        "    x = tok_emb + pos_emb # 이것이 최종 입력값이 된다. token에 position을 고려한 input\n",
        "    x = self.sa_head(x)\n",
        "\n",
        "\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "\n",
        "      logits, loss = self(idx_cond) # idx의 shape는 (B,T)이다.  여기서 self()는 forward를 부르는 함수라 한다. https://youtu.be/kCc8FmEb1nY?t=1886\n",
        "\n",
        "      logits = logits[:, -1, :] # 예측한 것의 마지막 토큰(즉, 다음 예측, autoregressive model로 치면 바로 다음 예측한 토큰)에 대한 확률값만 가져온다.\n",
        "\n",
        "      probs = F.softmax(logits, dim=-1) # softmax적용\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # softmax로 나온 다음 토큰에 대한 확률을 기반으로 다음 토크을 뽑는다.\n",
        "\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # 기존 idx를 새로 뽑은 토큰과 concat해서 shape가 (B,T+1)인 idx를 새로 만든다.\n",
        "\n",
        "    return idx # 매 스텝마다 sampling해서 token들이 concat된 최종 예측 sequence\n",
        "\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "logits, loss  = m(xb, yb)\n",
        "print(logits.shape, loss)\n",
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "id": "3WH5xglMweDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42764ac3-891f-4201-c904-fd485dd97096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 65]) tensor(4.2735, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "uoabMfqMY.ukrspu,Xvy!CJFfmMSgS.JCnOEmygGA&-SPlLyuGYfGXMaInv'Z,wX$,OMdsR!vHQImqSSPe;N\n",
            "'?.z;?UAfCAufX:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "optimizer = torch.optim.AdamW(m.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "QcWWngGpweCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(max_iters):\n",
        "\n",
        "  if steps % eval_interval == 0 or steps == max_iters - 1:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  # set_to_none 관련해서는 https://velog.io/@kjb0531/zerograd%EC%9D%98-%EC%9D%B4%ED%95%B4 참고\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "j8oUiXPwweAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a956e78f-c01c-4462-a617-09c7382421e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2507, val loss 4.2519\n",
            "step 500: train loss 2.6251, val loss 2.6253\n",
            "step 1000: train loss 2.5230, val loss 2.5281\n",
            "step 1500: train loss 2.4835, val loss 2.4622\n",
            "step 2000: train loss 2.4540, val loss 2.4539\n",
            "step 2500: train loss 2.4422, val loss 2.4394\n",
            "step 3000: train loss 2.4219, val loss 2.4474\n",
            "step 3500: train loss 2.4167, val loss 2.4377\n",
            "step 4000: train loss 2.4135, val loss 2.4303\n",
            "step 4500: train loss 2.3945, val loss 2.4267\n",
            "step 4999: train loss 2.4155, val loss 2.4202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=400)[0].tolist()))"
      ],
      "metadata": {
        "id": "yX_tTXKywd-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aac7d5d-fe0d-4163-b0b7-e326eef641fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Af blost onetecal toumur stheme rderxlesh sble:--\n",
            "ABe cthal sk molitha ve qutnoppland Pso Maindo of scepary he hath merer corginishueak.\n",
            "Whierd stiowome alllet,\n",
            "L;\n",
            "ble gs t y teil monend dfild; bandil avendinese deane\n",
            "Edild, ssat ndervey hilleme ge Yond thus.\n",
            "\n",
            "Ye heneres at,\n",
            "S:\n",
            "My, so imough thor ss yocut te?\n",
            "\n",
            "LIRMORDUS:\n",
            "PI chame Rerenetrgo yet;\n",
            "My byok oro aut hye tthouther sstom, I cker, halour \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 이제 Head를 여러 개를 추가하는 Multi head attention을 구현해서 다시 모델을 만들어보자."
      ],
      "metadata": {
        "id": "NG360LCpwJN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이는 그저 attention head를 parallel하게 여러 개를 추가한 것이다.\n",
        "Multi head를 추가했을 때의 이점은 다양한 관점에서의 attention 해석을 할 수있게 될 것이다."
      ],
      "metadata": {
        "id": "jj3KKahkweMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337) # pytorch의 random seed 고정. 재구현성을 위해 설정\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module): # 5. 에서 추가된 부분\n",
        "  def __init__(self, num_heads, head_size): # head_size는 각각 head에서의 size를 의미하는 것이다. 맨 처음에 총 크기를 의미하는게 아니라 총 크기에서 head 개수만큼으로 나눠준 것을 의미\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # nn.Modulelist는 Python의 list 타입 데이터 안에 원소 형태로 저장된 각각의 layer를 저장하는 것을 의미한다.\n",
        "    # nn.ModuleList는 nn.Sequential처럼 안에 데이터 넣는다고 바로 forwrad가 작동해서 순차적으로 하는 것이 아니다. 이것은 modulelist안에 있는 각각의 layer를 불러와서 안에 데이터를 넣는식으로 진행해야 한다.\n",
        "    # Multi head attention은 병렬적으로 계산해야 되는거다. Head 1과 Head 2는 서로 간섭하는 연산을 안한다.\n",
        "    # 따라서, 아래 forward처럼 각각의 head마다 입력인 x를 넣어서 각각의 head 결과들을 concat 하는 것이다.\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1) # dim = -1은 channel에 대해서 concat을 하겠다는 의미\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "  ### 이 클래스는 하나의 셀프 어텐션 head 단위이다.\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.key(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-Inf'))\n",
        "    wei = F.softmax(wei, dim = -1)\n",
        "\n",
        "    v = self.value(x)\n",
        "\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.sa_head = MultiHeadAttention(4, n_embed//4)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n",
        "    x = tok_emb + pos_emb t\n",
        "    x = self.sa_head(x)\n",
        "\n",
        "\n",
        "\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "\n",
        "\n",
        "\n",
        "      logits, loss = self(idx_cond)\n",
        "\n",
        "      logits = logits[:, -1, :]\n",
        "\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "optimizer = torch.optim.AdamW(m.parameters(), learning_rate)\n",
        "\n",
        "batch_size = 32\n",
        "for steps in range(max_iters):\n",
        "\n",
        "  if steps % eval_interval == 0 or steps == max_iters - 1:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "mPgBIIhvwd89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f0d439-3825-432a-f1f3-c44297ce0a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2324, val loss 4.2307\n",
            "step 500: train loss 2.6198, val loss 2.6378\n",
            "step 1000: train loss 2.4783, val loss 2.4858\n",
            "step 1500: train loss 2.4226, val loss 2.4245\n",
            "step 2000: train loss 2.3780, val loss 2.3861\n",
            "step 2500: train loss 2.3597, val loss 2.3673\n",
            "step 3000: train loss 2.3354, val loss 2.3498\n",
            "step 3500: train loss 2.3119, val loss 2.3289\n",
            "step 4000: train loss 2.3097, val loss 2.2979\n",
            "step 4500: train loss 2.3003, val loss 2.3063\n",
            "step 4999: train loss 2.2951, val loss 2.3055\n",
            "\n",
            "Shiser:\n",
            "We mallokier sorestrarl thorn hit tavoefars'll lost ins, ingd riee thedroee o, lol be hom da\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4번의 loss 값인 2.4202에 비해서, multi head attention을 추가하니 2.3 정도로 줄었다."
      ],
      "metadata": {
        "id": "OcBe0oPX5Yme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "ePLbgCNSwd3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa228b7c-55f3-47e8-ca11-aa2730364058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CHENE:\n",
            "Aou tle.\n",
            "\n",
            "Yan searly dome thall;\n",
            "On;\n",
            "Srienanged dollf!\n",
            "Anut I a sowol: wountret bllanthre hand four hely tho, you,\n",
            "Ahald tho sof quneeay\n",
            "Serve quwee: he grue hond:\n",
            "\n",
            "Tharesed ume doud thod Is\n",
            "BCUCKARe ghour by beate:\n",
            "Gh; hirts,\n",
            "If, thee---\n",
            "HOP-loth Rallyoo,\n",
            "Is ther diss ipne; sho nouprepear dullesn.\n",
            "\n",
            "Gee se?\n",
            "Nw;\n",
            "But bar:\n",
            "Waupt thigeretece, do;\n",
            "And hald, oungreel oildspt nre lorshndace,\n",
            "Gugec-:ater hadbarensit by thad lave goio pame mil?\n",
            "Whensee he pos I wary.\n",
            "\n",
            "Clit weea-sto bem'ss ait you to as thay thise my that onometteeed? ou iwiro cend out, hithinst dous be me; and\n",
            "A'lscr'd fre;\n",
            "Whoulsech lace, youdn,\n",
            "Whee the mar lock at, then, hit;\n",
            "Why?\n",
            "\n",
            "ARA! le inly,\n",
            "I'rier thelaver owee too bedotlet gouv\n",
            "Mand fts alve lost yeresto me ithep iciecameak ing as forst he meet! owne? wit.\n",
            "\n",
            "TRotegine for, sall woene erut aster, shatard, to, Gol, the my;\n",
            "Macy, fring whim fby Hete dlicpeeadicon ant ft to bat aer alene?\n",
            "\n",
            "\n",
            "Tarm: tis art hy Lornd Colrouce notle lonocar't tocus\n",
            "parceang dis,\n",
            "B:\n",
            "If me.\n",
            "\n",
            "A\n",
            "\n",
            "hunt gr dere mir?\n",
            "\n",
            "\n",
            "OMEst Youe sell dot doorme; ther gell robbopincak and wie the voteaiss aund is Herthe dome.\n",
            "\n",
            "QUWime faurethe whe st:\n",
            "Ist,\n",
            "RI's ist barce I thoutre the?\n",
            "\n",
            "dimy cods sil;\n",
            "G, ant ald; kauw shit billot To Co Mout sod?\n",
            "\n",
            "DUSICERD: uing butelis likn otrifoed an E samy, vef yrimed ithre,'\n",
            "Thesin gud:\n",
            "Gishe,\n",
            "On ad soud it cold Efanthe nos su, adit, fort eat.\n",
            "\n",
            "CLAY:\n",
            "Veive thare alcruck barrive; sharbe comy antee, we athenthes we Ife whill hot verk ther, tisuw-Ggloge serlold,\n",
            "I Oniest; teiteditredath cady forowecith!\n",
            "I the grers sitaelll of?\n",
            "\n",
            "\n",
            "HYorut se,\n",
            "For at oun wethato treense tat quplestey, lod ofir quiew, ha wo itarlow thy suntlebe adis-yens pay yre ould; aberentriearst wre thr:\n",
            "Wis\n",
            "And adiucheciut tratat knt athseeming\n",
            "Lo erous tam day ou, she, uns lon bel oflooth sataurdeves non.\n",
            "\n",
            "ORUCE:\n",
            "Shem ding sperely.\n",
            "\n",
            "AROLILO:\n",
            "S--fevin thethe sut tree yurerll'ld thoeve Ra my yourt whereare nom her E'd it wit to we, hy\n",
            "Srot whend lovece waking her the hoy!\n",
            "\n",
            "PENDY imyou no\n",
            "Nowp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Position-wise Feed forward neural network를 추가한다."
      ],
      "metadata": {
        "id": "wQSS0fVQ5zrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리가 multi head self attention을 하고 나서, logits(토큰 에측)을 계산하는 것은 너무 빠르다. 각 어텐션 head에서 찾은 정보를 좀 더 가공하는 계산도 필요하다.\n",
        "추가적으로 non linearity도 추가할 수 있다.\n",
        "- ChatGPT에서 받은 답변\n",
        "\n",
        "\n",
        "그게 FFNN의 역할이다.\n",
        "\n",
        "FFNN은 토큰 단위로 넣어서 이루어진다. 즉, 하나의 문장에서 단어 하나하나가 FFNN에 넣어서 계산된다는 의미이다.(https://youtu.be/kCc8FmEb1nY?t=5162)"
      ],
      "metadata": {
        "id": "U4OtECuS8Lul"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZvTUbb2i8KRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337) # pytorch의 random seed 고정. 재구현성을 위해 설정\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.key(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-Inf'))\n",
        "    wei = F.softmax(wei, dim = -1)\n",
        "\n",
        "    v = self.value(x)\n",
        "\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module): # 6. 에서 추가된 부분\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, n_embed),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed) #\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed) #\n",
        "    self.sa_head = MultiHeadAttention(4, n_embed//4)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) #\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device = device)) #\n",
        "    x = tok_emb + pos_emb #\n",
        "    x = self.sa_head(x)\n",
        "    x = self.ffwd(x)\n",
        "\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C) # make sense한 shape임.\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "\n",
        "      logits, loss = self(idx_cond)\n",
        "\n",
        "      logits = logits[:, -1, :]\n",
        "\n",
        "      probs = F.softmax(logits, dim=-1) #\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #\n",
        "\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "optimizer = torch.optim.AdamW(m.parameters(), learning_rate)\n",
        "\n",
        "batch_size = 32\n",
        "for steps in range(max_iters):\n",
        "\n",
        "  if steps % eval_interval == 0 or steps == max_iters - 1:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKIzOgwu7Szh",
        "outputId": "6a0a4bf9-8d1c-4ab9-9fd9-c0c0ee037d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1982, val loss 4.1985\n",
            "step 500: train loss 2.5810, val loss 2.5915\n",
            "step 1000: train loss 2.4613, val loss 2.4685\n",
            "step 1500: train loss 2.4047, val loss 2.4045\n",
            "step 2000: train loss 2.3512, val loss 2.3667\n",
            "step 2500: train loss 2.3299, val loss 2.3504\n",
            "step 3000: train loss 2.3239, val loss 2.3340\n",
            "step 3500: train loss 2.2909, val loss 2.3129\n",
            "step 4000: train loss 2.2908, val loss 2.2904\n",
            "step 4500: train loss 2.2730, val loss 2.2854\n",
            "step 4999: train loss 2.2628, val loss 2.2914\n",
            "\n",
            "ARECARDHHA:\n",
            "Yod\n",
            "Thrier the aved of ingooid botlet gous\n",
            "Mand fts all pa,\n",
            "Dout would co thep sciecledakniceraing sor he meet! wome? wondeckitteg no fo, pot-lerrene eyom aster, shatard, to, bol, the my;\n",
            "Macy, frand whir funt.\n",
            "\n",
            "Th licpen dicong haugt to bat ait alend?\n",
            "\n",
            "\n",
            "Tare: thirk to!\n",
            "\n",
            "Lornd lolde sean tou lonocar!\n",
            "Faccis: anceang dis,\n",
            "Thanfemeed youh no co dereirn troug sst thee sle\n",
            "Thoust.\n",
            "\n",
            "Ad it my gell reaboping to owisie, falll.\n",
            "\n",
            "Bus aund is Hortheor mas pes queave tuth nohe st: wit,\n",
            "Rorneref barce king'stre the fod my cods sil ho, ant ald; karo shit billot To Clll\n",
            "Iuts dus: bay of tou.\n",
            "\n",
            "Bou I ber'thaks rifhed an noss of vef you\n",
            "learthrearmannome und:\n",
            "Gowhe,\n",
            "Hom, digh mie cord Eeand o nos exeald the-ut heat.\n",
            "\n",
            "Clor nous lavinick: crul band therng sint cound nord, un athe ighs we Ifn whe lors.\n",
            "\n",
            "Bok ther thissw-Galog wicklalllmmanliest; thitedit\n",
            "Idatardved four fost froste cou as sitalll of theit thut sent:\n",
            "That oun welardowtles sof a tha lostetang coo.\n",
            "\n",
            "Weaike, ha wo ithel't thy suntlebe ad sise, spat yre ourd; ager hord, hird! I throw: shom tadiuch couthe atat knt aveserming\n",
            "Lore hy fear dis of, she, unt lon bellote Pod sataurdes and come leeve.\n",
            "\n",
            "No dint sperpay minds corntt--\n",
            "\n",
            "Fonst to eesut thee ou nill's rin\n",
            "evest tur yourt wherpare nom hro her thakit to wer hyour, Ronend lo dor wakith fer the hoy! Pobt it myoth, \n",
            "lende's ande ithe may tamy's pand:\n",
            "'pcavece pop ith tant, whall,\n",
            "Bicer wout.\n",
            "\n",
            "Nar,\n",
            "Whimm, wow one slet do in to wall tughe.\n",
            "Bker;\n",
            "Take an hible me:\n",
            "Eicks evicch cow of swith thand?\n",
            "\n",
            "Whe sod Edrargactell,\n",
            "aitht youf-'sthn'd.\n",
            "MHe ard inteme ulenm ary gow\n",
            "Becirs.\n",
            "\n",
            "INAMETIND IVIXow I buthte?\n",
            "Sarns we to day bunt krugle!\n",
            "Yot my whopeirn; ute ous tiss tul bererthes tene, ar'me vend? lato mbedds aiveall ulis?\n",
            "\n",
            "Whs the corth mosteir!\n",
            "I:\n",
            "Hoold- tue Pred;\n",
            "And, bouc's be loter\n",
            "Sento tht ty pave.\n",
            "\n",
            "OCHAS:\n",
            "Ohere'd, ther sin ugow heas o athe itht srakf yres fard teallbartett a ou hak orn koth sace,\n",
            "Wht; of dit ant, and I aricos cagh'd, waing pon santtunt so lingth Ca\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 의 loss인 2.3 비해서 loss가 2.29까지 줄었다.\n",
        "- 영상에서도 2.28에서 2.24로 줄어, 감소량이 그렇게 크진 않았다."
      ],
      "metadata": {
        "id": "eLMm2kFp5zpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 여기서는 성능 개선이 아닌, Decoder block을 쌓을 때 더 편하게 쌓고자 여태까지의 연산을 하나의 class로 묶어서 하나의 transformer block으로 만들고자 한다."
      ],
      "metadata": {
        "id": "JDeykmEm5znq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337) # pytorch의 random seed 고정. 재구현성을 위해 설정\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.key(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-Inf'))\n",
        "    wei = F.softmax(wei, dim = -1)\n",
        "\n",
        "    v = self.value(x)\n",
        "\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module)\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, n_embed),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module): # 7. 에서 추가된 부분\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.sa(x)\n",
        "    x = self.ffwd(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential( # 7. 에서 추가된 부분\n",
        "        Block(n_embed, n_head=4),\n",
        "        Block(n_embed, n_head=4),\n",
        "        Block(n_embed, n_head=4)\n",
        "    )\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x) # 7. 에서 추가된 부분\n",
        "\n",
        "\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "\n",
        "      logits, loss = self(idx_cond)\n",
        "\n",
        "      logits = logits[:, -1, :]\n",
        "\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "optimizer = torch.optim.AdamW(m.parameters(), learning_rate)\n",
        "\n",
        "batch_size = 32\n",
        "for steps in range(max_iters):\n",
        "\n",
        "  if steps % eval_interval == 0 or steps == max_iters - 1:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "HgdMsjtT_NoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35de8d8c-8f90-48f0-9787-068b329a4fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2116, val loss 4.2077\n",
            "step 500: train loss 2.9879, val loss 2.9910\n",
            "step 1000: train loss 2.6431, val loss 2.6352\n",
            "step 1500: train loss 2.5289, val loss 2.5233\n",
            "step 2000: train loss 2.4537, val loss 2.4840\n",
            "step 2500: train loss 2.4292, val loss 2.4298\n",
            "step 3000: train loss 2.3800, val loss 2.4108\n",
            "step 3500: train loss 2.3687, val loss 2.3904\n",
            "step 4000: train loss 2.3494, val loss 2.3482\n",
            "step 4500: train loss 2.3178, val loss 2.3407\n",
            "step 4999: train loss 2.3009, val loss 2.3288\n",
            "\n",
            "In- myouand wile me fin the she!\n",
            "\n",
            " a nole urrordsoste hof greide of wis hacke sits at wat'\n",
            "Wec!\n",
            "\n",
            "'w':\n",
            "Haire\n",
            "Whuthlive sowe fator.\n",
            "\n",
            "RCHUWEUCIOLLTS:\n",
            "Een net hat Lfit nok art,\n",
            "Hhasth beirint sas of come walle!\n",
            "Bons wour\n",
            "nesssis lis be you win say'ders to bilimensk avand chal shhat it willign lit atherit:\n",
            "My\n",
            "Sins mouse.\n",
            "\n",
            "WIIOSISIOOORECEE:\n",
            "Bince\n",
            "Innd, perasy' cray cos this, the, me:\n",
            "got come whiurt mee heall, thour unos dongerer; mod hid, groanf to hoe cone!':\n",
            "Ye, sivermou, bon dit bringacecis theime perer las, hes to wrer you Thanll thiord a:\n",
            "Sthat, yon, the mavestheusitswer ou sirs,\n",
            "Mweisth ce Had fanle low we baregh anceperare;\n",
            "I RUOV:\n",
            "Hof of win\n",
            "t seecer ve deal.\n",
            "\n",
            "COEECECEOTABGAOES YOKEM:\n",
            "Yo IUV Hight be ithallif are,\n",
            "Whe Inove brord pe bad Aok and 'and\n",
            "I tos pand as thall\n",
            "Surte, meevothe sute omerit ther heatharinclit way ether nove no pe thanf and aiur?\n",
            "\n",
            "Ghif ofputhe therth kert with she bee vare.\n",
            "\n",
            "\n",
            "Oor stink:\n",
            "Oan thereper ay then dus;\n",
            "Afile-oue hat hart Gis oy bale' wellavelou doatur chat Sir.\n",
            "\n",
            "Thovecudy gone'w to thiper.\n",
            "\n",
            "WOE ROIST,\n",
            "Fhe sints wime.\n",
            "\n",
            "EIS:\n",
            "Che thouris, hat Gaethome my aoour thass in thenl:\n",
            "Se\n",
            "Ture reem miso sthe beow brous hanl wisthallead his 'wert it conere MACRTAing love momen blak a, dedly hit on my arruse that Cafse the your thut de havathy me boomano wive thathn\n",
            "The wish coth ould guie yard love muour noval you galidinland nojor, thoure noe\n",
            "\n",
            "corer youuning aUENGEN:\n",
            "nncoren de arand sibcas ancte he swh math, swh, meare.\n",
            "\n",
            "Oy thamsat, that re, on ue thy, the treny nearercance at ways ther enong Lair pall\n",
            "Wan won!:\n",
            "\n",
            "Cares my thou now eme nor cameice weote\n",
            "I acafemen fy:\n",
            "Sise?\n",
            "\n",
            "\n",
            "FEVLETEOOLET:\n",
            "Sing urmavallawere, and usis must thathasit wole art nwet soth stith theuls if it we me nir my te.\n",
            "\n",
            "GEIUCIOOUS: proum ofnan eednam YI BECAICAAAAE:\n",
            "Rou his win sisfesmio at well heas thiste, thot ay, fe wat cisime rovist; herou gon thous and pof ewhe mutay thy meal beme the pome;\n",
            "Songtharly\n",
            "Tound the the, press ko lume.\n",
            "BE kieds do Wouste gand kathaper;\n",
            "Anay m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Block을 추가함에 따른 깊어진 Neural Net을 더 잘 optimize하기 위해 skip connection 추가"
      ],
      "metadata": {
        "id": "A-kiRjS_5zl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LYCAjjiF5zjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.key(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-Inf'))\n",
        "    wei = F.softmax(wei, dim = -1)\n",
        "\n",
        "    v = self.value(x)\n",
        "\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    self.proj = nn.Linear(n_embed, n_embed) # 8. 에서 추가된 부분\n",
        "    # https://youtu.be/kCc8FmEb1nY?t=5457\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.proj(out) # 8. 에서 추가된 부분\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4*n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embed, n_embed) # 8. 에서 추가된 부분\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(x)  # 8. 에서 추가된 부분\n",
        "    x = x + self.ffwd(x)  # 8. 에서 추가된 부분\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential( # 7. 에서 추가된 부분\n",
        "        Block(n_embed, n_head=4),\n",
        "        Block(n_embed, n_head=4),\n",
        "        Block(n_embed, n_head=4)\n",
        "    )\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "\n",
        "      logits, loss = self(idx_cond)\n",
        "\n",
        "      logits = logits[:, -1, :]\n",
        "\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "optimizer = torch.optim.AdamW(m.parameters(), learning_rate)\n",
        "\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "batch_size = 32\n",
        "for steps in range(max_iters):\n",
        "\n",
        "  if steps % eval_interval == 0 or steps == max_iters - 1:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9JL21tyB-Ie",
        "outputId": "9e799b96-b363-4cb2-a470-135ca3c5bde0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.041921 M parameters\n",
            "step 0: train loss 4.6347, val loss 4.6314\n",
            "step 500: train loss 2.4105, val loss 2.4048\n",
            "step 1000: train loss 2.3186, val loss 2.3166\n",
            "step 1500: train loss 2.2389, val loss 2.2510\n",
            "step 2000: train loss 2.2065, val loss 2.2372\n",
            "step 2500: train loss 2.1707, val loss 2.2072\n",
            "step 3000: train loss 2.1409, val loss 2.1956\n",
            "step 3500: train loss 2.1274, val loss 2.1695\n",
            "step 4000: train loss 2.0951, val loss 2.1599\n",
            "step 4500: train loss 2.0676, val loss 2.1487\n",
            "step 4999: train loss 2.0634, val loss 2.1314\n",
            "\n",
            "Row dut\n",
            "Me theer?\n",
            "\n",
            "VISANDY:\n",
            "No adl ye: by chom, I sall you ono the\n",
            "Sho lord:\n",
            "Hatishad, bowl for the that a magkn--dogry, bebed thouses of these wamis, I Hear dradond in eif heather of ally, facce,\n",
            "To mockn youd your, muds be geapaucks ofe bock, I omme a gokild, same the mone.\n",
            "\n",
            "IC be had wash engroy peatize duporen so; hay all, the by be and de pustes for light coldan'd grever\n",
            "Sell to wey chound me or the all hum docincs and I a copar'ds: besees mordse have, craice: he at come:\n",
            "But on the on ye forgion, the but soud me, fead the pook.\n",
            "\n",
            "VINENS:\n",
            "Os lat bearshey for his sgcreance,\n",
            "That that nent the fora are in of no go, foorfull, in bit dote.\n",
            "\n",
            "NUNIESO:\n",
            "I with the ponus thes, and that mathy sis groy dowarf show vily would.\n",
            "ALANUS:\n",
            "The agry'd wald\n",
            "Af hinks, sheres\n",
            "suen lattt these;---sbre the, out dayarise\n",
            "Thee your mee at warras heave now romfe vance a ladickl en come shald.\n",
            "Sase thim pear us me! by con kname you. Al\n",
            "To es saiding\n",
            "roths fald, grarever's fiting elver,\n",
            "The diven these to knein wordry meance a ncoush\n",
            "My am and in and afes, you live.\n",
            "Nor fordings you hip tubeme heiss a me agapingse ow.\n",
            "DOEN UES:\n",
            "Leldin kink,\n",
            "We ir featpet daid the wear somast in to his my bliker,\n",
            "Of id mone, good be gradin, a so boon, the fir thounl?\n",
            "\n",
            "GLULOUSNES:\n",
            "I the shad of green saus,\n",
            "Theis was on sut your have so me he dace, my of thy sapony'd flartes, the as my the, cangher I doncerss-fries a we the twoo heply groysentlete.\n",
            "\n",
            "Grton he heand if hee how us and but goagtess: came, you\n",
            "Wentlirng: not mown to that !\n",
            "\n",
            "Hish is gre astioriam end on do'll:\n",
            "The ignt: is pead of sated yout.\n",
            "Ledwn\n",
            "At we shating of soldrs,\n",
            "Till thou be, not cradsed will he gone setrach heads ure bear watis this ing notre, sausily,\n",
            "The hoorces the me imen in famed by hagal; weace, is it follds or sight a loblestgod a with\n",
            "cound own andst bardas\n",
            "To Son,\n",
            "Cod go crish and on'd what ir pall I but selje, have sof Heps crack sin.\n",
            "Ad, the nobly nout tothen is may of come othewar sted bear sow\n",
            "yus me lef, for wicen it of th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. 의 2.29에서  2.23까지 줄었다."
      ],
      "metadata": {
        "id": "cQEseFip5zhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Block을 추가함에 따른 깊어진 Neural Net을 더 잘 optimize하기 위해 Layer Normalization 추가\n",
        "\n",
        "https://youtu.be/kCc8FmEb1nY?t=5560\n",
        "\n",
        "단, 여기서는 원본 Transformer 논문과는 다르게 Layer normalization의 위치가 skip connection과 같이 있는 것이 아닌, Multi head attention, FFNN에 들어가기 전으로 바꾼다.\n",
        "- 원본 Transformer의 경우를 post norm, 여기서 구현하는 것을 pre norm이라 하는데, 요즘은 pre norm이 더 좋다 하여 이 방법을 많이 쓴다고 한다."
      ],
      "metadata": {
        "id": "_5UBmpbY5zf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 직접 작성해본 코드는 아니고, 가져온 것인데 사용하진 않았다.\n",
        "# nn.LayerNorm를 대신 사용\n",
        "\n",
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "id": "1Ch8ZeFGLchD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.key(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-Inf'))\n",
        "    wei = F.softmax(wei, dim = -1)\n",
        "\n",
        "    v = self.value(x)\n",
        "\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    self.proj = nn.Linear(n_embed, n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.proj(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4*n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embed, n_embed)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed) # 9. 에서 추가된 부분\n",
        "    self.ln2 = nn.LayerNorm(n_embed) # 9. 에서 추가된 부분\n",
        "    # Layer norm은 batch가 아니라 하나의 sample의 feature의 개수에 대해서 normalization을 하기 때문에 feature의 개수인 n_embed를 입력으로 넣어준다.\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))  # 8. 9. 에서 추가된 부분\n",
        "    x = x + self.ffwd(self.ln2(x))  # 8. 9. 에서 추가된 부분\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(n_embed, n_head=4),\n",
        "        Block(n_embed, n_head=4),\n",
        "        Block(n_embed, n_head=4),\n",
        "        nn.LayerNorm(n_embed)\n",
        "    )\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "\n",
        "      logits, loss = self(idx_cond)\n",
        "\n",
        "      logits = logits[:, -1, :]\n",
        "\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "optimizer = torch.optim.AdamW(m.parameters(), learning_rate)\n",
        "\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "batch_size = 32\n",
        "for steps in range(max_iters):\n",
        "\n",
        "  if steps % eval_interval == 0 or steps == max_iters - 1:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBtUbhJuLVEd",
        "outputId": "8b27de9c-ded7-430d-f752-80360204c976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.042369 M parameters\n",
            "step 0: train loss 4.3112, val loss 4.3103\n",
            "step 500: train loss 2.4357, val loss 2.4328\n",
            "step 1000: train loss 2.3157, val loss 2.3159\n",
            "step 1500: train loss 2.2235, val loss 2.2397\n",
            "step 2000: train loss 2.1968, val loss 2.2259\n",
            "step 2500: train loss 2.1424, val loss 2.1783\n",
            "step 3000: train loss 2.1217, val loss 2.1692\n",
            "step 3500: train loss 2.1012, val loss 2.1451\n",
            "step 4000: train loss 2.0810, val loss 2.1387\n",
            "step 4500: train loss 2.0562, val loss 2.1336\n",
            "step 4999: train loss 2.0383, val loss 2.1033\n",
            "\n",
            "Row dut\n",
            "Mofth eread?\n",
            "What pra't our that the to velacEth by to thy\n",
            "Should she atce addon, last be hath light on this that with thour:\n",
            "So sould a notid I Hattior, for in enfores leajok aloys fromffers mocke youd, would thou to eapauces ofe bock.\n",
            "I ome\n",
            "The to lougse the ploower so, be hath howse shald basign dumper,\n",
            "But hay all, the by be andede a my sif.\n",
            "\n",
            "JULABES:\n",
            "She thule emutel,\n",
            "To Vurdews fill bodde?\n",
            "\n",
            "NORIV:\n",
            "Thech bs anguter, oparyis:\n",
            "To neags sold have, Or, bake maath, plusce, on the ondy, for in herd,\n",
            "And soy!\n",
            "Seelf not;\n",
            "I promest.\n",
            "\n",
            "JUKET:\n",
            "For sbearshey for hades cret beare ton brocken it twee a as youl he to be donding he hea proche.\n",
            "\n",
            "NUNGEOM:\n",
            "I with that will the ot as that my with The my dower broke as your him the,\n",
            "Nith that that, lome, hin svich woodse arll\n",
            "sto I timus swall gootht now are way Amy lifes eanton, whe heave now rome\n",
            "Whan:\n",
            "Garl.\n",
            "\n",
            "ANENCE NOLARDUS:\n",
            "I I swall mane,\n",
            "Whe me! be is the,\n",
            "Hor womtly myes saiding\n",
            "TI have a that Enver's fiit soelk was gonde be the up your in wordry mes what nother\n",
            "My amun: with the fechengar is caresed eomn shonth the a bme heill anmes gaviess by suir you the simen kink,\n",
            "Wefir:\n",
            "Shapst daied you go. My as nipte men astashe\n",
            "ar,\n",
            "God.\n",
            "Good iug proble!\n",
            "Lard, a so bo bast'd fie, to nlow welll for mealt noth,\n",
            "gost need saus,\n",
            "Their wes on surmpone have sorme he dace, my oddes.\n",
            "\n",
            "EXKE RGD fartes, the as my throoughther IHIT:\n",
            "To o-f if an were welld eanspelles\n",
            "As me to the to brean?\n",
            "And may dow,--us all but oft,\n",
            "Tis acase cournse, bired: noTul wift!\n",
            "And f!\n",
            "\n",
            "LRORINCUS:\n",
            "Has but allees:\n",
            "As thewa\n",
            "thell no: it fall of say thy laind duf\n",
            "The lose him for soedrs,\n",
            "Till thou be, not cratouck mele not-le setrech? have bre biartwation's'd inglat rewas?\n",
            "\n",
            "JULIY:\n",
            "Let orces the me imen; not.\n",
            "\n",
            "Sabllow a kink crusian:\n",
            "Wholeds outen,\n",
            "to Roublesegre a with\n",
            "could own anest batake\n",
            "To So you beal adis pasterve, what if pall I bupeseljliugde be forn's in ewe in bed, the nobk thou my treen, I po of come othe a whe dis my soreedss the king we be this of th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.에서의 loss인 2.2379에 비해서 여기서는 loss가 2.1 까지 줄었다."
      ],
      "metadata": {
        "id": "g6Elztcx5zd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기까지가 Transformer 원문의 Decoder 파트였다."
      ],
      "metadata": {
        "id": "igkr44mhO1zs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Dropout 추가 및 마지막 코드 정리"
      ],
      "metadata": {
        "id": "62mbvAst5zbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/kCc8FmEb1nY?t=5888 dropout 언제 추가하는지."
      ],
      "metadata": {
        "id": "xPWAxzvTQFuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "n_layers = 3  # 9. 에서 추가된 부분\n",
        "dropout = 0.2 # 10. 에서 추가된 부분\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337) # pytorch의 random seed 고정. 재구현성을 위해 설정\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.key(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-Inf'))\n",
        "    wei = F.softmax(wei, dim = -1)\n",
        "\n",
        "    v = self.value(x)\n",
        "\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    self.proj = nn.Linear(n_embed, n_embed)\n",
        "    self.dropout = nn.Dropout(dropout) # 10. 에서 추가된 부분\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4*n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embed, n_embed),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential(\n",
        "        *[Block(n_embed, n_head = n_head) for _ in range(n_layers)]\n",
        "\n",
        "    )\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "\n",
        "\n",
        "\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "\n",
        "\n",
        "\n",
        "      logits, loss = self(idx_cond)\n",
        "\n",
        "      logits = logits[:, -1, :]\n",
        "\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "optimizer = torch.optim.AdamW(m.parameters(), learning_rate)\n",
        "\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "batch_size = 32\n",
        "for steps in range(max_iters):\n",
        "\n",
        "  if steps % eval_interval == 0 or steps == max_iters - 1:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "jm0i5v2Zwdvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_xJLyhGmwdsk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}